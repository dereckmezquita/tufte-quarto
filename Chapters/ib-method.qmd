# The Information Bottleneck Principle {#ch:ib}

As we already discussed ([\[sec:communication_problem_setting\]][1]), Shannon intentionally left out from information theory[^1] issues of meaning or relevance, and focused on the problem of transmitting information.

Contrarily, @tishby:1999 argue in [@tishby:1999] that lossy source compression provides a natural quantitative approach to the matter of relevance and, therefore, they use Information Theory itself to address relevance.

This chapter will present the Information Bottleneck Principle, the foundation of the emergent theory subject of this dissertation. The IB principle approach is related to [RDT]{acronym-label="RDT" acronym-form="singular+full"}. Hence, first we will briefly overview RDT as @tishby:1999 describe it [@tishby:1999; @slonim:2002]. Then, we will formally present the IB Principle, its problem setting and analytical solution, and show how it can be seen as a particular case of [RDT]{acronym-label="RDT" acronym-form="singular+long"}.

## Rate-Distortion Theory: relevance through a distortion function {#sec:RDT}

We know from [\[eq:shannon_1st_law\]][2] that for any rate $R \leq \HX$ there will be a loss in the reconstructed signal. [RDT]{acronym-label="RDT" acronym-form="singular+full"} addresses the problem of determining the rate $R$ that should be communicated over a channel so that the source (input signal $\rvX$) can be approximately reconstructed without exceeding an expected distortion.

### The [RDT]{acronym-label="RDT" acronym-form="singular+short"} problem

#### Problem setting

1.  Let the discrete random variable $\rvX$ denote the **source** of vectors randomly drawn from a probability distribution $p(x)$;

2.  Each vector $x\sim p(x)$ is a **message** (signal) you want to transmit among a set of possible messages $\aXX,i.e.\  x \in \aXX$ ;

3.  Let another discrete random variable $\rvZ$ denote[^2] a compressed **representation** of $\rvX$ ;

4.  This representation is defined by a **channel** $p(z|x)$, a stochastic mapping between each message $x \in \aXX$ to each code $z \in \aZZ$;

5.  The **rate** $R$ is the channel capacity, i.e. the average number of bits per element $\rx \in \aXX$ needed to specify a compressed element (code) $\rz \in \aZZ$.

6.  Let $d: \aXX \times \aZZ \to \Real^{+}$ be a function that denotes the **distortion measure** between $\rvX$ and its representation $\rvZ$. Examples of distortion measures are the mean square error, $d_{MSE}(x;z)=\langle (x - z)^2 \rangle$ or the Hamming distortion (probability of error) $d_{H}(x,z)=\truth_{[x \neq z]}$.

#### Problem Statement

Given the problem setting above, the [RDT]{acronym-label="RDT" acronym-form="singular+short"} problem[^3] is to find the minimal number of bits per symbol (rate $R$) that should be communicated over a channel so that the source $\rvX$ can be approximately reconstructed via a representation $\rvZ$ without exceeding an expected distortion $D$, defined by the distortion function $d(x;z)$.

### Understanding the [RDT]{acronym-label="RDT" acronym-form="singular+short"} problem

The core of the [RDT]{acronym-label="RDT" acronym-form="singular+short"} problem is the need for a good compressed representation of a message. From [\[eq:shannon_1st_law\]][2], any rate $I[\rvZ;\rvX] \leq H[\rvX]$ will imply a loss in the reconstructed signal, an expected distortion, $\langle d(x;z) \rangle$.

As we have seen in [\[noisy_channel_theorem\]][3], low values of $I[\rvZ;\rvX]$, calculated based on the joint distribution $p(x,z)=p(x)p(z|x)$, imply compact representations, i.e. $|\aZZ|$ is small. In the extreme, all messages are translated to the same code: $|\aZZ|=1$ and $I[\rvZ;\rvX]=0$. Contrastingly, high values of $I[\rvZ;\rvX]$ imply low compression. In the extreme, $\rvZ$ simply copies $\rvX$: $I[\rvZ;\rvX]=H[\rvX]$ and $|\aZZ|=|\aXX|$.

Suppose we can compress the input data to any amount of information from $0$ to $H[\rvX]$. What will define the relevance of information is the additional constraint of the problem: the distortion measure. Given such function, the partitioning of $\rvX$ defined by $p(z|x)$ has the *expected distortion*: $$\begin{aligned}
    \langle d[x;z] \rangle _{p(x,z)} = \sum_{x,z} p(x) p(z|x) d[x;z]
\end{aligned}$$

Consequently, we are assuming that the definition of relevance is part of the problem setting. In other words, [RDT]{acronym-label="RDT" acronym-form="singular+short"} is agnostic on any arbitrary choice of the distortion function. This choice, nevertheless, determines the relevant features of the signal[^4] and should be somehow related to the task we want to perform with the input. Thus, **an arbitrary distortion function is, in fact, an arbitrary feature selection** [@tishby:1999].

As we will see further ([1.2]), @tishby:1999 [@tishby:1999] propose a way to cope with this potential pitfall.

### [RDT]{acronym-label="RDT" acronym-form="singular+short"} as a variational problem {#sec:RDT_problem}

::: definition
The **rate-distortion function**, denoted by $R(D)$ is defined as: $$\begin{aligned}
        R(D) \equiv \underset{p(z|x):~ \langle d(x;z) \rangle \leq D}{\min} I[\rvZ;\rvX]. \label{rate-distortion function}
    
\end{aligned}$$
:::

Therefore, $R(D)$ is the minimum achievable rate among all normalised conditional distributions, $p(z|x)$, for which the distortion constraint is satisfied. The *rate-distortion function* is a non-increasing convex function of D in the *distortion-compression plane* [@cover:2006] (see [\[fig:distortion-compression-plane\]][4]).[^5]

The region above the curve corresponds to all achievable *distortion-compression* pairs, while below the curve is the non-achievable region. Let $\{D, I_{\rvX}\}$ be a *distortion-compression* pair, if it is in the achievable region, there is a representation $\rvZ$ with a compression level $I[\rvZ;\rvX]=\Ix$ and an expected distortion of at most $D$. If it is in the non-achievable region, there is no such representation $\rvZ$. This limit on the achievability of representations is a direct consequence of Shannon's laws ([\[shannon_laws\]][5]).

Instead of solving the minimisation problem in [\[rate-distortion function\]][6] exactly, the problem is usually approximated by the following Lagrangian relaxation functional: $$\begin{aligned}
    \mathcal{F}[p(z|x)] = I[\rvZ;\rvX] + \beta \langle d(x;z) \rangle_{p(x,z)},
\end{aligned}$$ under the normalisation constraint $\sum_z p(z|x)=1, \forall x \in \aXX$.

::: theorem
[]{#thrm:RDT_solution label="thrm:RDT_solution"} The solution of the variational problem [@tishby:1999] $$\begin{aligned}
        \frac{\partial \mathcal{F}}{\partial p(z|x)}=0,
    
\end{aligned}$$ for normalised distributions $p(z|x)$ is given by the exponential form $$\begin{aligned}
        p(z|x) = \frac{p(z)}{Z(x,\beta)}\exp (-\beta ~d(x;z)),\label{exponential-form}
    
\end{aligned}$$ where $Z$ is the normalisation factor (partition function). The Lagrange multiplier $\beta$ is positive and $$\begin{aligned}
        \frac{\partial R}{\partial D} = - \beta.
    
\end{aligned}$$
:::

This is an implicit solution[^6] as $p(z)$ on the right-hand side of [\[exponential-form\]][7] depends on $p(z|x)$[^7].

## The IB Principle: relevance through a target variable {#sec:IB_principle}

The problem of extracting what is relevant from data depends on a suitable definition of relevance. The main weakness of the [RDT]{acronym-label="RDT" acronym-form="singular+short"} approach is that it addresses relevance through a distortion function that is not related to a specific task at hand.

The **IB Principle**, suggested by  @tishby:1999 [@tishby:1999] introduces an alternative approach: defining a "target" variable is simpler and more direct than defining a distortion measure.

For example, in speech compression[^8], any compression beyond the signal's entropy cannot be perfectly reconstructed; it is a lossy compression. On the other hand, a transcript has orders of magnitude lower entropy than the acoustic waveform, which means that for the task of understanding what has been transmitted, it is possible to compress the signal *much* further without losing any information about meaning [@tishby:1999].

In many situations, we have access to an additional variable that determines what is relevant. If we want to recognize cats in pictures, maybe we do not need a 360 kb picture as depicted on the left in Figure [\[fig:ib_problem_setting\]][8]; the 5 kb representation on the right may suffice. The exact representation would not be sufficient for the task of recognizing the breed of the cat, in any case. **Relevance is task-dependent**.

### The IB Problem Setting {#ib_problem_setting}

#### Definitions

1.  Let $\rvX$ be a random variable that denotes the **Source** [^9] of **messages** $\rx \in \sA_{\rvX}$;

2.  Let $\rvY$ be a random **relevant variable** (or **Target**) that defines the intended meaning $p(y|x)$ of the message $x$;

3.  Let $\rvZ$ be an **information bottleneck** variable, the representation, that obeys the Markov chain $\rvY \leftrightarrow \rvX \leftrightarrow \rvZ$;

4.  Let the conditional p.d.f $p(z|x)$ be the **encoder**, a stochastic mapping from each value of $x \in \sA_{\rvX}$ to a codeword $z \in \sA_{\rvZ}$;

5.  $\IXZ$ is the **rate** (or compression level) of the encoder, and reflects how much the bottleneck representation $\rvZ$ compresses $\rvX$;

6.  Let the conditional p.d.f $p(y|z)$ be the **decoder**, a stochastic mapping from each value of $z \in \sA_{\rvZ}$ to a prediction $\hat{y} \in \sA_{\rvY}$;

7.  $\IZY$ is the **relevant information** that the compressed representation $\rvZ$ keeps from the label variable $\rvY$;

#### Assumptions

i.  The random variables $\rvX$, $\rvY$ and $\rvZ$, are **discrete**;

ii. $\sA_{\rvX}$, $\sA_{\rvY}$ and $\sA_{\rvZ}$ are **finite sets**;

iii. $\rvX$ and $\rvY$ are dependent, and **the joint distribution $P(\rvX=x, \rvY=y)=p(x,y)$ is known**;

iv. The source $\rvX$ is an ergodic process[^10]; therefore $x \sim p(x)$ are not necessarily mutually independent.

v.  The encoder and the decoder are stochastic mappings. Hence, act like noisy channels.[^11]

#### Problem statement

The *information bottleneck problem* consists of finding an encoder $p(z|x)$ that produces a codebook $\rvZ$ that compress $\rvX$ as much as possible, i.e. $\IXZ$ is minimal, while keeping the *relevant information* of $\rvX$ for predicting $\rvY$, $\IZY$. In other words, the representation $\rvZ$ acts like a **bottleneck** that \"squeezes\" the relevant information that $\rvX$ contains about the target $\rvY$ in a compressed form, hence the name \"information bottleneck\".

### Relation to other Information Theory Problems

Connections between problems allow extending ideas from one setup to another. In this regard, the IB problem is closely related to other coding problems like the *Indirect* or the *Remote Source-coding problem*, also known as the *CEO Problem*, and the *privacy funnel problem* [@zaidi:2020].

### Relation to Minimum Sufficient Statistics

In the IB problem, the target variable is what we want to predict. $\rvY$ acts as a parameter of $\rvX$ and $\rvZ \ind \rvY$. Thus, *the representation $\rvZ$ is a statistic of $\rvX$*.

For $\rvZ$ to be a **sufficient statistic** of $\rvX$ w.r.t. $\rvY$, it must preserve all relevant information in $\rvX$, $I[Y;X]=I[Z;X]$. In other words, no other statistic of $\rvX$ can provide any additional information as to the value of $\rvY$ then $\rvZ$ does.

The representation is **minimal** if it is the smallest among all possible representations.

Therefore, we can say that the information bottleneck is the problem of finding the *minimum sufficient statistics* of the random variable $\rvX$ w.r.t $\rvY$, and therefore, IB Lagrangian gives the minimum approximately sufficient statistic.

## The IB Curve {#sec:ib_functional}

As in RDT, the compactness of the representation is measured by $I[\rvZ;\rvX]$. The distortion upper bound constraint, however, is replaced by a lower bound constraint over the *relevant information*, $I[\rvZ;\rvY]$ [@shamir:2010].

::: definition
The *IB Curve* or *relevance-compression function* is the functional that expresses the IB problem [@bachrach:2003]: $$\begin{aligned}
    R^{(\text{IB})}(I_{\rvY}) &= \underset{p(z|x):~ I[\rvZ;\rvY]\geq I_{\rvY}}{\min} I[\rvZ;\rvX],\\
    \text{or alternatively:} \nonumber                                              \\
    I_{\rvY}^{(\text{IB})}(R)&= \underset{p(z|x):~ I[\rvZ;\rvX]\leq R}{\max} I[\rvZ;\rvY],
\end{aligned}$$ where the random variables form a Markov chain $\rvY \leftrightarrow  \rvX \leftrightarrow  \rvZ$ and the minimisation is over all the normalised conditional distributions $p(z|x)|\sum_x p(z|x)=1$ for which the constraint is satisfied.
:::

A straightforward observation is that the Markovian relation characterises $p(z)$ and $p(y|z)$ through [@slonim:2002] $$\begin{aligned}
    \begin{cases}
        p(z) = \sum_{x,y} p(x,y,z) = \sum_x p(x)p(z|x)  \\
        p(y|z) = \frac{1}{p(z)}\sum_x p(x,y,z) = \frac{1}{p(z)}\sum_x p(x,y) p(z|x).\label{observation}
    \end{cases}
\end{aligned}$$

### The information plane {#sec:information-plane}

Moreover, the plane where the horizontal axis corresponds to $I[\rvZ;\rvX]$ and the vertical axis to $I[\rvZ;\rvY]$, named **information plane** (see [\[fig:information-plane\]][9]) is the natural equivalent to the distortion-compression plane in [RDT]{acronym-label="RDT" acronym-form="singular+long"} ([\[fig:distortion-compression-plane\]][4]). Let the pair ${R, \Iy}$ denote some levels of compression and relevant information, respectively. If this pair is located below the curve, some compressed representation $\rvZ$ has a compression level $R=I[\rvZ;\rvX]$ and relevant information $\Iy=I[\rvZ;\rvY]$. The points laying on the IB Curve are the optimal representations for a certain level of relevant-information (or precision) $\Iy$ or a certain level of compression (or complexity) $R$.

## The IB Lagrangian {#sec:ib_problem_relaxation}

The Lagrangian relaxation of the IB functional is also a variational problem:

$$\begin{aligned}
    \LH_{\beta}^{\text{(IB)}}[p(z|x)] = \IZX - \beta \IZY, \label{IB_Lagrangian}
\end{aligned}$$ where $\beta$ is the Lagrangian multiplier attached to the constrained relevant information [@tishby:1999].

At $\beta=0$, no feature of the signal is relevant, and all messages are quantised (compressed) to a single point. At $\beta=\infty$, the solution is pushed toward arbitrarily detailed quantisation (no compression). "By varying the (only) parameter, $\beta$, one can explore the tradeoff between the preserved meaningful information and compression at various resolutions" [@tishby:1999].[]{#sec:resolution label="sec:resolution"}

Unlike the [RDT]{acronym-label="RDT" acronym-form="singular+short"} problem ([1.1.3]), in the IB problem, the constraint on the meaningful information is *nonlinear* in the mapping $p(z|x)$, and it is a much harder variational problem. Notably, there is an analytical solution for IB Lagrangian ([\[IB_Lagrangian\]][10]). However, for the sake of clarity, before deriving this exact solution, we will show how IB can be seen as a particular case of [RDT]{acronym-label="RDT" acronym-form="singular+short"}. This development will further help us to derive the analytical solution more directly.

## IB problem as a particular case of the RDT problem {#sec:IB_special_RDT}

From the [DPI]{acronym-label="DPI" acronym-form="singular+full"} ([\[sec:dpi\]][11]), $$\begin{aligned}
    I[\rvX;\rvY] \geq I[\rvZ;\rvY].
\end{aligned}$$ Therefore, we can consider that the relevant information of $\rvX$ not captured by the representation $\rvZ$ is a natural choice for the expected distortion, as it represents a distortion in bits. $$\begin{aligned}
    \langle d[x;z] \rangle = I[\rvX;\rvY]-I[\rvZ;\rvY]\geq 0
\end{aligned}$$ From this definition, we can derive the following theorem:

::: theorem
If $\langle d[x;z] \rangle_{p(x,z)}=I[\rvX;\rvY]-I[\rvZ;\rvY]$, then $\\d[x;z] = \KL(p(y|x)~||~p(y|z))$.
:::

::: proof
*Proof.* $$\begin{aligned}
        &\langle d[x;z] \rangle_{p(x,z)} =I[\rvX;\rvY]-I[\rvZ;\rvY]\nonumber\\
        &=\sum_{x,y} p(x, y) \log \frac{p(x,y)}{p(x) p(y)} - \sum_{z,y} p(z, y) \log \frac{p(z,y)}{p(z) p(y)}.
    
\end{aligned}$$ Since $p(a,b) = p(b|a) p(a)$, we have: $$\begin{aligned}
        =&\sum_{x,y} p(y|x) p(x) \log \frac{p(y|x)\cancel{p(x)}}{ \cancel{p(x)}p(y)} -\sum_{z,y} p(y|z) p(z) \log \frac{p(y|z)\cancel{p(z)}}{ \cancel{p(z)}p(y)}.
    
\end{aligned}$$ From [\[observation\]][12] : $$\begin{aligned}
        =&\sum_{x,y} p(y|x) p(x) \log \frac{p(y|x)}{ p(y)} - \sum_{z,y,x} \frac{p(y|x) p(z|x) p(x) \cancel{p(z)}}{\cancel{p(z)}}~\log \frac{p(y|z)}{ p(y)}\\
        =&\sum_{x,y} p(y|x) p(x) \log \frac{p(y|x)}{ p(y)} - \sum_{z,y,x} p(y|x) p(z,x)~\log \frac{p(y|z)}{ p(y)}.
    
\end{aligned}$$ From the normalisation constraint, $\sum_{z} p(z|x)=1$: $$\begin{aligned}
        =&\cancelto{\sum_{z} p(z|x)}{1} \cdot \sum_{x,y} p(x) p(y|x) \log \frac{p(y|x)}{ p(y)} - \sum_{z,y,x} p(y|x) p(z,x)\log \frac{p(y|z)}{ p(y)}\\
        =&\sum_{z,x} p(z|x)p(x) \left[\sum_{y}  p(y|x) \log \frac{p(y|x)}{ p(y)}\right] - \sum_{z,x} p(x,z) \left[ \sum_{y} p(y|x) \log \frac{p(y|z)}{ p(y)}\right]\\
        =&\sum_{z,x} p(x,z) \left[\sum_{y}  p(y|x) \left(\log \frac{p(y|x)}{ p(y)}- \log \frac{p(y|z)}{ p(y)}\right)\right]\\
        =&\sum_{z,x} p(x,z) \left[\sum_{y}  p(y|x) \left(\log \frac{p(y|x) \cancel{p(y)}}{ \cancel{p(y)} p(y|z)}\right)\right]\\
        =&\E_{p(z,x)}  \KL{\left(~p(y|x)~ ||~ p(y|z)~ \right)}.
    
\end{aligned}$$ Therefore $$\begin{aligned}
        \langle d[x;z] \rangle_{p(x,z)} &= \langle \KL{\left(~p(y|x)~ ||~ p(y|z)~ \right)} \rangle_{p(x,z)}\\
        d[x;z] &= \KL{\left(~p(y|x)~ ||~ p(y|z)~ \right)}
    
\end{aligned}$$ ◻
:::

## Information Bottleneck Solution

characterises the general form of the optimal solution to the rate-distortion problem. As we showed that the IB problem could be seen as a particular case of the RDT problem, the IB solution is straightforward:[^12]

::: theorem
[]{#thrm:IB solution label="thrm:IB solution"} The analytical solution of the variational problem $$\begin{aligned}
        \frac{\partial \mathcal{L}_{\beta}^{\text{(IB)}}[p(z|x)]}{\partial p(z|x)} = 0,
    
\end{aligned}$$ for normalised distributions $p(z|x)$ is given by the exponential form $$\begin{aligned}
        \begin{cases}
        p(z|x) &= \frac{p(z)}{Z(x,\beta)}\exp (-\beta ~\KL{\left(~p(y|x)~ ||~ p(y|z)~ \right)}),\\
        p(z) &= \sum_{x,y} p(x,y,z) = \sum_x p(x)p(z|x)  \\
        p(y|z) &= \frac{1}{p(z)}\sum_x p(x,y,z) = \frac{1}{p(z)}\sum_x p(x,y) p(z|x).
        \end{cases}
    
\end{aligned}$$ where $Z$ is the normalisation factor (partition function). The Lagrange multiplier $\beta$ is positive and $$\begin{aligned}
        \beta = \frac{\partial I[Z;Y]}{\partial \IZX}.
    
\end{aligned}$$
:::

::: proof
*Proof.* Apply $d[x;z] = \KL{\left(~p(y|x)~ ||~ p(y|z)~ \right)}$ to . ◻
:::

## Concluding Remarks

In this section, we presented the [IB]{acronym-label="IB" acronym-form="singular+short"} Problem ([1.2.1]) and the [IB]{acronym-label="IB" acronym-form="singular+short"} Lagrangian ([1.4]), with its corresponding analytical companion, the information plane ([1.3.1]).

The exciting aspect of the [IB]{acronym-label="IB" acronym-form="singular+short"} Problem is that it uses the "help" of a relevant variable to define the distortion measure. Therefore, we have a task-specific distortion measure (loss function). In opposition to [MLT]{acronym-label="MLT" acronym-form="singular+short"} and [RDT]{acronym-label="RDT" acronym-form="singular+short"}, which are loss-function-agnostic, in the IB method, the [KL]{acronym-label="KL" acronym-form="singular+full"} of the true distribution $p$ and the model $q$ emerges as the natural choice ([1.5]).

Despite the similarities with the supervised learning problem ([\[sec:mlt_problem\]][13]), the IB Problem assumes knowledge of the distribution $\PXY$, and it is not yet in the realm of [ITML]{acronym-label="ITML" acronym-form="singular+long"}.

[^1]: Which Shannon has always referenced as Communication Theory.

[^2]: This compressed representation of $\rvX$ is usually denoted by $\rvZ, \rvT$ or $\hat{\rvX}$

[^3]: First defined by Shannon .

[^4]: The same can be said of a learning algorithm loss function in [MLT]{acronym-label="MLT" acronym-form="singular+short"}, which determines what is relevant to be learned.

[^5]: We will explain what $\beta$ means later.

[^6]: Implicit solution means a solution in which dependent variable is not separated.

[^7]: $p(z)=\sum_{x,z} p(z|x)~p(z)$

[^8]: By the time of  publication, Tishby was working on speech-related problems.

[^9]: The IB problem is a one-shot coding problem, the operations are performed letterwise .

[^10]: The *ergodic* property means statistical homogeneity :  its statistical properties can be deduced from a single, sufficiently long, random sample of the process.

[^11]: Notice that given the Markov chain $\rvY \leftrightarrow \rvX \leftrightarrow \rvZ$, due to reparemetrisation invariance (), a deterministic mapping of the data does not throw out information, i.e.  let $f:\aXX \to \aYY$ be deterministic, $I[f(\rvX);\rvY]=\IXY$.

[^12]: The analytical solution to the IB problem is sometimes called the self-consistent equations.

  [1]: #sec:communication_problem_setting {reference-type="ref" reference="sec:communication_problem_setting"}
  [2]: #eq:shannon_1st_law {reference-type="ref" reference="eq:shannon_1st_law"}
  [3]: #noisy_channel_theorem {reference-type="ref" reference="noisy_channel_theorem"}
  [1.2]: #sec:IB_principle {reference-type="ref" reference="sec:IB_principle"}
  [4]: #fig:distortion-compression-plane {reference-type="ref" reference="fig:distortion-compression-plane"}
  [5]: #shannon_laws {reference-type="ref" reference="shannon_laws"}
  [6]: #rate-distortion function {reference-type="eqref" reference="rate-distortion function"}
  [7]: #exponential-form {reference-type="ref" reference="exponential-form"}
  [8]: #fig:ib_problem_setting {reference-type="ref" reference="fig:ib_problem_setting"}
  [9]: #fig:information-plane {reference-type="ref" reference="fig:information-plane"}
  [1.1.3]: #sec:RDT_problem {reference-type="ref" reference="sec:RDT_problem"}
  [10]: #IB_Lagrangian {reference-type="ref" reference="IB_Lagrangian"}
  [11]: #sec:dpi {reference-type="ref" reference="sec:dpi"}
  [12]: #observation {reference-type="ref" reference="observation"}
  [1.2.1]: #ib_problem_setting {reference-type="ref" reference="ib_problem_setting"}
  [1.4]: #sec:ib_problem_relaxation {reference-type="ref" reference="sec:ib_problem_relaxation"}
  [1.3.1]: #sec:information-plane {reference-type="ref" reference="sec:information-plane"}
  [1.5]: #sec:IB_special_RDT {reference-type="ref" reference="sec:IB_special_RDT"}
  [13]: #sec:mlt_problem {reference-type="ref" reference="sec:mlt_problem"}
