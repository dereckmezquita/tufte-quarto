# Information Bottleneck and Representation Learning {#ch:ib_and_rl}

[]{#ch:ib_rl label="ch:ib_rl"}

This chapter presents the idea of using the [IB]{acronym-label="IB" acronym-form="singular+short"} principle for representation learning in general, not specific for Deep Learning, which will be the subject of [\[ch:ib_and_dl\]][1].

In [1.1], we show *why* to learn representations. In [1.2], we discuss *how* to characterise a good representation. [1.6] presents the two levels of representation in learning, which will help us understand *what* to represent.

In [1.3], we finally present the IB Learning problem, its difference to the IB method, *how* to find good representations with it, and its strengths and weaknesses as a representation learning framework. Finally, we close the chapter with [1.10] that brings evidence that the IB framework can predict bounds on human learning.

## Representation Learning {#sec:why_representation_learning}

In our human experience, we know that a good representation of data is crucial for accomplishing tasks. The Hindu-Arabic numeral system advantages, for example, are so manifest that it has been adopted almost everywhere.

In the history of Machine Learning, good representations have always played a central role. In its first years, before trying to solve a task, researchers would *feature engineer*: use their knowledge of the problem in hand to *encode* the data into a representation easier for computers to learn the task.

The goal of designing features is to separate explanatory factors of variation behind the high dimensional observed data. The challenge is that many of the "factors of variation" influence every piece of data we can observe  [@goodfellow:2016].

Consider the problem of Object Classification[^1]; each pixel depends on different factors: the viewing angle of the picture, the object's pose, the quality and calibration of the lens, the conditions of lightning, unrelated background objects.

Over time, it became clear that the success of machine learning was so heavily dependent on appropriate features that finding them should also be part of the process of learning itself. Therefore, or is a set of techniques that allows a machine to learn features and use them to perform a specific task. Learned representations often result in better performance and flexibility, allowing a more straightforward adaptation of an AI system to new tasks, with the minimal human intervention  [@goodfellow:2016]. Furthermore, the recent success of Deep Learning, which is one of many ways to learn representations, has shown the power of this *encoder-decoder* scheme.

## Desiderata for representations {#sec:desiderata}

What are good representations of the data? A good representation makes a subsequent learning task easier [@goodfellow:2016].  @achille:2017emergence [@achille:2017emergence] present a mathematical definition using information theory.

task

:   : In supervised learning, we want to find the stochastic conditional distribution **$p(y|x)$** of a target variable $\rvY$ that we refer as the task: $$\begin{aligned}
            \rvY := P(\rvY|\rvX=x)
        
    \end{aligned}$$

representation

:   : $\rvZ$ is a representation of $\rvX$ if it can be fully described by the stochastic conditional $p(z|x)$: $$\begin{aligned}
            \rvZ := P(\rvZ|\rvX=x)
        
    \end{aligned}$$

sufficient:

:   $\rvZ$ is a sufficient representation of $\rvX$ w.r.t $\rvY$ if $\rvY \to \rvX \to \rvZ$ form a Markov chain[^2] and: $$\begin{aligned}
            \IZY=\IXY.
        
    \end{aligned}$$

minimal:

:   $\rvZ$ has the smallest amount of information among all the sufficient representations of $\rvX$. This means there is an encoding from $\rvX$ to $\rvZ$ that keeps only relevant information[^3]: $$\begin{aligned}
          \exists ~\rvX \mapsto \rvZ~ | ~~ \IZX = \IZY = \IXY
      
    \end{aligned}$$

invariant:

:   to the effect of nuisances (noise).[^4] Let $\rvN$ be a nuisance for the task $\rvY$. If $\rvN$ does not have information about $\rvY$, there should not be information of $\rvN$ in the representation $\rvZ$ , the classifier could fit spurious correlations:$$\begin{aligned}
        \rvN\perp \rvY &\Rightarrow I[\rvN;\rvY]=0 \nonumber\\
        &\Rightarrow I[\rvZ;\rvN]=0
      
    \end{aligned}$$

maximally disentangled:

:   information lies on components of the representation $\rvZ$ and not in the correlations of them. Then, mathematically, let $TC$ denote the total correlation, *multi-information*.$$\begin{aligned}
        TC(\rvZ)=& \KL(p(z)||\prod_i^n p(z_i)),\\
        TC(\rvZ)=& 0 \implies z_1 ~\ind~ z_2 ~\ind~ \cdots ~\ind~ z_n.
      
    \end{aligned}$$

This desiderata for representations corresponds directly with our goals for learning algorithms. We want our models to predict the task correctly (sufficiency). Simultaneously, we want them to generalise to out-of-sample examples (invariance to nuisance factors).

$$\begin{aligned}
         \text{accuracy} &\leftrightarrow \text{sufficiency}\\
         \text{generalisation} &\leftrightarrow \text{invariance/minimality}\\
         \text{explainability} &\leftrightarrow \text{disentanglement}
    
\end{aligned}$$

Another desired characteristic, albeit often forgotten, is that we want our models to be explainable.[^5] This characteristic relates to *disentangling* the underlying causes (factors) of the observed data (maximally disentangled) [@goodfellow:2016].

Although disentanglement may be an abstract characteristic not very well defined, @achille:2017emergence [@achille:2017emergence] propose a simplification by defining it as the total correlation of the representation features.

The only property of the desiderata that still does not correspond with learning algorithms is *minimality*. However, it is straightforward that a small sufficient representation has a smaller chance of containing spurious correlations, and it is more likely to generalise well. Minimal sufficient representations have no spurious factors that do not explain the variability of the observed data. As we will show, a representation is invariant only if it is also minimal.

### Invariant if minimal {#sec:invariant_if_minimal}

::: theorem
Let $\rvN$ be a nuisance for the target $\rvY$ and let $\rvZ$ be a sufficient representation of the input $\rvX$ w.r.t $\rvY$. Suppose that $\rvZ$ depends on $\rvN$ only through $\rvX$ ( i.e. , $\rvN \to \rvX \to \rvZ$). We also consider that $\rvX$ has all information about $\rvY$; therefore, we can say that it is a **deterministic function** of $\rvY$ and nuisances $\rvX:= f(\rvY; \rvN)$.

To say that $\rvZ$ is invariant if and only if it is minimal implies that $\IZN = \IZX - \IXY$: $$\begin{aligned}
\forall~ \rvZ ~|~ \IZY = \IXY, \nonumber\\
\IZX = \IXY \Longleftrightarrow \IZN = 0, \nonumber\\
\IZN = \IZX - \IXY.
\end{aligned}$$ This equality holds up to a small residual $\epsilon$: $$\begin{aligned}
    \IZN &= \IZX - \IXY - \epsilon, ~0 \leq \epsilon \leq H[\rvY|\rvX]
\end{aligned}$$
:::

::: proof
*Proof.* $$\begin{aligned}
\begin{cases}
\rvY, \rvN \to \rvX \to \rvZ \qquad &\text{(by definition)}\hfil\\
I[\rvZ;\rvY,\rvN] \leq I[\rvZ;\rvX] \qquad &\text{(DPI)}\hfil\\
I[\rvZ;\rvY,\rvN] = I[\rvZ;\rvN] + I[\rvZ;\rvY|\rvN] \qquad &\text{(chain rule)}\hfil
\end{cases}
\end{aligned}$$ $$\begin{aligned}
&I[\rvZ;\rvN] + \cancelto{\scriptscriptstyle \IZY}{I[\rvZ;\rvY|\rvN]} \leq I[\rvZ;\rvX] \tag{$\rvN \ind \rvY$}\\
&I[\rvZ;\rvN] \leq I[\rvZ;\rvX] - \cancelto{\scriptscriptstyle \IXY}{\IZY} \tag{$\rvZ$ sufficiency}\\
&I[\rvZ;\rvN] = I[\rvZ;\rvX] - \IXY -\epsilon, \epsilon \geq 0 \tag{$\epsilon$ lower bound}
\end{aligned}$$ Now we only need to prove the upper bound for $\epsilon$: $$\begin{aligned}
\epsilon &= \IZX - I[\rvZ;\rvN] - \IXY \nonumber\\
&= I[\rvZ;\rvY,\rvN] - I[\rvZ;\rvN] - \IXY \tag{$\rvX:=f(\rvY; \rvN)$}\\
&= \cancel{I[\rvZ;\rvN]} + I[\rvZ;\rvY|\rvN] - \cancel{I[\rvZ;\rvN]}- \IXY \tag{chain rule}\\
&= \cancelto{\scriptscriptstyle H[\rvY]}{H[\rvY|\rvN]} - H[\rvY|\rvN;\rvZ] - H[\rvY] + H[\rvY|\rvX] \tag{$\rvN \ind \rvY$}\\
&= \cancel{H[\rvY]} - H[\rvY|\rvN;\rvZ] - \cancel{H[\rvY]} + H[\rvY|\rvX] \nonumber\\
 &\leq  H[\rvY|\rvX] \tag{$\epsilon$ upper bound}
\end{aligned}$$

$I[\rvZ;\rvN] = I[\rvZ;\rvX] - \IXY -\epsilon, ~0 \leq \epsilon \leq H[\rvY|\rvX]$ ◻
:::

As a consequence of this proposition, it is possible to construct invariant representations, which will generalise well, by reducing the amount of information the representation $\rvZ$ contains about the input $\rvX$ while keeping $\IZY$, the amount of information we need for the task. As $H[\rvY] \ll H[\rvX]$ **the compressibility of the input determines generalisation**. Thus, it is independent on the hypothesis space of the learning algorithm.

## IBT Learning Problem: Learning Approximately Minimal Sufficient Disentangled Representations {#sec:ibt_learning_problem}

We have discussed what constitutes a good representation. This section is about finding such representations. For that, we will adjust the IB Problem Setting ([\[ib_problem_setting\]][2]) for supervised learning.[^6]

### Definitions

1.  Let $\rvX$ be the random variable that denotes the **generator** (or **source**) of instance vectors $\rx$ of the learning problem (**messages**), randomly drawn from a probability distribution $P(\rvX)$,\
    $x\sim P(\rvX),~ x \in \sA_{\rvX}$;

2.  Let $\rvY$ be a random **relevant variable** (the **Target**) which represents the solution $\ry$ for the problem $\rx$, the intended meaning $p(y|x)$ of the message $x$,\
    $y\sim P(\rvY),~ y \in \sA_{\rvY}$;

3.  A **task supervisor** knows the **task** distribution $P(\rvY|\rvX)$ and returns an output vector $\ry_i$ for every input vector $\vx_i$[^7]:\
    $\vy_i := p(y|\vx_i)$;

4.  Let $\rvZ$ be a **bottleneck** random variable that denotes a compressed representation of the input $\rvX$ that is sufficient w.r.t. $\rvY$ and obeys the Markov chain $\rvY \leftrightarrow \rvX \leftrightarrow \rvZ$;

5.  Let the stochastic conditional distribution $q(z|x)$ be an **encoder**of input instances into representations,\
    $\rz := q(z|x)$.

6.  Let the stochastic conditional distribution $q(y|z)$ be a **decoder** of representations into solutions of the problem,\
    $\hat{\ry} := q(y|z)$.

7.  A **learning algorithm** $\AA$, which is the functional that given a dataset $D_n = \{(\rx_1,\ry_1), \cdots, (\rx_n, \ry_n)\}$ of $n$ inputs and outputs of the task, selects a hypothesis $h = \underset{\text{decoder}}{q(y|z)} \circ \underset{\text{encoder}}{q(z|x)}$ from the hypothesis space $\HH$: $$\begin{aligned}
            \AA: \underbrace{(\XX \times \YY)^n}_{D_n} \to \HH.
          
    \end{aligned}$$

### Assumptions {#sec:ibt_learning_assumptions}

i.  The random variables $\rvX$, $\rvY$ and $\rvZ$, are **discrete**;

ii. $\rvY \to \rvX \to \rvZ$ form a Markov-chain;

iii. $\sA_{\rvX}$, $\sA_{\rvY}$ and $\sA_{\rvZ}$ are **finite sets**;

iv. **No assumption on $\DD=\PXY$**.[]{#free-D label="free-D"}

v.  **$\DD=\PXY$ is unknown at the training stage**.

vi. **$\DD=\PXY$ is fixed:** the ordering of examples in the sample is irrelevant.

vii. $\rvX$ is i.i.d. sampled.[^8]

viii. The encoder and the decoder are **stochastic** mappings.[^9]

ix. the **distortion measure** between $\rvX$ and its representation $\rvZ$ is $\IXY - \IZY = \KL(p(y|x)||p(y|z))$.[^10]

x.  the **entanglement** of a random variable $\rvZ$ is defined as *total correlation* of its components [@achille:2018infodropout].

### Problem statement

Given the problem setting above, the [IBT]{acronym-label="IBT" acronym-form="singular+short"} learning problem is to find the encoder $p(z|x)$ and decoder $p(y|z)$ such that:

1.  the encoder maximises the compression of the input $\rvX$ into the representation $\rvZ$ while preserving the maximum information about the "meaning" $\rvY$. In other words, the encoder that generates minimal sufficient disentangled representations of the input.

2.  the decoder is trivial as a result of the characteristics of the representation.

3.  The selection is based on a training set of $n$ i.i.d. observations drawn from the distribution $\PXY$.

### IBT learning as a variational problem

Finding the encoder for minimal sufficient disentangled representations is equivalent to finding a distribution $p(z|x)$ that solves the following constrained optimisation problem: $$\begin{aligned}
        q(z|x):=&\underset{p(z|x)}{\argmin}  \quad  \IZX&  \\
        &\textrm{s.t.} \quad 0 \leq \IXY - \IZY& \nonumber\\
        &\quad\quad 0 \leq TC(Z).&\nonumber
    
\end{aligned}$$ This nonlinearly constrained optimisation problem[^11] is very similar to the IB Problem ([\[sec:IB_principle\]][3]). It just adds the total correlation constraint and assumes no knowledge over $\PXY$.  @tishby:1999 [@tishby:1999] proposed solving the IB problem using a relaxed minimisation, the IB Lagrangian: $$\begin{split}
        \underset{p(z|x)}{\min} \quad  & \IZX  \\
        \textrm{s.t.}\quad & \IZY \leq \IXY \\
        \end{split}
        \quad \Longrightarrow \quad
        \begin{split}
            &\min \IZX + \beta (\cancel{\IXY} - \IZY),\\
            &\min \IZX - \beta \IZY.
        \end{split}$$

Let us also apply a Lagrangian relaxation to our representation learning problem: $$\begin{aligned}
        \LH&=  \IZX + \beta (\IXY - \IZY) + \gamma TC(z),\\
        \text{Let }\beta^{-1}&= \frac{1}{\beta},\\
         \gamma'= \frac{\gamma}{\beta}\\
        \LH &=  \cancelto{\scriptscriptstyle \HYZ}{(\IXY - \IZY)} + \beta^{-1} \IZX + \gamma' TC(z),\\
        \LH &=  \HYZ + \beta^{-1} \IZX + \gamma' TC(z).
    
\end{aligned}$$ Let us denote $p_\theta(z|x)$ (encoder) and $p_\theta(y|z)$ (decoder) the unknown conditional distributions we want to estimate[^12], parametrised by $\theta$.

Then, rewriting the Lagrangian as a per sample loss function, we have: $$\begin{aligned}
        H[\rvY|\rvZ] & \approx \E_{(x,y) \sim p(x,y)}[\E_{z \sim p_{\theta}(z|x)} - \log p_{\theta}(y|z)] \\
        \IZX & = \E_{x \sim p(x)} \KL(p_{\theta}(z|x)||p(z))\\
        TC(z) &= \KL(p(z)||\textstyle \prod_j q(z_j))\\
        \hat{\LH} &= \frac{1}{n} \sum^{n} \E_{z \sim p_{\theta}(z|x_i)} - \log p_{\theta}(y_i|z) \nonumber\\
         &+ \beta^{-1} \KL(p_{\theta}(z|x_i)||p(z)) \nonumber\\
         &+ \gamma' \KL(p(z)||\textstyle \prod_j p_{\theta}(z_j)).
    
\end{aligned}$$

The second and third terms of the loss are intractable, as we need to know $p(z)$ to compute, which is an unknown of our problem. @achille:2018infodropout, however, prove that if $\beta^{-1}=\gamma'$, and we assume a factorised unknown distribution, the Lagrangian can be solved [@achille:2018infodropout].

$$\begin{aligned}
    &\hat{\LH} = \underbrace{\frac{1}{n} \sum^{n} \E_{p_{\theta}(z|x_i)} - \log p_{\theta}(y_i|z)}_{\hat H(p, p_{\theta})}+ \beta^{-1} \KL (p_{\theta}(z|x_i)||p_{\theta}(z)),\\
    &p_\theta(z)=\prod_j p_{\theta}(z_j).\\
    &\hat{\LH} = \hat H(p, p_{\theta}) + \beta^{-1} \KL(p_{\theta}(z|x)||p_{\theta}(z)) \qquad\text{Activations IB\label{activations_ib}}
    
\end{aligned}$$[]{#sec:activations_lagrangian label="sec:activations_lagrangian"}

Where $\hat H(p, p_{\theta})$ is the cross-entropy, and the second term is a regulariser that penalises the transfer of information from $\rvX$ to $\rvZ$. In other words, the regulariser penalises complexity measured as $\IZX$. The usage of cross-entropy loss and this kind of regularisers is widespread in practice. Nevertheless,  @achille:2019phd gave theoretical ground for such choices[^13] [@achille:2019phd].

Minimising the standard IB Lagrangian assuming the activations are independent, i.e. $q(z)=\textstyle \prod_i q(z_i)$ is equivalent to enforcing disentanglement. Practitioners already adopt this independence assumption on the grounds of simplicity since the actual marginal $p(z)$ is incomputable.  @higgins:2017 also empirically observed that using a factorised model results in \"disentanglement\" [@higgins:2017]. Because of the previous propositions, we can assume the activations are indeed independent and ignore the TC term.[^14]

::: restatable
corollaryspecialib []{#th:specialib label="th:specialib"} Any learning algorithm that:

-   assumes a stochastic $p(y|x)$;

-   uses a $\KL$-equivalent loss (for example the cross-entropy loss or the logistic loss);

-   and a regularisation term that penalises the amount of information of the input stored in the model,

is learning a minimal sufficient disentangled representation and, in fact, solving the IB learning problem.
:::

## The IB Achille's Heel {#sec:achilles_heel}

Achille and Soatto noticed a problem with the Activations IB, it is incomputable:

-   Z is a representation of yet not observed future data;

-   During training, a valid minimisation of I\[Z;X\] would be to memorise the indexes of each label;

-   During testing, once the weights are fixed, the network is not a stochastic mapping;

-   The only other way to compute IB would be with the true distribution P(X,Y), but that is unknown in our problem setting.

This realisation is very important. Many of the critiques on IBT (that we will see in [\[sec:ibt_criticism\]][4]) are due to not addressing it. Achille and Soatto not only acknowledged the problem but also proposed a solution ([1.6]). To explain how they arrive on that, we first need to explain how they analised the cross-entropy loss in an information-theoretical perspective.

## Rethinking generalisation: Cross-entropy and overfitting {#sec:rethink_generalisation}

In previous sections, we derived the cross-entropy loss ([\[activations_ib\]][5]) from a list of desired properties for representations([1.2]). We also showed that generalisation relates to the compressibility of the input([1.2.1]).

 @zhang:2016 demonstrates that the expressivity of [DNNs]{acronym-label="DNN" acronym-form="plural+short"} is enough to fit random labels [@zhang:2016]. Thus, at least for [DNNs]{acronym-label="DNN" acronym-form="plural+short"}, generalisation is more *not overfitting* than *not underfitting*. This characteristic may be the case for other learning techniques as well. In this section, we will keep *rethinking* generalisation on this new information-theoretical perspective and try to elucidate how cross-entropy loss relates to overfitting and memorisation.

Classical [MLT]{acronym-label="MLT" acronym-form="singular+short"} assumes that we select a hypothesis $h$ parametrised by $\theta$. Conceptually, we already *rethought* generalisation as determined only by the compressibility of the input ([1.2.1]). In this sense, the task is determined by the training dataset only.[^15] Thus, instead of a parametrised model, we will assume a parametrised unknown distribution $P(\rvD|\theta)$. In this context [@achille:2017emergence]:

::: theorem
Given $\rvD=(\rvX,\rvY),~ \rvD \sim P(\rvX, \rvY| \theta)$, and a representation $\rvW$ of $\theta$, s.t. $\rvY|\rvX \leftrightarrow W \leftrightarrow \theta$ form a Markov-chain [@achille:2017emergence]: $$\begin{aligned}
    H_{p,q}[\rvD|\rvW]=H_p[\rvD,\theta] + I[\theta;\rvD|\rvW] +
    \KL(p \parallel q)-I[\rvD;\rvW|\theta]
  
\end{aligned}$$
:::

::: proof
*Proof.* Notice that the output weight $\rvW$ of the training process can be seen as a random variable (that depends on the stochasticity of the initialisation, training steps, and the data); i.e. $\rvW$ is a representation of the dataset $\rvD$ and we can talk about $\IWD$.

First, we show that minimising $H_{p,q}[y|x]$ is equivalent to minimising $H_{p,q}[x,y]$ $$\begin{aligned}
    min H_{p,q}[y|x,w] = min H_{p,q}[\rvD |\rvW].
  
\end{aligned}$$ When a learning algorithm optimises the cross-entropy loss, it is effectively just minimising the KL-divergence, as the first term (entropy) is a constant:

$$\begin{aligned}
    &\text{from \eqref{eq:KL_decomposition}}\nonumber\\
    &min H_{p,q}[y|x,w] = min \left(\cancel{H_{p}[y|x, w]} + \E\KL(p(y|x, \theta)||q(y|x, w))\right).
  
\end{aligned}$$ The same happens in the minimisation of the cross-entropy of the joint dataset: $$\begin{aligned}
    &\text{from \eqref{eq:KL_decomposition}} \nonumber \\
    &min H_{p,q}[x,y|w] = min \left(\cancel{H_{p}[x,y, w]} + \KL(p(x, y, \theta)||q(x, y, w))\right)
  
\end{aligned}$$ Here, we show that the divergence of $y|x$ is the same as the divergence of joint distribution $x,y$, a step that was assumed by  @achille:2017emergence: $$\begin{aligned}
    &\KL(p(x,y) || q(x,y)) = \E_p \log \frac{p(x,y)}{q(x,y)}\\
    &= \E_p \log \frac{p(y|x)p(x)}{q(y|x)p(x)} = \E_p \left[\log p(y|x)p(x)- \log q(y|x)p(x)\right]\\
    &= \E_p \left[ \log p(y|x) + \cancel{\log p(x)} - \left( \log q(y|x) + \cancel{\log p(x)} \right) \right]\\
    &= \E_p \left[\log p(y|x) - \log q(y|x) \right]= \E_p \log \frac{p(y|x)}{q(y|x)}\\
    &= \KL(p(y|x)||q(y|x))
  
\end{aligned}$$ Therefore we can say that a learning algorithm minimises $H_{p,q}[\rvD|\rvW]$. $$\begin{aligned}
    &\text{from \eqref{eq:KL_decomposition}}\nonumber\\
    &H_{p,q}[\rvD|\rvW] = H_{p}[D, W] + \KL(P(\rvD, \theta) \parallel Q(\rvD, \rvW))
  
\end{aligned}$$ To prove that: $$\begin{aligned}
    H_{p,q}[\rvD|\rvW]=H_p[\rvD|\theta] + I[\theta;\rvD|\rvW] +
    \E\KL(p \parallel q)-I[\rvD;\rvW|\theta],
   
\end{aligned}$$ we just need to prove that: $$\begin{aligned}
    H_p[\rvD|\rvW] = H_{p}[\rvD, \theta] + I[\rvD|\rvW; \theta] - I[\rvD; \rvW|\theta].
  
\end{aligned}$$ This equivalence is clear with the help of the following Venn diagrams[^16]: $$\begin{aligned}
    &\vcenter{\hbox{\begin{venndiagram3sets}[shade=lightgray!30!white,labelA={$\rvD$},labelB={$\rvW$}, labelC={$\theta$}, tikzoptions={scale=.4}]]
          \fillANotB
        \end{venndiagram3sets}}}  &\ =\
    \vcenter{\hbox{\begin{venndiagram3sets}[shade=lightgray!30!white,labelA={$\rvD$},labelB={$\rvW$}, labelC={$\theta$}, tikzoptions={scale=.4}]]
          \fillANotC
        \end{venndiagram3sets}}}  &\ +\
    \vcenter{\hbox{\begin{venndiagram3sets}[shade=lightgray!30!white,labelA={$\rvD$},labelB={$\rvW$}, labelC={$\theta$}, tikzoptions={scale=.4}]]
          \fillACapCNotB
        \end{venndiagram3sets}}}  &\ -\
    \vcenter{\hbox{\begin{venndiagram3sets}[shade=lightgray!30!white,labelA={$\rvD$},labelB={$\rvW$}, labelC={$\theta$}, tikzoptions={scale=.4}]]
          \fillACapBNotC
        \end{venndiagram3sets}}} \\
    &H_p[\rvD|\rvW]  &\ =\ H_p[\rvD|\theta]  &\ +\ I[\rvD|\rvW; \theta]  &\ -\ I[\rvD; \rvW|\theta]
  
\end{aligned}$$ ◻
:::

Let us examine the cross-entropy decomposition: $$\begin{aligned}
  H_{p,q}[\rvD|\rvW]=\underbrace{H_p[\rvD|\theta]}_{\text{intrinsic error}} + \underbrace{I[\theta;\rvD|\rvW]}_{\text{sufficiency}} +
  \underbrace{\KL(p \parallel q)}_{\text{efficiency}}-\underbrace{I[\rvD;\rvW|\theta]}_{\text{memorisation}}
\end{aligned}$$

intrinsic error:

:   $H_{p}[\rvD|\theta]$ relates to the intrinsic error that we would find even if we knew $p_{\theta}$;

sufficiency:

:   $I[\theta;\rvD|\rvW]$ measures how much information of $\theta$ was compressed in the weights;

efficiency:

:   $\KL(p \parallel q)$ measures the efficiency[^17] of the representation, the number of additional bits we need to represent the input with $q(w|D)$ instead of using $p_{\theta}$ (see [\[sec:cross-entropy\]][6]);

memorisation:

:   $I[\rvD;\rvW|\theta]$ is the last and only negative term. It relates to overfitting and measures how much information about the dataset unrelated to $\theta$ is memorised in the weights.

The optimiser will try to increase memorisation because it is the only negative term. Thus,  @achille:2017emergence propose a naïve method to eliminate this proneness to overfitting: adding back the memorisation term in the lossThus,  [@achille:2017emergence]. $$\LL(\rvW) = H_{p,q}[\rvD|\rvW] + I[\rvD;\rvW|\theta]$$ To calculate $I[\rvD;\rvW|\theta]$ true distribution, $p_\theta$ is needed. Nevertheless, we are just trying to approximate $p_\theta$ with $q$ during training. Hence we are presented with the *chicken-egg* problem. Rather, one can add a Lagrangian multiplier to upper bound $I[\rvD;\rvW|\theta]$: $$\LL(\rvW) = H_{p,q}[\rvD|\rvW] + \beta^{-1} I[\rvD;\rvW] \label{eq:weights_ib} \qquad\text{Weights IB}$$ Remarkably, this has the same form as the IB Lagrangian, [\[IB_Lagrangian\]][7]. When $\beta=1$, [\[eq:weights_ib\]][8] reduces to the [ELBO]{acronym-label="ELBO" acronym-form="singular+short"} loss used in variational inference [@achille:2019phd p. 53].

## Two levels of representation {#sec:2_levels}

Some criticism on [IBT]{acronym-label="IBT" acronym-form="singular+short"} derive from a lack of rigour in explaining the fundamentals (see [\[sec:ibt_criticism\]][4]).

The crucial problem in [MLT]{acronym-label="MLT" acronym-form="singular+short"} is that we want to predict the behaviour (bound) of learning algorithms in future data while we can only access past performance.

This dichotomy translates to representation learning by two intertwined but different representations:

1.  the representation of a *dataset* (past data), a function that can be stored in memory for the later accomplishment of the task. It needs to keep useful information for future decisions without squandering resources in remembering spurious correlations or one-time events.

2.  the representation of an *input example* (current data): which need to keep the essence of the scene at hand;

Borrowing the terminology of Deep Learning, @achille:2019phd [@achille:2019weights] call these two levels of representation of *information in the weights* and *information in the activations*, respectively [@achille:2019phd; @achille:2019weights].

Several [IBT]{acronym-label="IBT" acronym-form="singular+short"} papers do not address this difference. In particular, some of the seminal work  [@tishby:2015; @shwartz-ziv:2017; @tishby:2017]. How can we minimise the information in the activations while we cannot access future data? There is a missing step.

Notice that we now have two Lagrangians. The original ([\[sec:activations_lagrangian\]][9]) and this new Lagrangian emerged from eliminating overfitting. $$\begin{aligned}
  \underset{\text{input}}{\rvX} \xrightarrow{\hspace*{1.4cm}}\underset{\text{activations}}{\rvZ}& \xrightarrow{\hspace*{1.4cm}}\underset{\text{label}}{\rvY}\\
  \underset{q(\rvZ|\rvX)}{\min\LL(W)} = H_{p,q}[\rvY|\rvZ]& + \beta^{-1} \IZX \qquad\text{Activations IB}\\
  \underset{\text{dataset}}{D} \xrightarrow{\hspace*{1.4cm}}\underset{\hspace*{.15cm}\text{weights}\hspace*{.15cm}}{W}& \xrightarrow{\hspace*{1.4cm}}\underset{\text{real distribution}}{P(\rvY|\rvX)\hspace*{.5cm}}\\
  \underset{q(\rvD|\rvW)}{\min\LL(W)} = H_{p,q}[\rvD|\rvW]& +\beta^{-1} \IDW \qquad\text{Weights IB}\\
\end{aligned}$$ []{#sec:duality label="sec:duality"}

Intuitively, there is a strong connection between *information in the weights* and *information in the activations*. $\IZX$, which measures the complexity of the activations representation, can be defined by the amount of weight in the network: low or zero weights will connect to the activations that are not in the optimal activation representation $z^*$, which minimises $\IZX$.

In  ,  @achille:2017emergence have proved that indeed there is a bound: $$\begin{aligned}
  \IZX \leq \IWD
\end{aligned}$$ As $\empIWD$ can be calculated, this development allows one to regularise the training explicitly. This explicit regularisation is what   proposes [@achille:2018infodropout].

Besides, even without calculating the information in the weights one can control it by injecting noise, which can be modulated from zero, no effect in the rate of the encoder, to the capacity of the channel, which leaves the encoder with no information left.

> *We know the past but cannot control it.\
> We control the future but cannot know it.*\
>
> ::: flushright
> --- Claude Shannon
> :::

## Shannon vs. Fisher Information {#sec:shannon_fisher}

We still have the problem that to calculate $\IXY$, we need to know $\PXY$. We can, however, bound the amount of information using Fisher Information [\[sec:fisher_information\]][10]. We use: $$\begin{aligned}
  \IXY=&\KL(P(\rvX,\rvY) \Vert P(\rvY)P(\rvX))\\
  =&\E_{\rvX}\KL(P(\rvY|\rvX)\Vert P(\rvY)), \label{eq:ExKL}
  
\end{aligned}$$ to rewrite [\[eq:weights_ib\]][11](Weights IB) as: $$\begin{aligned}
 \LL(W) = H_{p,q}(D|W) +\beta^{-1} \KL\left({\underbrace{Q(W|D)}_{\text{training output}}}\Vert {\underbrace{P(W)}_{\text{fixed prior}}} \right)
\end{aligned}$$

In other words, $\IWD$ is the divergence of the encoder $Q(W|D)$ and the expected prior averaging all possible datasets, the unknown distribution. If we change the assumption [1.3.2], [\[free-D\]][12], and assume that the unknown distribution is an isotropic Gaussian[^18], the information in the weights when $W_*$ is minimal, is given by: $$\begin{aligned}
  \KL{Q(W|D)}{P(W)}=\frac{1}{2} \left( \log |F n|+\cancel{\log \lambda^2I} + \cancel{\frac{W_*^2}{\lambda^2I}} \right) ,
\end{aligned}$$ where the cancelled terms are the ones that do not depend on $Q(W|D)$ and can be ignored, $\log |F|$ is the log-determinant of Fisher Information Matrix of the weights, and $n$ is the number of samples in the dataset.

This assumption is quite interesting as it gives us an analytical and fast calculation of a bound to $\IWD$: $$\begin{aligned}
\IZX \leq \IDW \leq \log |F(W^*)| \label{bounds}
\end{aligned}$$ Even if the unknown distribution $P(D)$ is not an isotropic gaussian, we can think that near optima, it approximates one. We can arrive at the same result by approximating the Hessian with a Taylor expansion.

## Connection to Variational Autoencoders {#sec:vae}

@achille:2018infodropout show how the previous development relates with [VAEs]{acronym-label="VAE" acronym-form="plural+full"}  [@achille:2018infodropout]. [VAEs]{acronym-label="VAE" acronym-form="plural+full"}  [@kingma:2014] aim to reconstruct, given a training dataset $\mathcal{D}={x_i}$, a latent variable $z$. The paper proposes that this can be thought as generating $z$ through some unknown generative process $p_{\theta}(x|z)$. In practice, this is done by minimising: $$\begin{aligned}
              \LH = \frac{1}{N} \sum^{N} \E_{z \sim p_{\theta}(z|x_i)} - \log p_{\theta}(y_i|z) +  \KL (p_{\theta}(z|x_i)||\textstyle \prod_i p_{\theta}(z_i)).
          
\end{aligned}$$ This minimisation is performed through sampling using SVGB [@kingma:2014]. It is clear by the formulation that VAE is equal to [\[eq:weights_ib\]][11](Weights IB) where $\beta=1$.

## Connection to PAC-Bayes

@achille:2018infodropout also relate their work with PAC-Bayes [@achille:2018infodropout]. From [@mcallester:2013 Thrm 2], $$\begin{aligned}
                  &\forall \text{(fixed)} \lambda > 1/2,\ p(w),\ q(w|D), \nonumber \\
                  &\E_D[L^{\text{test}}q(w|D)] \leq \nonumber\\
                  &\frac{1}{N(1-\frac{1}{2\lambda})} \left(H_{p,q}(y|x,w)+ \lambda L_{\text{max}}\E_D[\KL[q(w|D)||p(w)] \right)\label{eq:mcallester_thrm2}
                
\end{aligned}$$ where $L_{\text{max}}$ is the maximum per-sample loss function. The [RHS]{acronym-label="RHS" acronym-form="singular+full"} coincides, modulo a constant, with [\[eq:weights_ib\]][11] if we use $q(w)$ instead of $p(w)$. Since $$\begin{aligned}
                  &E_D[\KL[q(w|D)||q(w)]] \nonumber\\
                   =&E_D[\KL[q(w|D)||p(w)]] - \KL[q(w)||p(w)]\\
                \leq &\E_D[\KL(q(w|D)||p(w))]\label{eq:pacbayes},\tag{PAC-Bayes}
                
\end{aligned}$$ the sharpest PAC-Bayes upper bound to the test error is obtained when $p(w)=q(w)$, in which case, [\[eq:pacbayes\]][13] reduces (modulo a constant) to the IB Lagrangian of the weights. Unfortunately, the marginal $q(w)$ of the weights is not tractable, as already stated. To circumvent this problem, we consider instead that the sharpest PAC-Bayes upper bound that can be obtained using a tractable factorised prior $p(w) = \tilde{q}(w)=\prod_i q(w_i)$.[^19]

### Relation to @dziugaite:2017 bounds

We notice that this relation was independently explored by @dziugaite:2017, who worked on the hypothesis that SGD finds good solutions only if they are surrounded by a large volume of good solutions [@dziugaite:2017], if so, the expected error rate of a classifier drawn at random from this volume should match that of the SGD solution. bounds the expected error of a classifier chosen from a distribution $Q$ in terms of the $\KL$ divergence from a prior $P$, and if the volume of good solutions is large, and not too far from the mass of $P$, we obtain a good bound.

They use [SGD]{acronym-label="SGD" acronym-form="singular+short"} to optimise the PAC-Bayes bound on the error rate of a stochastic neural network, a [DNN]{acronym-label="DNN" acronym-form="singular+short"} that represents a stochastic mapping $p(y|x)$. The objective function is the sum of

-   the empirical *surrogate*[^20] loss averaged over a random pertubation of the [SGD]{acronym-label="SGD" acronym-form="singular+short"} solution;

-   a generalisation error bound that acts as a regulariser.

Recall :

From this, we can say that @dziugaite:2017 are solving an instance of the IB learning problem.

Moreover, their objective can be written as  [@dziugaite:2017 sec. 6]: $$\begin{aligned}
  \min \E_{W \sim \NN} L(W,S) + [w-w_0]^\top \diag(s) [w-w_0]
\end{aligned}$$ where $s$ is the score function. In other words, they are calculating the diagonal of the Fisher Information Matrix as a regularizer.

## Evidence of the IB limit in a human learned task {#sec:efficient_color_naming}

 @zaslavsky:2018 [@zaslavsky:2018] had the sagacious idea of using the IB method to analyse anthropological evidence.

We have already established that intelligent agents, whether artificial or biological, need language to represent a complex environment. Natural languages reflect different solutions to this problem. The current most accepted theory in Anthropology and Linguistics suggest that while languages vary to accommodate language-specific needs (due, for example, to variations in the environment), they evolve into efficient representations [@zaslavsky:2018]. Although not explicit in [@zaslavsky:2018], it is evident that the evolution of natural languages can be seen as a learning process for the task of efficient communication by a society.

The paper analyses natural languages in the context of colour naming. It is based on the World colour Survey (WCS), "*a large colour-naming database obtained from informants of mostly unwritten languages spoken in pre industrialised cultures that have had limited contact with modern, industrialised society*" [@lindsey:2009]. Assuming that each colour of WCS corresponds to a specific meaning; it formulates the problem of colour naming in an information-theoretical perspective analogous to the IB problem setting [@tishby:1999]:

With that formulation, it is possible to calculate the IB limits and analyse the different languages colour-naming solutions in this framework:

In [\[fig:efficient_colornaming_result\]][14], it is possible to see evidence that languages efficiently compress ideas into words by optimising the tradeoff between complexity and accuracy of the lexicon according to the IB principle.

This analysis corroborates the current theory on human language evolution. Furthermore, the drive for information-theoretical efficiency explains why human languages categorise colour as they do and may also apply to learning in general.

The hypothesis is that languages evolve to become more efficient in a tradeoff between conciseness (complexity, generalisation) and precision. The prediction capability is just an expected consequence of an efficient representation of meaning. The conciseness of the representation of knowledge, given an acceptable error margin, is a proxy of the agent's intelligence. The IB limit is an epistemic limit that is valid for machines, humans and aliens.

## Concluding Remarks {#sec:conclusion-ibrl}

This chapter presented the [IBT]{acronym-label="IBT" acronym-form="singular+short"} as a general representation learning theory (not specific to Deep Learning). The bulk of this chapter is based on works by Stefano Soatto and Alessandro Achille and their prolific research group  . , in particular, has been one of the biggest influences in this dissertation. It was presented in the same workshop[^21] where Tishby presented [IBT]{acronym-label="IBT" acronym-form="singular+short"} for the first time.

 @achille:2017emergence accomplishments in this chapter were threefold:

1.  It explained the emergence of invariance (generalisation) and disentanglement in the proposed learning setting.

2.  It addressed one of the weaknesses of [IBT]{acronym-label="IBT" acronym-form="singular+short"}: the confusion about past and future data. [MLT]{acronym-label="MLT" acronym-form="singular+short"} provides rigorous guarantees for future performance (test time) based on the past data (training time). Conversely, several of the initial [IBT]{acronym-label="IBT" acronym-form="singular+short"} papers were not clear with what is happening during training and how it is different in test time.

3.  It showed the crucial role of noise and how it can be controlled in favour of generalisation.

4.  It demonstrated that the information in the weights, despite being difficult to measure, can be bounded by the Fisher information Matrix: $$\begin{aligned}
        \IZX \leq \IDW \leq \log |F(W^*)| \leq \log |F(W)|
      
    \end{aligned}$$

Noteworthy, the Deep Learning setting does not seem to correspond to the conditions of , as   has shown that Deep Learning converges even in the absence of a regulariser in the loss function.

### Assumptions {#assumptions}

1.  [MLT]{acronym-label="MLT" acronym-form="singular+short"} assumptions

2.  Information is what changes belief.

3.  [IBT]{acronym-label="IBT" acronym-form="singular+short"} for Representation Learning assumptions:

    i.  The random variables $\rvX$, $\rvY$ and $\rvZ$ are **discrete**;

    ii. $\rvY \to \rvX \to \rvZ$ form a Markov-chain;

    iii. $\sA_{\rvX}$, $\sA_{\rvY}$ and $\sA_{\rvZ}$ are **finite sets**;

    iv. **No assumption on $\DD=\PXY$**.

    v.  **$\DD=\PXY$ is unknown at the training stage**.

    vi. **$\DD=\PXY$ is fixed:** the ordering of examples in the sample is irrelevant.

    vii. $\rvX$ is is i.i.d. sampled.

    viii. The encoder and the decoder are **stochastic** mappings.

    ix. the loss function is in the form of a [IB]{acronym-label="IB" acronym-form="singular+short"} Lagrangian (), $\LL(\rvW) = H_{p,q}[\rvD|\rvW] + \beta^{-1} I[\rvD;\rvW]$ has a regulariser term that penalises the memorisation of the dataset.

We took the liberty to add the assumption that constrains the problem to finite alphabets (discrete random variables). Unfortunately, with few exceptions, the literature on [IBT]{acronym-label="IBT" acronym-form="singular+short"} does not underscore this constraint nor, alternatively, demonstrate why one can use differential entropy.[^22]

[^1]: *"What object does this picture represent?"* Object Classification is the task of assigning a category (a label) for an image.

[^2]: Note that $\rvY \neq \hat{\rvY}$. Here, the Markov chain is from the unknown target variable $\rvY$ to the representation $\rvZ$ through the input. See [\[fig:ibt_learning_setting\]][15].

[^3]: Minimal representations are generally equated to low-dimensional data. However, as we have exposed in [\[ch:information\]][16], a high dimensional representation can have little information. Thus, for example, sparse representations that force most of its bits to be zero are high-dimensional low-informational representations.

[^4]: Nuisances are factors of variation that affect data, but are otherwise irrelevant for the task.

[^5]: Disentanglement and minimality also simplify the subsequent inference (decoding).

[^6]: For consistency with [\[ch:mlt\]][17], we will repeat some definitions in this section.

[^7]: Notice that here $\vy_i$ is not the label but a vector that represents the probability of each label.

[^8]: We could use an ergodic process, but for simplification we will use i.i.d. sampling.

[^9]: Notice that given the Markov chain $\rvY \leftrightarrow \rvX \leftrightarrow \rvZ$, due to reparemetrisation invariance (), a deterministic mapping of the data does not throw out information, i.e.  let $f:\aXX \to \aYY$ be deterministic, $I[f(\rvX);\rvY]=\IXY$.

[^10]: This assumption is not strictly required, as it can be derived. The only reason to keep it here is to make the comparison of different problem settings easier.

[^11]: Prior to the publishing of  , there was no known algorithm to minimise the IB Lagrangian for discrete $\rvX$ and $\rvY$ with large state spaces or non-Gaussian continuous joint distribution.

[^12]: $p_\theta(y|x)=\sum p_\theta(z|x) p_\theta(y|z)\\p(y|x,\theta^*)=p(y|x)$

[^13]: The reference constraint their findings to [DNNs]{acronym-label="DNN" acronym-form="plural+short"} optimised with [SGD]{acronym-label="SGD" acronym-form="singular+short"}. We regard the result more general than that.

[^14]: This insight allowed  @alemi:2016 [@achille:2018infodropout] independently develop basically the same algorithm for estimating mutual information for any distribution using [DNNs]{acronym-label="DNN" acronym-form="plural+short"} .

[^15]:  @achille:2019phd defines the task as the dataset distribution for which we only have one sample .

[^16]: Our assumptions guarantee that all information measures in the diagram are positive, thus there is no problem in using the Venn diagram in this case.

[^17]: It relates to generalisation, as additional bits of information can correlate to noise.

[^18]: An isotropic Gaussian is one where the covariance matrix is represented by $\Sigma=\lambda^2 I$.

[^19]: This assumption is also made in the [MDL]{acronym-label="MDL" acronym-form="singular+short"} framework.

[^20]: The *surrogate* loss is the logistic loss, which is differentiable.

[^21]: Deep Learning: Theory, Algorithms, and Applications. Berlin, June 2017 <http://doc.ml.tu-berlin.de/dlworkshop2017>

[^22]: For example,  @alemi:2016 use differential entropy but do not address the fact that the IB Principle restrain itself to discrete random variables .

  [1]: #ch:ib_and_dl {reference-type="ref" reference="ch:ib_and_dl"}
  [1.1]: #sec:why_representation_learning {reference-type="ref" reference="sec:why_representation_learning"}
  [1.2]: #sec:desiderata {reference-type="ref" reference="sec:desiderata"}
  [1.6]: #sec:2_levels {reference-type="ref" reference="sec:2_levels"}
  [1.3]: #sec:ibt_learning_problem {reference-type="ref" reference="sec:ibt_learning_problem"}
  [1.10]: #sec:efficient_color_naming {reference-type="ref" reference="sec:efficient_color_naming"}
  [2]: #ib_problem_setting {reference-type="ref" reference="ib_problem_setting"}
  [3]: #sec:IB_principle {reference-type="ref" reference="sec:IB_principle"}
  [4]: #sec:ibt_criticism {reference-type="ref" reference="sec:ibt_criticism"}
  [5]: #activations_ib {reference-type="ref" reference="activations_ib"}
  [1.2.1]: #sec:invariant_if_minimal {reference-type="ref" reference="sec:invariant_if_minimal"}
  [6]: #sec:cross-entropy {reference-type="ref" reference="sec:cross-entropy"}
  [7]: #IB_Lagrangian {reference-type="ref" reference="IB_Lagrangian"}
  [8]: #eq:weights_ib {reference-type="eqref" reference="eq:weights_ib"}
  [9]: #sec:activations_lagrangian {reference-type="ref" reference="sec:activations_lagrangian"}
  [10]: #sec:fisher_information {reference-type="ref" reference="sec:fisher_information"}
  [11]: #eq:weights_ib {reference-type="ref" reference="eq:weights_ib"}
  [1.3.2]: #sec:ibt_learning_assumptions {reference-type="ref" reference="sec:ibt_learning_assumptions"}
  [12]: #free-D {reference-type="ref" reference="free-D"}
  [13]: #eq:pacbayes {reference-type="ref" reference="eq:pacbayes"}
  [14]: #fig:efficient_colornaming_result {reference-type="ref" reference="fig:efficient_colornaming_result"}
  [15]: #fig:ibt_learning_setting {reference-type="ref" reference="fig:ibt_learning_setting"}
  [16]: #ch:information {reference-type="ref" reference="ch:information"}
  [17]: #ch:mlt {reference-type="ref" reference="ch:mlt"}
