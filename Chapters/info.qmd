# Information Theory {#ch:information}

This chapter derives Shannon Information from Probability Theory, explicates some implicit assumptions in the usage of Shannon Information, and explains basic Information Theory concepts.

## From Probability to Information {#sec:prob2info}

In [\[sec:anatomy_ia\]][1], we exposed that an agent updates its model of the environment from sensory data, experience. We have also shown how this update happens; a sceptical agent *proportions her beliefs to the evidence* according to Bayes' theorem.

The amount of this update on knowledge is not uniform. Some experiences are more valuable than others, i.e. some evidence will produce a more considerable change in the agent's knowledge, leading to a greater impact in her future actions. We say that those experiences are more informative.

::: definition
[]{#def:information label="def:information"} **Information** is what changes belief [@sowinski:2016; @caticha:2008].
:::

Let us say that an agent's *prior* belief in a statement $S$ is $P(S)$.[^1] After experiencing some evidence $\re$, her *posterior* set of beliefs is updated to incorporate the evidence, $P(S|\re )$.[^2] The prior and the posterior are related by the product rule ([\[sec:independent_events\]][2]) [@sowinski:2016]: $$\begin{aligned}
    \underbrace{P(S|\re)}_{Posterior}&= \frac{P(\re | S )}{P(\re)}\cdot \underbrace{P(S)}_{Prior}
\end{aligned}$$

We shall call this ratio by which prior and posterior are related as the likelihood ($\LH$):

This update procedure can be generalised to a set of experiences. Consider a sequence of experiences: $E = \{\re_t\}_0^T$ $$p(S|\re_0) \to p(S|\re_0 \land \re_1) \to \cdots \to p(S|\re_0 \land \re_1 \land \cdots \land \re_T)$$ But according to the Cox axiom [\[sec:desiderata_language_sceptical, consistency\]][3], an agent may partition her experiences in any way she chooses, and this does not affect her final belief [@sowinski:2016]. Therefore[^3]: $$\begin{aligned}
    \LH(\re; S) &= \frac{\text{Posterior}}{\text{Prior}} = \frac{P(S | \re )}{P(S)} = \frac{P(\re | S )}{P(\re)} \label{eq:likelihood}\\
    P(S|\re ) &= \LH(\re; S)\cdot P(S).
 
\end{aligned}$$ Simply by observing equation [\[eq:likelihood\]][4], we can conclude that if information ($i$) is what changes belief, information and likelihood must be related to one another: $$\begin{aligned}
    i_S(\re) = f(\LH(\re; S)).
\label{eq:information_as_function_of_likelihood} 
\end{aligned}$$

Moreover, if an experience does not change a belief ($\LH(\re;S)=1$), it contains no information: $f(1)=0$.

We also hope that when the likelihood changes by an infinitesimal amount, information does not change discontinuously, so $f$ is continuous.

The information gathered from independent *events* must reflect the commutativity of Cox's axiom [\[consistency\]][5] [\[axiom:order\]][6].

Let $\LH_1 = \LH (e_1; S)$ and $\LH_2 = \LH (e_2; S)$, information must satisfy the functional constraints [@sowinski:2016]: $$\begin{cases}
    f(\LH_1 \land \LH_2) &= f(\LH_1) + f(\LH_2)\\
    f(1) &= 0\\
    f &\text{is continuous.}
\end{cases}$$ This functional form can be solved, and its solution is [@caticha:2008]: $$\begin{aligned}
    f = A \cdot \ln{\LH(\re; S)} \therefore \nonumber\\
    i_S(\re) = A \cdot \ln{\LH(\re; S)}.\label{eq:i_equals_log_LH}
\end{aligned}$$ From equations [\[eq:i_equals_log_LH\]][7] and [\[eq:likelihood\]][4], $$\begin{aligned}
    i_S(\re) &= A \cdot \ln{ \frac{P(S | \re )}{p(S)}}\\
    i_S(\re) &= A \cdot \ln{ P(S|e )} - A \cdot \ln{p(S)}.
\label{eq:is_ln} 
\end{aligned}$$ The constant $A$ allows us to use any base $b$ in the logarithm: $$\begin{aligned}
    A = \frac {1}{\ln b} \to i_S(\re) = \log_b P(S|\re ) - \log_b P(S).
\end{aligned}$$ We can argue that the amount of information gained by the agent about the world is equivalent to some amount of *hidden information* $h$ that was revealed to the agent by the *event* $e$.

Hence, $i_S(e)=-\Delta h(e)$, from eq. [\[eq:is_ln\]][8]: $$\begin{aligned}
    i_S(e) &= \log{ P(S|e )} - \log {P(S)} \\
    i_S(e) &= - \left[ \biggl(\underbrace{-\log{P(S|e )}}_{h(S|e )}\biggr) - \biggl(\underbrace{-\log{P(S)}}_{h(S)}\biggr) \right] \\
    i_S(e)&=-\Delta h(e).
\end{aligned}$$

Delightfully, our definition of *hidden information* that reduces the uncertainty of the agent, and emerged from our definition of information, $$\begin{aligned}
    h (S) = - \log P(S)
\end{aligned}$$ is equivalent to Shannon's self information[^4]: $$\begin{aligned}
    I[\src] = - \log p(\rs)
\end{aligned}$$

In [IT]{acronym-label="IT" acronym-form="singular+full"}, self-information is defined as the entropy contribution of an individual message (or symbol); in other words, how much an individual *event* can attain uncertainty reduction. This uncertainty reduction is what we derived.

**Shannon's information can be derived from probability theory.**[]{#sec:probability2information label="sec:probability2information"}$\qedhere$

## Shannon's Mathematical Theory of Communication

[IT]{acronym-label="IT" acronym-form="singular+full"} has an identifiable beginning: Shannon's $1948$ paper  was a giant leap towards understanding communication and defining *information*.[^5] Despite his acknowledging of the influence from previous works by pioneers such Harry Nyquist and Ralph Hartley, it was Shannon's unifying vision that revolutionised communication and provided a *blueprint* for the information age [@aftab:2001]. His theory defines unbreachable limits, the *laws of information* [@stone:2015]:

i.  There is an upper limit, the **channel capacity**, to the amount of information that can be communicated through a channel;

ii. **Noise** reduces the **channel capacity**;

iii. There is an encoding that allows **lossless** communication trough a **noisy channel**.

The idea of transmitting information with zero error through a noisy channel is not intuitive, and its theoretical proof was an unexpected result. In the following sections, we will explain the concepts of [IT]{acronym-label="IT" acronym-form="singular+short"} that allow us to comprehend these *laws of information*.

### The communication problem setting {#sec:communication_problem_setting}

Shannon deliberately chose not to deal with fuzzy concepts as intelligence or meaning:

> ***The fundamental problem of communication is that of reproducing at one point either exactly or approximately a message selected at another point.** Frequently, the messages have meaning; that is, they refer to or are correlated according to some system with certain physical or conceptual entities. These **semantic aspects of communication are irrelevant to the engineering problem**. The significant aspect is that the actual message is one **selected from a set** of possible messages.*

*--- Claude Shannon, [@shannon:1948]*

Conceptually, this setting can be explained as follows ([\[fig:communication_setting\]][9]): [^6]

The Source $\rvS$:

1.  selects a message $\srcsymb$ from a set of possible messages $\srcalph$.[^7]

2.  The encoder $\encsymb$ encodes the message $\srcsymb$ into a string of symbols $\encsymb$, the signal; and

3.  transmits this string of inputs $\encsymb$ through a noisy channel $p(\decsymb|\encsymb, \rvN)$.

In the Destination:

1.  The decoder $\dec := p(\decsymb|\encsymb, \rvN)$ receives a string of symbols $\decsymb$,

2.  decodes the string $\decsymb$ into the most probable message $\hat{\srcsymb}$.

## Information

The reason for communication is to change another agent's behaviour. In other words, *communication either affects the conduct of the recipient, or it is like it has never happened* [@shannon:1948 p.100]. We have already established ([1.1], definition [\[def:information\]][10]) that *information is what changes belief*; thus, changes an agent's conduct. So, **communication is transmitting information**.

Noteworthy, information is independent of the *encoding* or the chosen channel. Thus, one can use any language (English, Portuguese, music, images, dance) and any transmission medium (letter, telegraphy, microwaves) that the transmitted information remains the same.

To simplify, Shannon constrained semantics to the act of choosing a message from a set of finite possibilities. A source (a person, a machine or a phenomenon) that always sends the same message never surprises the receiver, and the message carries no information. On the contrary, a source that sends symbols at random is impossible to predict, and, therefore, every message carries maximal information.

Therefore, *information is a measure of freedom of choice in selecting the message* [@shannon:1949 p.100]. In other words, it is a measure of surprisal or uncertainty reduction.

In the aforementioned famous paper, Shannon limited to say that mathematically, if the set of possible messages $\srcalph$ is finite, any function of the size of this set $f(|\srcalph|)$ is a measure of information and that the logarithmic function is a natural choice. We shall expand on this idea.

### A guessing game {#guessing_game}

Imagine a number from 1 to 1000. Let us assume that you picked the number at random. Thus, each number in the range had the same chance of being chosen, $\frac{1}{1000}$. How many questions are needed to guess the number correctly? Well, it depends on what are the allowed answers. One could ask:

-   How many hundreds the number have?

-   Then, how many tens the number have?

-   Then, how many units?

In this case, the number of questions needed is three, the height of the tree in [\[fig: branching\]][11], because we allowed each answer to be a *digit*; therefore, the *branching factor* $b$ of the decision tree was 10. It is easy to notice that the tree's height is $\log_b (1000)$.

It is now clear what Shannon meant by saying that the logarithmic function was the natural measure of information. The logarithm will give the decision tree's height (number of questions) based on the number of possible answers (the logarithm base). The branching factor is just a measurement unit and can be chosen arbitrarily.

The smallest branching factor is 2, a *bit*. So, one bit is the amount of information that resulted from choosing between two equally likely options.

To solve the same guessing game with *bits*, with yes or no questions, one proceeds with a binary search, and in the worse case it will need $\log_{2}(1000)=\frac{\log_{10}(1000)}{\log_{10}(2)}\approx 9.96 \therefore 10$ questions.

How about if the choice was among not equally likely options? Let us examine the simplest case of an unfair coin, which turns *heads* $75\%$ of the time.

Here, we expect the outcome to be *heads*, so if it turns *tails*, we get surprised. Before the coin flip, we were 25% certain (our belief measure) that the *experiment* would turn *tails*. If it turns *tails*, our certainty reaches 100%, growing by a factor of $\frac{1}{0.25}=4$. So it is reasonable to think that our uncertainty of the *tails* outcome decreased by a factor of 4 as well. We were 75% certain that the *experiment* would turn *heads*. If it in fact turns *heads*, our uncertainty of the *heads* outcome decreased by a factor of $\frac{1}{0.75}\approx 1.\overline{3}$. How do we transform this uncertainty reduction factor[^8] to a measure in bits? In other words, how do we measure in bits the information gained by unveiling an outcome?

Notice that 1 *bit* is the amount of information that reduces uncertainty from 2 possible states to 1, a factor of 2. Also, 2 bits of information reduce the uncertainty from the 4 possible representable states with 2 bits to 1, a factor of 4. So, if an outcome has probability $p(\encsymb)$: $$\begin{aligned}
    \frac{1}{p(\encsymb)} \text{ factor}  \implies \log_2 \frac{1}{p(\encsymb)} \text{bits} = - \log_2 p(\encsymb) \text{ bits}
\end{aligned}$$

If the factor is a measure of the reduction in freedom of choice, the factor is the information gained by knowing the *experiment's* outcome. Thus, this factor is known as **self-information** or information content of an outcome[^9]:

::: definition
[]{#def:surprisal label="def:surprisal"} The **information content, self-information, surprisal**, or **Shannon information** of a particular outcome $\encsymb$ of an *experiment* is defined as: $$\begin{aligned}
        I[\encsymb] = h[\encsymb]= -\log p(\encsymb)\\
        \tag{information content of outcome}
    
\end{aligned}$$
:::

As we already had derived in [\[sec:probability2information\]][12].

### Entropy {#sec:entropy}

In practice, however, we are not usually interested in the information of a particular outcome, but in how surprised, on average, we will expect to be with the entire set of possible outcomes.

::: definition
[]{#entropy_definition label="entropy_definition"} The entropy $H[\rvX]$ of a random variable $\enc$ is defined to be the average Shannon information content of its possible outcomes: $$\begin{aligned}
        H[\rvX] \eqdef \E_p \frac{1}{\log p(\encsymb)} = -\sum_{\encsymb \in \encalph} p(\encsymb) \log p(\encsymb) \text{ bits/symbol}.
    \label{eq:entropy} 
\end{aligned}$$
:::

Entropy can be seen in two ways[^10]:

1.  as the quantity of information "produced" by the source [@shannon:1949 p.18].

2.  as a measure of *uncertainty* or lack of pattern.

Average information shares the same definition as Entropy; therefore, to know whether a quantity is information or Entropy depends on whether it is given or taken [@stone:2015]. In other words, uncertainty reduced is information gained, and vice-versa. If a random variable $\enc$ is very uncertain, it has high Entropy. If we are told the outcome of the variable $\rvX = \encsymb_j$, we have been given information equal to the uncertainty we had. Thus, receiving an amount of information is equivalent to having the same amount of Entropy taken away.

## The source

In the problem setting proposed by Shannon, the source generates a message, symbol by symbol. The choice of each symbol depends on the "preceding choices as well as the particular symbols in question" [@shannon:1949 p.10].

A mathematical model that follows this description is known as a *stochastic process*. A stochastic process can represent any discrete source. "Conversely, any stochastic process may be considered a discrete source" [@shannon:1949].

::: definition
A **stochastic (or random) process** is a set of random variables indexed by a variable $i \in \Natural$ (usually representing time): $$\begin{aligned}
        {\src_i}, i \in \Natural \\
        \tag{Stochastic Process}
    
\end{aligned}$$
:::

In the original formulation, Shannon modelled the source as a stochastic process indexed by time. He thought the source as an entity that emits a specific rate, amount of information (bits) per period (seconds): $$\begin{aligned}
\srcrate \eqdef \frac{H[\src]}{T_{\src}} \frac{\text{ bits}}{\text{ second}}
\end{aligned}$$ where $T_{\src}$ is the average time in seconds of transmitting a symbol. For simplification sake, from now on we will just say that the source rate is: $$\begin{aligned}
    \srcrate = H[\src] \text{ bits/symbol}~ \text{~or~} ~H[\src] \text{ bits/transmission}
    
\end{aligned}$$

### Markov chains

More specifically, Shannon proposed using a special kind of stochastic process called an *ergodic Markov chain* to model the source.

::: definition
An order-k **Markov chain** is a stochastic process that satisfies the following property: $$\begin{aligned}
        P(\src_i|\src_{i-1}, \src_{i-2}, \cdots, \src_{i-k})=P(\src_i|\src_{i-1}, \src_{i-2}, \cdots, \src_1)
    
\end{aligned}$$ The **ergodic** property means statistical homogeneity [@shannon:1949]: its statistical properties can be deduced from a single, sufficiently long, random sample of the process.
:::

An order-k ergodic Markov chain is a process with a memory of $k$ states. By modelling the source as an ergodic Markov chain, Shannon showed that his theory not only works for phenomena that can be modelled as i.i.d. random variables. The source can behave like a chain of random variables $\{\src\}$, each representing an outcome $\srcsymb \in \srcalph$ that are dependent on each other, as long as the sequence produced is longer than the number of symbols needed to the Markovian process achieve its stability.

## Data compression: encoder/decoder

An encoder transforms information into data. For example, the same information can be transformed into an audio file with spoken English, a piece of writing in Portuguese, or even an image. These encodings represent the information uniquely and differ in the amount of data (*bits*) they use ([\[fig:360k, fig:27k, fig:5k\]][13]).

::: sidecaption
Different representations of a cat and their encoding sizes in bits.
:::

An analogy with natural languages can better explain this idea. Languages encode ideas into words in different ways. For example, while in English *"to be"* is universal, Portuguese has two different verbs: *"ser"* and *"estar"*; the first for permanent, unchanging cases; the second for temporary situations such as mood or weather. At the same time, similar or identical meanings appear in unrelated languages [@zaslavsky:2018].

Thus, a message in a natural language can be translated (encoded) to another language, and both messages will hardly have the same number of words, characters, or size in *bits*: $$\begin{aligned}
    \srcblk=\{\src_1, \cdots, \src_n\}&~\xrightarrow{~encoding:~X(\rvS) ~}~
    \{\enc_1,  \cdots, \enc_k\}=\encblk.\\
    \encblk=\{\enc_1, \cdots, \enc_k\}&~\xrightarrow{~decoding:~X^{-1}(\rvX)~}~
    \{\src_1,  \cdots, \src_n\}=\srcblk.
\end{aligned}$$

Besides, some symbols are more important in a message: "Mst nglsh spkrs wll ndrstnd ths phrs wtht vwls[^11]". Here we created *codewords* for words in English that a receiver can understand by the context (and certainly if she has a *codebook*[^12]).

Shannon's source coding theorem is about encoding messages efficiently, a form of data compression [@stone:2015]. Here we present some definitions that will help us understand the theorem later.

::: definition
A **(n, k) block code**, also known as a codebook, is a set of $n$ codewords represented by a sequence of $k$ bits: $$\begin{aligned}
\{\enc^k(1),\enc^k(2),...,\enc^k(n)\},~\enc^k(i) \in \encalph^k, ~n \in \Natural.
\end{aligned}$$
:::

::: definition
Let $\srcblk$ be a block of $n$ random variables, representing consecutive symbols $\src_i \in \srcalph$ emitted by the source. A **binary block encoder** $\enc$ is a function: $$\begin{aligned}
    \enc:~ \srcalph^n \to {\{0,1\}}^k
\end{aligned}$$ that "translates" the block of source symbols (the message) into a code $\encblk$ of $k$ bits, using a $(|\srcalph^n|, k)$ code: $$\begin{aligned}
    \enc(\srcblk)=\{\encsymb_1,  \cdots, \encsymb_k\}=\encvec \in {\{0,1\}}^k
\end{aligned}$$
:::

::: definition
The rate $\encrate$ of a binary block encoder is: $$\begin{aligned}
        \encrate = \sR_{(n, k)} = \frac{\log |\srcalph^n|}{k}=\frac{n}{k}\log |\srcalph|~\frac{\text{bits}}{\text{symbol}}
        
\end{aligned}$$
:::

Shannon's source coding theorem ([1.5.6]) is essentially about data compression. The encoding process yields inputs with a specific distribution $P(\rvX)$. The shape of this distribution[^13] determines its entropy $H[\rvX]$ and, therefore, how much information each input carries [@stone:2015].

::: sidecaption
Entropy of the source vs. coding capacity.
:::

[]{#fig:tube label="fig:tube"}

Shannon proved a relation between the source's entropy and its optimal encoding (this relation will be shown in [1.5.6]). The source's entropy is a lower bound on the minimum bits/symbol needed to encode it. The intuition is simple, imagine the Entropy of the source as a "tube "(see [\[fig:tube\]][14]). The capacity of the tube is the rate of bits/symbol we expect from the source. The encoder is a connection to the tube.

If we use fewer bits than the entropy to encode it, we lose information (see [\[fig:information_loss\]][15]). Conversely, if we use more bits than the entropy, we are wasting resources (see [\[fig:resource_waste\]][16]).

### An encoding example

Let us use an example to illustrate better this crucial concept in [IT]{acronym-label="IT" acronym-form="singular+short"}[^14]. Imagine building a weather station that sends the moment weather condition to a distant control room. Also, there are eight weather conditions in which we are interested. In this case, a message transmits one symbol from $\srcalph$. $$\begin{aligned}
    \srcalph = \{\rw_0, \rw_1, \rw_2, \rw_3, \rw_4, \rw_5, \rw_6, \rw_7\}
\end{aligned}$$ How can we encode these weather conditions?

### Raw bit content

The first idea is to enumerate $\srcalph$ in binary, using 3 bits/symbol. $$\begin{aligned}
    \encalph = \{&\encsymb_0=000, \encsymb_1=001, \encsymb_2=010, \encsymb_3=011, \nonumber \\
    &\encsymb_4=100, \encsymb_5=101, \encsymb_6=110, \encsymb_7=111\} \label{example_encoding}
\end{aligned}$$ This encoding provides a model of the source that has maximum entropy (all outcomes are equiprobable, thus have the same encoding size)[^15]: $$\begin{aligned}
    p(\encsymb_i)&=\frac{1}{|\encalph|}, \forall i \in [0,7]\\
    H[\enc]&= - \sum^{|\encalph|}\frac{1}{|\encalph|} \log \frac{1}{|\encalph|}\\
    &= \log |\encalph|.
\end{aligned}$$ Is this a good encoding?

### Maximum Entropy Principle

If all information we have is how many weather conditions are there, the size of the source alphabet, the best model is the one that conveys this information and has maximum Entropy, it makes no further assumptions. This maximally entropic model has the worst-case scenario for the average number of questions needed to find out which outcome is the right one: $$\begin{aligned}
    P({\src}) = \{&p_0=\tfrac{1}{8},p_1=\tfrac{1}{8},p_2=\tfrac{1}{8},p_3=\tfrac{1}{8}, \nonumber \\
    &p_4=\tfrac{1}{8},p_5=\tfrac{1}{8},p_6=\tfrac{1}{8},p_7=\tfrac{1}{8}\}
\end{aligned}$$

In this case, that encoding [\[example_encoding\]][17] is indeed a good option. Notice that the encoding process yields a specific distribution $P(\enc)$, which determines its entropy $H[\enc]$ and, therefore, how much information per symbol it carries [@stone:2015]. The maximum entropy is obtained with this equiprobable distribution, the *uniform distribution* ([\[sec:uniform_distribution\]][18]).

Let us assume now that another information about the source is given. The weather station is in the Atacama desert, and $P({\src'}) = \{p_0=75\%,p_1=10\%,p_2=5\%,p_3=1\%, p_4=1\%,p_5=1\%,p_6=1\%,p_7=1\% \}$. With this new information about the source. Can we do better? Sure.

First, let us calculate the lower bound (maximum efficiency) of the bits/symbol rate of the source encoding, $\encrate= H[\src']$: $$\begin{aligned}
    H[\src']&= 0.75 \log \frac{1}{0.75} + 0.15 \log \frac{1}{0.15} + 0.05 \log \frac{1}{0.05} + 5\biggl(0.01 \log \frac{1}{0.01}\biggr) \nonumber \\
    &\approx 1 \frac{\text{bits}}{\text{symbol}}
\end{aligned}$$

We know that theoretically we cannot have an encoding with less than 1 bit/symbol in average. But we can improve from 3 bits/symbol (see [\[fig:equi_tree\]][19])[^16][^17]: $$\begin{aligned}
    \sA_{\enc'} = \{&\encsymb'_0=0, \encsymb'_1=10, \encsymb'_2=110, \encsymb'_3=11100,\nonumber \\
    &\encsymb'_4=111010, \encsymb'_5=111011, \encsymb'_6=11110, \encsymb'_7=11111\}
\end{aligned}$$

The average encoding size per message symbol in $\enc'$ is: $$\begin{aligned}
    &0.75 \cdot 1 + 0.15 \cdot 2 + 0.05 \cdot 3 + 0.03 \cdot 5 + 0.02 \cdot 6 \nonumber \\
    &\approx 1.5 \frac{\text{ bits}}{\text{symbol}}
\end{aligned}$$

### Cross-Entropy {#sec:cross-entropy}

This average encoding size per message symbol has a special name: the Cross-Entropy. It is evident the similarity of the definition of Cross-Entropy and Entropy. If our model $q$ of the real distribution $p$ is absolute right ($p=q$), the Cross-Entropy is equal to the Entropy $H_{p,q}=H_p$. If not (as it is in most cases), $H_{p,q} > H_p$.

In our the Atacama weather station example, the cross-entropy between the real distribution $p=p(\srcsymb)$ and the encoding distribution $q=p(\encsymb)$ was 1.5 bits/symbol. So, we can say the efficiency of the encoding $\enc(\srcsymb)$ is $\tfrac{\text{information}}{\text{data}} = \tfrac{H[\src]}{H_{p,q}[\src]} = \tfrac{1}{1.5} \approx 67\%$. We calculated $H_{p,q}$ knowing the sizes of each possible $\srcsymb_i$.

Let us use another example, imagine that we transport the weather station from the Atacama to London, where the probability distribution of the weather is $P({\src''}) = \{p_0=5\%,p_1=5\%,p_2=10\%,p_3=15\%, p_4=15\%,p_5=20\%,p_6=20\%,p_7=10\% \} \therefore H[\src'']\approx 2.8$, and keep using the same encoding. The encoding will be much less efficient. The average size of a message symbol in this situation is: $$\begin{aligned}
    H_{p,q}[\src''],~ &p = P(\src''), q = P(\enc') \\
    &= 0.05 \cdot 1 + 0.05 \cdot 2 + 0.1 \cdot 3 + 0.45 \cdot 5 + 0.35 \cdot 6 \nonumber \\
    &\approx 4.8 \frac{\text{ bits}}{\text{symbol}}
\end{aligned}$$ The efficiency of the encoding is $2.8/4.8=58.33\%$.

::: definition
**Cross-entropy** is the average number of bits needed to encode data coming from a source $\src$ with distribution $p(\srcsymb)$ when using model $q(\srcsymb)$. $$\begin{aligned}
        H_{p,q}[\src]=-\sum_{\srcsymb \in \sA_{\src}} p(\srcsymb) \log q(\srcsymb)
    
\end{aligned}$$
:::

### KL Divergence (or Relative Entropy)

The amount by which the Cross-Entropy and the Entropy diverge is the KL Divergence:

::: definition
The **relative entropy or Kullback--Leibler divergence** between two probability distributions $p(\srcsymb)$ and $q(\srcsymb)$ that are defined over the same alphabet $\srcalph$ is: $$\begin{aligned}
        \KL(p||q) = \sum_{\srcsymb} p(\srcsymb)~\log \frac{p(\srcsymb)}{q(\srcsymb)} = \E_{\rvS} \log \frac{p}{q}\\
        \KL(p||q) = H_{p,q}[\src] - H_p[\src] \label{eq:KL_decomposition}
    
\end{aligned}$$
:::

In our example: $$\begin{aligned}
    \KL(p_{\text{the Atacama}}||q_{\text{London}}) = \underbrace{H_{p,q}[\src'']}_{\approx 4.8} - \underbrace{H_p[\src'']}_{\approx 2.8} \approx 2 \frac{\text{bits}}{\text{symbol}}
\end{aligned}$$

### Shannon's source encoding theorem {#sec:source_encoding_theorem}

Now that we understand how the source encoding works, let us take a moment to appreciate the geniality of Shannon. Here, we show how he demonstrated the size of the optimal encoding without ever explaining which encoding is that in the first place.

::: restatable
theoremsourceencodingtheorem []{#th:source_encoding label="th:source_encoding"} The optimal binary encoding $\enc^{k}=(X_1, \cdots, X_k),~ \enc_i \in \{0,1\}$, of a n-symbols message $\srcblk=(\src_1, \cdots, \src_n)$, where $\src_i \in \srcalph$ are i.i.d. $\sim p(\srcsymb)$ has an expected size $k \approx n H[\src]$ for sufficiently large $n$.
:::

::: proof
*Proof.* A one-to-one mapping $\srcblk \mapsto \enc^{k}$ is invertible. If we enumerate all elements of $\srcblk$ in binary, we will need $k$ bits. Thus, with absolute certainty: $$\begin{aligned}
        k\leq \log \lceil|\srcblk |\rceil = \log \lceil 2^{n \log |\srcalph|}\rceil = n \log |\srcalph|+1~\text{ bits}\label{eq:k_upper bound}
    
\end{aligned}$$ Can we do better? We know from statistics that most possible outcomes are unlikely. In other words, there is a small set of very likely outcomes that are most probable. So let us use this property of Nature.

We will divide all sequences $\srcblk$ into two sets: the typical set ($\typicalset$) and its complement, the atypical set ($\neg~ \typicalset$), which can be seen in [\[fig:typical_atypical\]][20].

::: definition
The **typical set** $\typicalset$ with respect to $p(\srcsymb)$ is the subset of sequences $\srcblk=(\src_1,\cdots,\src_n ), \src_i \in \srcalph$, where: $$\begin{aligned}
            \begin{cases}
                P(\typicalset)=\sum_{\srcblk \in \typicalset} P(\srcblk)> 1-\epsilon, \text{ for sufficiently large }n\\
            P(\sample \in \typicalset) \approx p(s_i), \forall i.\label{typical_set_definition}
            \end{cases}
        
\end{aligned}$$
:::

In other words, for a sequence of $n$ i.i.d. random variables $\src \equiv(\src_1,\cdots,\src_n )$, each drawn from $p(\srcsymb)$, the outcome $\vm=(\srcsymb_1, \cdots, \srcsymb_n)$ is almost sure to belong to the typical set $\typicalset$, if n is large, and the probability of any outcome is almost the same.

Let us put aside that we do not know the size of the typical set, $|\typicalset|$.

We know that: $$\begin{aligned}
        |\typicalset| &\ll |\neg \typicalset| < |\srcblk |, \\
        P(\typicalset)&\gg P(\neg \typicalset ),\\
        \E(k) &= \lceil P(\typicalset)\log|\typicalset| + P(\neg \typicalset )\log |\neg \typicalset|\rceil.
    
\end{aligned}$$

Therefore, from [\[eq:k_upper bound\]][21] we can predict that: $$\begin{aligned}
        \E(k) &\ll  n \log |\srcalph|+1~\text{ bits}
    
\end{aligned}$$

Now, we need to find $|\typicalset|$. For this, we will use the [AEP]{acronym-label="AEP" acronym-form="singular+full"}, formalised bellow [@cover:2006]:

::: theorem
[]{#th:aep label="th:aep"} If $\src_1, \cdots, \src_n$ are i.i.d. sampled from the same distribution $p(\srcsymb)$, then: $$\begin{aligned}
            -\frac{1}{n}\log P(\src_1, \cdots, \src_n) \to H[\src] \text{ in probability.}
        
\end{aligned}$$
:::

::: proof
*Proof.* From the theorem definition, $\src_i$ are independent. Then from the Product Rule (eq. [\[eq:Product_Rule\]][22]): $$\begin{aligned}
            - \frac{1}{n} \sum_{i=1}^{n} \log P(\underbrace{\src_1, \cdots, \src_n}_{\srcblk}) &\overset{\text{~eq.\ref{eq:Product_Rule}}}{=} -\frac{1}{n} \log(\prod_{i=1}^{n} \cancelto{p(\srcsymb)}{P(\src_i))}\\
            &=\frac{1}{n} \sum_{i=1}^{n} -\log p(\srcsymb)
        
\end{aligned}$$ From the weak law of large numbers: $$\begin{aligned}
         n \to \infty,~\frac{1}{n} \sum_{i=1}^{n}\xi_i \to \mathbb{E}(\xi) \label{eq:law_large_numbers}
        
\end{aligned}$$ Therefore, using the fact that a statistic of a random variable is a random variable, let $\xi=- \log P(\src_i)$ [@cover:2006] and using [\[eq:entropy\]][23] and [\[eq:law_large_numbers\]][24]: $$\begin{aligned}
            n \to \infty,& \nonumber\\
            \frac{1}{n} \sum_{i=1}^{n}(- \log P(\src_i))
            &\to \underbrace{\E_p (- \log p(\srcsymb))}_{H[\src]}\\
            \therefore -\frac{1}{n}\log P(\srcblk) &\to H[\src]~\qed
        
\end{aligned}$$ ◻
:::

Now that we proved , let us use it to define $|\typicalset|$: $$\begin{aligned}
            -\frac{1}{n} \log P (&\srcblk)\to H[\src] \text{ in probability}\\
            P (&\srcblk) \to 2 ^{-n(H[\src])} \therefore\\
            2 ^{-n(H[\src] + \epsilon)}\leq P (&\srcblk)\leq 2 ^{-n(H[\src] - \epsilon)} \text{ in probability}\label{eq:epsilon}
        
\end{aligned}$$ We also know that: $$\begin{aligned}
            1 &= \sum_{\srcblk} P(\srcblk)\\
            1 &\geq \sum_{\srcblk \in \typicalset}P(\srcblk)\\
            1 &\geq |\typicalset|~ P(\srcblk)
        
\end{aligned}$$ From [\[eq:epsilon\]][25]: $$\begin{aligned}
            1 &\geq |\typicalset|~ 2 ^{-n(H[\src] + \epsilon)}\\
            \therefore~ |\typicalset| &\leq 2 ^{n(H[\src] + \epsilon)}\label{eq:typical_upper bound}
        
\end{aligned}$$ This upper bound to $|\typicalset|$ is all we need to prove . $$\begin{aligned}
            \E(k) &= \lceil P(\typicalset)\log|\typicalset|  \nonumber \\
            &+ \cancelto{\epsilon}{P(\neg \typicalset)} \cancelto{|\srcblk|=n \log |\srcalph|}{\log |\neg \typicalset|} \rceil \\
            &\simeq \lceil (1-\epsilon)\log 2^{n(H[\src]+\epsilon)} + \cancelto{\epsilon' n}{\epsilon n \log |\srcalph|}\rceil\\
            &\simeq \lceil (1-\epsilon)[n(H[\src]+\epsilon)] + \epsilon' n \rceil\\
            &\simeq \lceil n(H[\src]+\epsilon - \epsilon n H[\src] - \epsilon^2) + n (\epsilon')\rceil\\
            &\simeq \lceil n(H[\src]+\cancelto{\epsilon''}{\epsilon - \epsilon H[\src] - \epsilon^2}) +  n (\epsilon')\rceil\\
            &\simeq \lceil n(H[\src]+\epsilon''+\epsilon')\rceil = \lceil n(H[\src]+\varepsilon)\rceil\\
            \therefore \nonumber\\
            \E(k) &\simeq n H[\src] \qedhere
        
\end{aligned}$$ ◻
:::

We proved that the average information per symbol of the coding generated by the optimum encoder has the same average information per symbol as the source, $H[\src]\frac{\text{bits}}{\text{symbol}}$. Due to this property, it is quite common to talk about $H[X]$ as the entropy of the source.

### Typical Set

We defined the typical set and discovered some of its properties in the proof of the source coding theorem, but we left one behind. We only needed the upper bound for $|\typicalset|$, let us now derive its lower bound. From [\[eq:epsilon\]][25] and the typical set definition [\[typical_set_definition\]][26]: $$\begin{aligned}
        \sum_{\srcblk \in \typicalset}2^{-n(H[\src]-\epsilon)}&\geq 1 - \epsilon\\
        |\typicalset|2^{-n(H[\src]-\epsilon)}&\geq 1 - \epsilon\\
        |\typicalset|&\geq (1 - \epsilon)2^{n(H[\src]-\epsilon)}\label{eq:typical_lower bound}
    
\end{aligned}$$ Therefore, from [\[eq:typical_lower bound\]][27] and [\[eq:typical_upper bound\]][28] we can derive: $$\begin{aligned}
     (1 - \epsilon)2^{n(H[\src]-\epsilon)} \leq &|\typicalset| \leq 2 ^{n(H[\src]+\epsilon)}\\
     &|\typicalset| \to 2 ^{nH[\src]}\label{eq:typical_set_size}
\end{aligned}$$ With that, we can list some useful properties of $\typicalset$:

1.  almost all probability is concentrated in the typical set, by definition [\[typical_set_definition\]][26] ;

2.  elements in the typical set are nearly equiprobable  [\[eq:epsilon\]][25];

3.  the number of elements in the typical set is nearly $2^{H[\src]}$ [\[eq:typical_set_size\]][29].

Going back to : $$\begin{aligned}
    &\frac{1}{n}\log \biggl(\frac{1}{P(\srcblk)}\biggr) &\to H[\src] \nonumber\\
    H[\src] -\epsilon \leq  &\frac{1}{n}\log \biggl(\frac{1}{P(\srcblk)}\biggr)\leq H[\src] +\epsilon
\end{aligned}$$

We can think of the middle term as the Entropy of a sample size $n$. Thus a typical sample gives us an amount of information close to the average information from the source, $H[\src]$[^18].

## The channel: Data transmission

The channel is simply the medium used to transmit the signal $\encvec$ from the encoder to the decoder[^19]. It may be anything from a band of radio frequencies, an electrical wire, a beam of light, or a postal service. As we did before, we can also think the channel as a "tube" which carries information (see [\[fig:tube\]][14]).[^20]

::: definition
Mathematically, a **discrete channel** is the conditional probability $$\begin{aligned}
        p(\decsymb|\encsymb), \decsymb \in \decalph, \encsymb \in \encalph.
    
\end{aligned}$$
:::

### Noiseless Channel Capacity

::: definition
The **operational capacity** of a channel is the maximum rate of bits per transmission that the medium is physically capable of transmitting. It is, in fact, just a number of bits per transmission. We can think of it as the maximum entropy it is capable of transmitting in the absence of noise: $$\begin{aligned}
        \sC_{\text{operational}}= R = \max_{p(\encsymb)} \log |\encalph| \text{ bits/usage}.
    
\end{aligned}$$
:::

### The noisy channel

All practical communications, however, are noisy [@stone:2015]. Noise reduces the rate at which information can be communicated reliably. Shannon proved that information could be communicated, with arbitrarily small error, at a rate limited only by the channel capacity.

To understand how noise affects the channel capacity, we need to understand the concepts of **conditional entropy**, **joint entropy** and **mutual information**.

### Conditional Entropy

The residual uncertainty we have about a random variable given that we already know the outcome of another random variable is the **conditional entropy**[^21]:

::: definition
The **conditional entropy** or **equivocation** $H[\enc|\dec]$ of $\enc$ given $\dec$ is: $$\begin{aligned}
        H[\enc|\dec] &\eqdef \sum_{\decsymb \in \decalph} p(\decsymb) \left[ \sum_{\encsymb \in \encalph} p(\encsymb|\decsymb) \log \frac{1}{p(\encsymb|\decsymb)}\right]\\
        &=- \sum_{\encsymb\decsymb \in \encalph\decalph} p(\encsymb,\decsymb) \log p(\encsymb|\decsymb)
    
\end{aligned}$$
:::

### Joint Entropy

We have defined the entropy of a single random variable in [\[eq:entropy\]][23]. Now, we extend the definition to a pair of random variables. As the pair can be seen as a single vector-valued random variable, there is nothing new in this definition [@cover:2006 p.15].

::: definition
The **joint entropy** $H[\rvX, \rvY]$ of a pair of discrete random variables $(\rvX, \rvY)$ with joint distribution $p (x, y)$ is defined as: $$\begin{aligned}
        H [X,Y] &\triangleq -\E \log P(\rvX, \rvY) \\
        &= - \sum_{\encsymb \in \sA_x} \sum_{\decsymb \in \sA_y} p(\encsymb,\decsymb) \log p(\encsymb,\decsymb).
    \label{eq:joint_entropy} 
\end{aligned}$$
:::

### Mutual Information

::: definition
[]{#def_mutual_information label="def_mutual_information"} The **mutual information** $I[\enc;\dec]$ between two variables, such as a channel input $\enc$ and output $\dec$, is the amount of information obtained about one random variable through observing the other random variable. $$\begin{aligned}
        I[\enc;\dec] &= \sum_i \sum_j p(\encsymb_i, \decsymb_j) \log \frac{p(\encsymb_i, \decsymb_j)}{p(\encsymb_i) p(\decsymb_j)} \text{ bits} \label{eq:mutual_information}\\
        &= H[\enc] - H[\enc|\dec]\\
        &= H[\dec] - H[\dec|\enc]\\
        &= H[\enc] + H[\dec] - H[\enc, \dec]\\
        &= H[\enc, \dec] - [H[\enc|\dec] + H[\dec|\enc]] \text{ bits}
    
\end{aligned}$$
:::

::: marginfigure
::: venndiagram2sets
::: scope
:::

::: scope
:::

(0,2.7) node $\HX$; (5,2.7) node $\HY$; (2.5,0) node $\HXY$;
:::

::: venndiagram2sets
::: scope
:::

(0,2.7) node $\HX$; (5,2.7) node $\HY$; (2.5,0) node $\HXgivenY$;
:::

::: venndiagram2sets
::: scope
:::

(0,2.7) node $\HX$; (5,2.7) node $\HY$; (2.5,0) node $\HYgivenX$;
:::

::: venndiagram2sets
::: scope
:::

(2.5,0) node $\IXY=\IYX$;
:::
:::

For a visual understanding of these measures, see [\[fig:mutual\]][30]. The mutual information can also be seen as a measure of the mutual dependence between the two variables, as the mutual information is the same as the Kullback--Leibler divergence between the joint distribution and the product of the variables marginal distributions: $$\begin{aligned}
    I[\enc;\dec] = \KL(p(\encsymb, \decsymb)||p(\encsymb)p(\decsymb)).
\end{aligned}$$

### Data Processing Inequality {#sec:dpi}

We cannot increase information by applying a deterministic function to the data, nor decrease information if the deterministic function is invertible.

::: theorem
Let three random variables form the Markov chain $\rvX \rightarrow \rvY \rightarrow \rvZ$, implying: $$\begin{aligned}
        p(\rx,\ry,\rz)=p(\rz|\ry)p(\ry|\rx)p(\rx).
    
\end{aligned}$$ No processing of $\rvY$, deterministic or random, can increase the information that $\rvY$ contains about $\enc$: $$\begin{aligned}
        I[\rvX;\rvY] \geqslant I[\rvX;\rvZ]
    
\end{aligned}$$
:::

::: proof
*Proof.* We refer to [@cover:2006 th.2.8.1] for proof. ◻
:::

::: theorem
Let $\rvX \rightarrow \rvY \rightarrow \rvZ$ form a Markov Chain, then functions of the data $\rvY$ cannot increase the information about $\rvX$, $I[\rvX;\rvY]\geqslant I[\rvX;g(\rvY)]$.[]{#th:reparemetrisation_invariance label="th:reparemetrisation_invariance"}
:::

::: proof
*Proof.* $\rvZ=g(\rvY) \therefore I[\rvX;g(\rvY)]=I[\rvX;\rvZ]$. By the [DPI]{acronym-label="DPI" acronym-form="singular+full"} property: $$\begin{aligned}
        I[\rvX;\rvY] &\geqslant I[\rvX;\rvZ]\\
        I[\rvX;\rvY] &\geqslant I[\rvX;g(\rvY)]
    
\end{aligned}$$ ◻
:::

### Noisy channel capacity

Given that in a noisy channel $\dec = \enc + \noise$, where $\noise$ is the noise in the channel, from the mutual information definition: $$\begin{aligned}
    I[\enc;\dec]&=H[\dec] - H[\dec|\enc]\label{eq:mutual_information_decomposition}\\
    &=H[\dec] - H[(\enc + \noise)|\enc].
\end{aligned}$$ If $\enc$ is known, the uncertainty from $\enc$ is none: $$\begin{aligned}
    I[\enc;\dec]&=H[\dec] - H[\noise|\enc]
\end{aligned}$$ By definition, $\noise$ and $\enc$ are independent, therefore: $$\begin{aligned}
    I[\enc;\dec]&=H[\dec] - H[\noise]\tag{from \eqref{eq:mutual_information_decomposition}}\\
    \therefore~ H[\dec|\enc] &= H[\noise]
\end{aligned}$$

::: definition
The **information capacity** or *effective capacity* of a noisy channel is defined as: $$\begin{aligned}
        C &= \max_{p(x)} I[\enc;\dec]\\
        &=\max_{p(x)} (H[\dec] - H[\dec|\enc]) \text{ bits/transmission.}\\
        &=\max_{p(x)} (H[\enc] - H[\enc|\dec]) \text{ bits/transmission}.
    
\end{aligned}$$
:::

The information capacity can be derived theorem from Shannon's noisy channel theorem ([1.7]).

## Shannon's noisy channel theorem {#noisy_channel_theorem}

In his second and, perhaps, most crucial theorem, Shannon proved that provided $H[\enc]\leq C$, the average error ($\epsilon$), when averaged over all possible encoders approaches to zero ($\epsilon \to 0$) as the length of the input $\encvec$ increases. Therefore, there must exist at least one encoder that produces an error as small as $\epsilon$ [@cover:2006 p. 198].

::: restatable
theoremnoisychanneltheorem []{#th:shannon_2nd_law label="th:shannon_2nd_law"} All rates below capacity C are achievable. Specifically, for every rate $R < C$, there exists a sequence of ($2^{nR}, n$) codes with maximum probability of error $\lambda^{(n)} \to 0$. Conversely, any sequence of ($2^{nR}, n$) codes with $\lambda^{(n)} \to 0$ must have $R \leq C$.
:::

Once again, Shannon proved with a counterintuitive argument. He demonstrates there is an encoder that produces an arbitrarily small error without showing how to find this encoder.

Instead of proving the theorem (for which we refer to [@mackay:2002] and [@cover:2006]), let us give an intuitive explanation of the proof.

Consider $n$ uses of the channel as our block usage. There are $|\encalph|^n$ possible inputs $\encvec$ and $|\decalph|^n$ possible outputs $\decvec$ in the block usage. We want to prove that for any $\decvec$, it is possible to derive an unique message that generated it.

If $n$ is large, any particular $\encvec \in \enc^n$ is very likely to produce an output in a small subspace of the output alphabet, the typical output set, given $\encvec$. So, it is possible to find a non-confusable subset of the input sequences that produce disjoint output sequences.

Take $\encvec \sim p(\enc^n)$. Recall  , the total number of typical output sequences $\decvec$ is $2^{nH[\dec]}$ (see [\[fig:fan\]][31] (B)), all sequences being almost equiprobable. For any sequence $\encvec$, there are about $2^{nH[\dec|\enc]}$ probable sequences (see [\[fig:fan\]][31] (A)).

Now we restrict ourselves to the subset of the typical inputs, such that the corresponding typical output sets are disjoint. We can expect the *number of non-confusable inputs* to be: $$\begin{aligned}
    |\typical{\rvX \to \rvY}|\leq \frac{2^{nH[\dec]}}{2^{nH[\dec|\enc]}}=2^{n(H[\dec]-H[\dec|\enc])}=2^{nI[\enc;\dec]}
\end{aligned}$$

The maximum value of this bound is achieved by the process $\enc$ that maximises $I[X;Y]$. Therefore, $n \max_{p(\encsymb)}I[X;Y]$ is the maximum amount of bits that can be transmitted in $n$ usages of the channel, which proves the first law of information (see [\[shannon_laws\]][32]): $$\begin{aligned}
    C\text{\tiny{noisy channel}} &= \max_{p(x)} I[\enc;\dec]\label{eq:shannon_1st_law}.
\end{aligned}$$

We can rewrite [\[eq:shannon_1st_law\]][33] as: $$\begin{aligned}
    C\text{\tiny{noisy channel}} &= \max_{p(x)} (H[\enc]-H[\noise])\label{eq:2nd_law} ,
\end{aligned}$$ which states that noise reduces channel capacity. So, this is also a proof for the second law of information ([\[shannon_laws\]][32]).

## Beyond Shannon's Information

Even before Shannon's , other information measures have been defined and studied. In this section we will expose two other notions of information that we will use further in the dissertation: Algorithmic information and Fisher information.

### Algorithmic information (Kolmogorov-Chaitin complexity) {#sec:k-c_complexity}

Developed independently by Chaitin, Solomonoff, and Kolmogorov in the $1960$s, *algorithmic information* (most commonly known as Kolmogorov complexity) of an object (a message) is the length of the shortest program capable of producing the object as an output [@stone:2015].

For example, in this definition the string:\
'T6ucFndKEjTyqIGYuXUKqI6fJ6HBRL'\
is more complex than\
'abcabcabcabcabcabcabcabcabcabc'. We can express both in the Python programming language as an example:

        ``T6ucFndKEjTyqIGYuXUKqI6fJ6HBRL''

versus

        ``abc''*10

If the object is compressable (shorter program), it has more regularity. Thus, there is a relation between complexity and compressibility.

### Fisher Information {#sec:fisher_information}

Let $P_{\theta}$ denote a family of parametric distributions on a space $\XX$ with probability mass or density function given by $p_{\theta}$.

::: definition
The Fisher information $I_{\rvX}(\theta)$ of a random variable $\rvX$ w.r.t. the parameter $\theta$ is the matrix: $$\begin{aligned}
_{ij} &:= \E_{\theta} \left[ \nabla_{\theta_i} \log p_{\theta}(\rvX) \cdot  \nabla_{\theta_j} \log p_{\theta}(\rvX)^\top \right] \\
        &= \E_{\theta}\left[ \frac{\partial \ell}{\partial \theta_i} \cdot \frac{\partial \ell}{\partial \theta_j}^\top \right],
    
\end{aligned}$$ where $\ell(\rx|\theta)= \log p(\rx|\theta)$ is often called the score function.
:::

The Fisher information measures the overall sensitivity of the functional relationship $p$ to changes of $\theta$ by weighting the sensitivity at each potential outcome $\rx$ w.r.t $p_{\theta}(\rx)$ [@maarten:2017]

A common simplification of the Fisher Information Matrix (FIM) is to reduce it to the diagonal: $$\begin{aligned}
_{i} &:= \E_{\theta} \left[ \nabla_{\theta_i} \log p_{\theta}(\rvX)^2 \right]
\end{aligned}$$

### Occam factor

There are countless problems in science that require that given a limited dataset, preferences be assigned to alternative hypotheses of different complexities. The **Occam's razor** is the principle that states a preference for simple theories. Although it is often advocated for aesthetic reasons,  @mackay:2002 gave a Bayesian explanation for its empirical success that does not depend on any bias towards beauty [@mackay:2002].

Consider evaluating the plausibility of two alternative theories $\HH_1$ and $\HH_2$, in the light of given evidence $C$ ([\[fig:model_comparison\]][34]). Simple models make precise predictions, while complex models are capable of making a greater variety of predictions. Hence, if $\HH_2$ is more complex, it must spread its predictive capability more thinly over the data space $\DD$ than $\HH_1$. Thus, where the gathered data $C$ is compatible with both theories, the simpler $\HH_1$ will be more probable than $\HH_2$. $$\begin{aligned}
    \frac{P(\HH_1|\DD)}{P(\HH_2|\DD)}&=\frac{P(\HH_1)}{P(\HH_2)}\frac{P(\DD|\HH_1)}{P(\DD|\HH_2)}\\
    \therefore~ P(\HH_1)&=P(\HH_1),\\
        \frac{P(\HH_1|\DD)}{P(\HH_2|\DD)}&=\frac{P(\DD|\HH_1)}{P(\DD|\HH_2)}\label{evidence_comparisson}
\end{aligned}$$

##### Quantifying Occam's razor

We already established that we can rank models based by evaluating the evidence $P(\DD|\HH_i)$ [\[evidence_comparisson\]][35]: $$\begin{aligned}
    P(\DD|\HH_i) = \int P(\DD|\vw, \HH_i) P(\vw|\HH_i) d\vw \label{evidence_integral}
\end{aligned}$$ Taking for simplicity the one-dimensional case and applying Laplace's method, we can approximate the evidence by multiplying the peak of $P(\DD|\HH_i)$ by $\sigma_{\vw|\DD}$ (approximating the shaded areas in [\[fig:model_comparison\]][34]) [@mackay:2002]: $$\begin{aligned}
    \underbrace{P(\DD|\HH_i)}_\text{Evidence} \simeq \underbrace{P(\DD|\vw_{\text{\tiny MP}}, \HH_i)}_\text{Best-fit likelihood} \times \underbrace{P(\vw_{\text{\tiny MP}}|\HH_i) \sigma_{\vw|\DD}}_\text{Occam's factor}
\end{aligned}$$ The Occam's factor is the amount by which the accessible volume of $\HH_i$'s hypothesis space collapses when data arrive. This relates to how we measure information ([1.3.1]). **The Occam's factor log is a measure of the amount of information we gain about the model's parameters when data arrive.**

The Occam's factor is the basis of  @mackay:2002's Evidence Framework. The connection was no surprise given that we derived Information from the Bayesian interpretation of Probability ([1.1]).

## Concluding Remarks

This chapter derived the *information* measure from its definition and then summarised information-theoretical concepts.

### Assumptions

1.  A definition of intelligence ([\[def:intelligence\]][36])

2.  Knowledge is a set of beliefs, quantifiable by real numbers and dependent on prior evidence ([\[sec:desiderata_language_sceptical\]][37], [\[beliefs\]][38]);

3.  Bayesian inference assumptions:

    1.  Common sense ([\[sec:desiderata_language_sceptical\]][37], [\[common_sense\]][39]);

    2.  Consistency ([\[sec:desiderata_language_sceptical\]][37], [\[consistency\]][5]);

    3.  Minimality ([\[sec:desiderata_language\]][40], [\[rational_minimality\]][41]).

4.  [MLT]{acronym-label="MLT" acronym-form="singular+short"} specific assumptions for the learning problem:

    1.  No assumption on $\DD=\PXY$;

    2.  $\DD=\PXY$ is unknown;

    3.  $\DD=\PXY$ is fixed: no "time" parameter.

    4.  Independent sampling;[]{#fixed_assumption label="fixed_assumption"}

    5.  Labels may assume non-deterministic values ($h$ can be stochastic, but can also be deterministic);

    6.  Learning is an optimisation problem in the hypothesis space.

5.  [IT]{acronym-label="IT" acronym-form="singular+short"}-specific assumptions:

    1.  Information is what changes belief;

    2.  $\srcalph$ and $\encalph$ are finite sets;[]{#itm:finite_sets label="itm:finite_sets"}

    3.  Sampling from an ergodic stochastic process and sampled data is typical;[]{#itm:ergodic label="itm:ergodic"}

    4.  Labels may assume non-deterministic values (an encoder/decoder can be stochastic or deterministic).

### The first comparison between MLT and IT {#sec:first_comparison}

At this point, we have not yet expressed the Machine Learning Problem as an Information Theory problem. Still, as [MLT]{acronym-label="MLT" acronym-form="singular+short"} and [IT]{acronym-label="IT" acronym-form="singular+short"} both share *Bayesian inference* as the basis they do not invalidate each other. Both may have found the same truths by different paths.

The main differences in [IT]{acronym-label="IT" acronym-form="singular+short"} from [MLT]{acronym-label="MLT" acronym-form="singular+short"} assumptions are [\[itm:finite_sets,itm:ergodic\]][42]. In [\[ch:intermezzo\]][43], we will see that the first is not a problem at all. The ergodic process sampling, in its turn, is a less constrained assumption than the i.i.d. sampling in [MLT]{acronym-label="MLT" acronym-form="singular+short"}. For simplification sake, we may assume that both sample i.i.d.

[^1]: $P(S)$ is in fact $P(S|K)$, but we supress it to reduce the clutter.

[^2]: Here we are talking about *events*: $P(S|\re )$ is a short hand for $P(S|\{\re\} \land K )$.

[^3]: We have already ( [\[sec:sceptical_agents\]][44]) delved a little on the implications of the indifference to the order of evidence which is also an indifference in sequential versus simultaneous updating.

[^4]: Also known as the Shannon information content of an outcome  or Hartley's information.

[^5]: In a rare piece of collaboration, Shannon asked his lunchroom table colleagues at Bell Labs to come up with a snappier name than *binary digit*. *Bigit* was considered, but John Tukey's proposal, *bit*, was chosen .

[^6]: $\srcsymb$ is the intended message. One can think about it as the meaning or the semantics.

[^7]: $\srcalph$ is the alphabet or the set of possible outcomes of the random variable $\src$.

[^8]:  @mackay:2002 call this factor *Occam's factor* .

[^9]: Information theory magnitudes are functions of the probabilities random variables and not directly of a random variable. To address this difference, we opt to use square brackets instead of parenthesis.

[^10]: We will constrain our explanations of Information Theory to the discrete case. It can be argued that if we are interested in models that computers will use, some quantisation will always happen.

[^11]: "Most English speakers will understand this phrase without vowels".

[^12]: A *codebook* is a dictionary that relates words in the source alphabet, $\srcalph$ to words, codes, in the encoder alphabet $\encalph$.

[^13]: The relationship between information (entropy) and the shape of the distribution is crucial for the [IBT]{acronym-label="IBT" acronym-form="singular+short"} perspective.

[^14]: This example is inspired by  

[^15]: The probability distribution that produces maximum entropy is the *uniform distribution* ([\[sec:uniform_distribution\]][18])

[^16]: Any distribution that is not uniform will lead to an average tree height that is smaller that the uniform distribution. The uniform distribution is the worst case.

[^17]: In [\[ch:mlt\]][45], we used $\XX$ to represent the domain of $\rvX$, here we use $\sA_{\rvX}$ to represent the same domain to emphasise that the domain is finite; it is an alphabet.

[^18]: This insight reminds us of the sample complexity, discussed in [\[ch:mlt\]][45]

[^19]: $\encsymb \mapsto \decsymb$

[^20]: This definition of a discrete channel covers the deterministic case where $\decsymb = f(\encsymb)$.\
    In most cases, the usage of a channel is determined by the period in which it is being used. Thus, some prefer to define the capacity in bits/second.

[^21]: In the communication setting, we usually want to know the residual information in $\dec$ that is not from $\enc, H[\dec|\enc]$, which we call noise.

  [1]: #sec:anatomy_ia {reference-type="ref" reference="sec:anatomy_ia"}
  [2]: #sec:independent_events {reference-type="ref" reference="sec:independent_events"}
  [3]: #sec:desiderata_language_sceptical, consistency {reference-type="ref" reference="sec:desiderata_language_sceptical, consistency"}
  [4]: #eq:likelihood {reference-type="ref" reference="eq:likelihood"}
  [5]: #consistency {reference-type="ref" reference="consistency"}
  [6]: #axiom:order {reference-type="ref" reference="axiom:order"}
  [7]: #eq:i_equals_log_LH {reference-type="ref" reference="eq:i_equals_log_LH"}
  [8]: #eq:is_ln {reference-type="ref" reference="eq:is_ln"}
  [9]: #fig:communication_setting {reference-type="ref" reference="fig:communication_setting"}
  [1.1]: #sec:prob2info {reference-type="ref" reference="sec:prob2info"}
  [10]: #def:information {reference-type="ref" reference="def:information"}
  [11]: #fig: branching {reference-type="ref" reference="fig: branching"}
  [12]: #sec:probability2information {reference-type="ref" reference="sec:probability2information"}
  [13]: #fig:360k, fig:27k, fig:5k {reference-type="ref" reference="fig:360k, fig:27k, fig:5k"}
  [1.5.6]: #sec:source_encoding_theorem {reference-type="ref" reference="sec:source_encoding_theorem"}
  [14]: #fig:tube {reference-type="ref" reference="fig:tube"}
  [15]: #fig:information_loss {reference-type="ref" reference="fig:information_loss"}
  [16]: #fig:resource_waste {reference-type="ref" reference="fig:resource_waste"}
  [17]: #example_encoding {reference-type="eqref" reference="example_encoding"}
  [18]: #sec:uniform_distribution {reference-type="ref" reference="sec:uniform_distribution"}
  [19]: #fig:equi_tree {reference-type="ref" reference="fig:equi_tree"}
  [20]: #fig:typical_atypical {reference-type="ref" reference="fig:typical_atypical"}
  [21]: #eq:k_upper bound {reference-type="eqref" reference="eq:k_upper bound"}
  [22]: #eq:Product_Rule {reference-type="ref" reference="eq:Product_Rule"}
  [23]: #eq:entropy {reference-type="eqref" reference="eq:entropy"}
  [24]: #eq:law_large_numbers {reference-type="eqref" reference="eq:law_large_numbers"}
  [25]: #eq:epsilon {reference-type="eqref" reference="eq:epsilon"}
  [26]: #typical_set_definition {reference-type="eqref" reference="typical_set_definition"}
  [27]: #eq:typical_lower bound {reference-type="eqref" reference="eq:typical_lower bound"}
  [28]: #eq:typical_upper bound {reference-type="eqref" reference="eq:typical_upper bound"}
  [29]: #eq:typical_set_size {reference-type="eqref" reference="eq:typical_set_size"}
  [30]: #fig:mutual {reference-type="ref" reference="fig:mutual"}
  [1.7]: #noisy_channel_theorem {reference-type="ref" reference="noisy_channel_theorem"}
  [31]: #fig:fan {reference-type="ref" reference="fig:fan"}
  [32]: #shannon_laws {reference-type="ref" reference="shannon_laws"}
  [33]: #eq:shannon_1st_law {reference-type="eqref" reference="eq:shannon_1st_law"}
  [34]: #fig:model_comparison {reference-type="ref" reference="fig:model_comparison"}
  [35]: #evidence_comparisson {reference-type="eqref" reference="evidence_comparisson"}
  [1.3.1]: #guessing_game {reference-type="ref" reference="guessing_game"}
  [36]: #def:intelligence {reference-type="ref" reference="def:intelligence"}
  [37]: #sec:desiderata_language_sceptical {reference-type="ref" reference="sec:desiderata_language_sceptical"}
  [38]: #beliefs {reference-type="ref" reference="beliefs"}
  [39]: #common_sense {reference-type="ref" reference="common_sense"}
  [40]: #sec:desiderata_language {reference-type="ref" reference="sec:desiderata_language"}
  [41]: #rational_minimality {reference-type="ref" reference="rational_minimality"}
  [42]: #itm:finite_sets,itm:ergodic {reference-type="ref" reference="itm:finite_sets,itm:ergodic"}
  [43]: #ch:intermezzo {reference-type="ref" reference="ch:intermezzo"}
  [44]: #sec:sceptical_agents {reference-type="ref" reference="sec:sceptical_agents"}
  [45]: #ch:mlt {reference-type="ref" reference="ch:mlt"}
