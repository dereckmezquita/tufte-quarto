[
  {
    "objectID": "Chapters/context.html#problem",
    "href": "Chapters/context.html#problem",
    "title": "4  Introduction",
    "section": "4.1 Problem",
    "text": "4.1 Problem\nIn the last decade, we have witnessed a myriad of astonishing successes in Deep Learning. Despite those many successes in research and industry applications, we may again be climbing a peak of inflated expectations. If in the past, the false solution was to “add computation power” on problems, today we try to solve them by “piling data”([fig:machine_learning_2x]). Such behaviour has triggered a winner-takes-all competition for who collects more data (our data) amidst a handful of large corporations, raising ethical concerns about privacy and concentration of power (O’Neil 2016).\n\nO’Neil, Cathy. 2016. Weapons of Math Destruction: How Big Data Increases Inequality and Threatens Democracy. USA: Crown Publishing Group.\n\nZhang, Chiyuan, Samy Bengio, Moritz Hardt, Benjamin Recht, and Oriol Vinyals. 2016. “Understanding Deep Learning Requires Rethinking Generalization.” https://arxiv.org/abs/1611.03530.\nNevertheless, we know that learning from way fewer samples is possible: humans show a much better generalisation ability than our current state-of-the-art artificial intelligence. To achieve such needed generalisation power, we may need to understand better how learning happens in deep learning. Rethinking generalisation might reshape the foundations of machine learning theory (Zhang et al. 2016).\n\n4.1.1 Possible new explanation in the horizon\nIn \\(2015\\),  Tishby and Zaslavsky (2015) proposed a theory of deep learning  (Tishby and Zaslavsky 2015) based on the information-theoretical concept of the bottleneck principle, of which Tishby is one of the authors. Later, in 2017,  Shwartz-Ziv and Tishby (2017) followed up on the IBT with the paper  , which was presented in a well-attended workshop8, with appealing visuals that clearly showed a “phase transition” happening during training. The video posted on Youtube (Tishby 2017) became a “sensation”9, and received a wealth of publicity when well-known researchers like Geoffrey Hinton10, Samy Bengio (Apple) and Alex Alemi (Google Research) have expressed interest in Tishby’s ideas (Wolchover 2017). they are called formal languages.8 Deep Learning: Theory, Algorithms, and Applications. Berlin, June 2017 http://doc.ml.tu-berlin.de/dlworkshop2017\nTishby, Naftali. 2017. “Information Theory of Deep Learning.” https://youtu.be/bLqJHjXihK8. https://youtu.be/bLqJHjXihK8.\n9 By the time of this writing, this video as more than \\(84,000\\) views, which is remarkable for an hour-long workshop presentation in an academic niche. https://youtu.be/bLqJHjXihK810 Another Deep Learning Pioneer and Turing award winner (2018).\nWolchover, Natalie. 2017. “New Theory Cracks Open the Black Box of Deep Learning.” https://www.quantamagazine.org/new-theory-cracks-open-the-black-box-of-deep-learning-20170921/; Simons Foundation.\n\n\nTishby, Naftali, and Noga Zaslavsky. 2015. “Deep Learning and the Information Bottleneck Principle.” In 2015 IEEE Information Theory Workshop (ITW), 1–5. IEEE.\n\nI believe that the information bottleneck idea could be very important in future deep neural network research. — Alex Alemi\n\nAndrew Saxe (Harvard University) rebutted  Shwartz-Ziv and Tishby (2017) claims in   and was followed by other critics. According to Saxe, it was impossible to reproduce  (Shwartz-Ziv and Tishby 2017)’s experiments with different parameters.\n\nShwartz-Ziv, Ravid, and Naftali Tishby. 2017. “Opening the Black Box of Deep Neural Networks via Information.” https://arxiv.org/abs/1703.00810.\nHas the initial enthusiasm on the IBT been unfounded? Have we let us “fool ourselves” by beautiful charts and a good story?\n\n\n4.1.2 Problem statement\nThe practice of modern machine learning has outpaced its theoretical development. In particular, deep learning models present generalisation capabilities unpredicted by the current machine learning theory. There is yet no established new general theory of learning which handles this problem.\nIBT was proposed as a possible new theory with the potential of filling the theory-practice gap. Unfortunately, to the extent of our knowledge, there is still no comprehensive digest of IBT nor an analysis of how it relates to current MLT."
  },
  {
    "objectID": "Chapters/context.html#objective",
    "href": "Chapters/context.html#objective",
    "title": "4  Introduction",
    "section": "4.2 Objective",
    "text": "4.2 Objective\nThis dissertation aims to investigate to what extent can the emergent Information Bottleneck Theory help us better understand Deep Learning and its phenomena, especially generalisation, presenting its strengths, weaknesses and research opportunities.\n\n4.2.1 Research Questions"
  },
  {
    "objectID": "Chapters/context.html#methodology",
    "href": "Chapters/context.html#methodology",
    "title": "4  Introduction",
    "section": "4.3 Methodology",
    "text": "4.3 Methodology\n\nGiven that IBT is yet not a well-established learning theory, there were two difficulties that the research had to address:\n\nThere is a growing interest in the subject, and new papers are published every day. It was essential to select literature and restrain the analysis.\nEarly on, the marks of an emergent theory in its infancy manifested in the form of missing assumptions, inconsistent notation, borrowed jargon, and seeming missing steps. Foremost, it was unclear what was missing from the theory and what was missing in our understanding.\n\nAn initial literature review on IBT was conducted to define the scope.11 We then chose to narrow the research to theoretical perspective on generalisation, where we considered that it could bring fundamental advances. We made the deliberate choice of going deeper in a limited area of IBT and not broad, leaving out a deeper experimental and application analysis, all the work on ITL12  (Principe 2010) and statistical-mechanics-based analysis of SGD  (P. Chaudhari and Soatto 2018; Pratik Chaudhari et al. 2019). From this set of constraints, we chose a list of pieces of IBT literature to go deeper ([ch:literature]).\nIn order to answer , we discuss the epistemology of AI to choose fundamental axioms (definition of intelligence and the definition of knowledge) with which we deduced from the ground up MLT, IT and IBT, revealing hidden assumptions, pointing out similarities and differences. By doing that, we built a “genealogy” of these research fields. This comparative study was essential for identifying missing gaps and research opportunities.\nIn order to answer , we first dissected the selected literature ([ch:literature]) and organised scattered topics in a comprehensive sequence of subjects.\nIn the process of the literature digest, we identified results, strengths, weaknesses and research opportunities.\n\n11 Not even the term IBT is universally adopted.12 ITL makes the opposite path we are taking, bringing concepts of machine learning to information theory problems.\nPrincipe, Jose C. 2010. Information Theoretic Learning: Renyi’s Entropy and Kernel Perspectives. Springer Science & Business Media.\n\nChaudhari, P., and S. Soatto. 2018. “Stochastic Gradient Descent Performs Variational Inference, Converges to Limit Cycles for Deep Networks.” In 2018 Information Theory and Applications Workshop (ITA), 1–10. https://doi.org/10.1109/ITA.2018.8503224.\n\nChaudhari, Pratik, Anna Choromanska, Stefano Soatto, Yann LeCun, Carlo Baldassi, Christian Borgs, Jennifer Chayes, Levent Sagun, and Riccardo Zecchina. 2019. “Entropy-Sgd: Biasing Gradient Descent into Wide Valleys.” Journal of Statistical Mechanics: Theory and Experiment 2019 (12)."
  },
  {
    "objectID": "Chapters/context.html#contributions",
    "href": "Chapters/context.html#contributions",
    "title": "4  Introduction",
    "section": "4.4 Contributions",
    "text": "4.4 Contributions\nIn the research conducted, we produced three main results that, to the extent of our knowledge, are original:\n\nThe dissertation itself is the main expected result: a comprehensive digest of the IBT literature and a snapshot analysis of the field in its current form, focusing on its theoretical implications for generalisation.\nWe propose an Information-Theoretical learning problem different from MDL proposed by  (Hinton and Van Camp 1993) for which we derived bounds using Shannon’s . These results, however, are only indicative as they lack peer review to be validated.\nWe present a critique on Achille (2019)’s explanation (Achille 2019; Achille and Soatto 2018) for the role of layers in Deep Representation in the IBT perspective ([sec:achille_proof_critique]), pointing out a weakness in the argument that, as far as we know, has not yet been presented. We then propose a counter-intuitive hypothesis that layers reduce the model’s “effective” hypothesis space. This hypothesis is not formally proven in the present work, but we try to give the intuition behind it ([sec:proposed_hypothesis]). This result has not yet been validated as well.\n\n\nHinton, Geoffrey E, and Drew Van Camp. 1993. “Keeping the Neural Networks Simple by Minimizing the Description Length of the Weights.” In Proceedings of the Sixth Annual Conference on Computational Learning Theory, 5–13.\n\nAchille, Alessandro. 2019. “Emergent Properties of Deep Neural Networks.” PhD thesis, UCLA. https://escholarship.org/uc/item/8gb8x6w9.\n\nAchille, Alessandro, and Stefano Soatto. 2018. “Emergence of Invariance and Disentangling in Deep Representations.” J. Mach. Learn. Res. 19 (1): 1947–80."
  },
  {
    "objectID": "Chapters/context.html#dissertation-preview-and-outline",
    "href": "Chapters/context.html#dissertation-preview-and-outline",
    "title": "4  Introduction",
    "section": "4.5 Dissertation preview and outline",
    "text": "4.5 Dissertation preview and outline\nThe dissertation is divided into two main parts ([pt:background] and [pt:emergence_of_theory]), with a break in the middle ([pt:intermezzo]).\n\nBackground ([pt:background])\n\nChapter 2–Artificial Intelligence: The chapter defines what artificial intelligence is, presents the epistemological differences of intelligent agents in history, and discusses their consequences to machine learning theory.\nChapter 3 — Probability Theory: The chapter derives propositional calculus and probability theory from a list of desired characteristics for epistemic agents. It also presents basic Probability Theory concepts.\nChapter 4 — Machine Learning Theory: The chapter presents the theoretical framework of Machine Learning, the PAC model, theoretical guarantees for generalisation, and expose its weaknesses concerning Deep Learning phenomena.\nChapter 5 — Information Theory: The chapter derives Shannon Information from Probability Theory, explicates some implicit assumptions, and explains basic Information Theory concepts.\n\nIntermezzo ([pt:intermezzo])\n\nChapter 6 — Information-Theoretical Epistemology: This chapter closes the background part and opens the IBT part of the dissertation. It shows the connection of IT and MLT in the learning problem, proves that Shannon theorems can be used to prove PAC bounds and present the MDL Principle, an earlier example of this kind of connection.\n\nThe emergence of a theory ([pt:emergence_of_theory])\n\nChapter 7 — IB Principle: Explains the IB method and its tools: KL as a natural distortion (loss) measure, the IB Lagrangian and the Information Plane.\nChapter 8 — IB and Representation Learning: Presents the learning problem in the IBT perspective (not specific to DL). It shows how some usual choices of the practice of DL emerge naturally from a list of desired properties of representations. It also shows that the information in the weights bounds the information in the activations.\nChapter 9 — IB and Deep Learning: This chapter presents the IBT perspective specific to Deep Learning. It presents IBT analysis of Deep Learning training, some examples of applications of IBT to improve or create algorithms; and the IBT learning theory of Deep Learning. We also explain Deep Learning phenomena in the IBT perspective.\nChapter 10 — Conclusion: In this chapter, we present a summary of the findings, answer the research questions, and present suggestions for future work.\n\n\nWe found out that IBT does not invalidate MLT; it just interprets complexity not as a function of the data (number of parameters) but as a function of the information contained in the data. With this interpretation, there is no paradox in improving generalisation by adding layers.\nFurthermore, they both share more or less the same “genealogy” of assumptions. IBT can be seen as particular case of MLT. Nevertheless, IBT allows us to better understand the training process and provide a different narrative that helps us comprehend Deep Learning phenomena in a more general way."
  }
]