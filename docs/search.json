[
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "The emergence of\nan Information Bottleneck Theory\nof Deep Learning\n",
    "section": "",
    "text": "Welcome\nThis is a showcase of the Tufte-Quarto project type.\nSee pdf.\n\n\n\n\nFigure 1: A thesis built with Tufte-Quarto. Click to download pdf."
  },
  {
    "objectID": "Chapters/context.html#problem",
    "href": "Chapters/context.html#problem",
    "title": "Introduction",
    "section": "Problem",
    "text": "Problem\n\n\n\n\nFigure 1.2: Source: https://xkcd.com/1838/. Reprinted with permission.\n\nIn the last decade, we have witnessed a myriad of astonishing successes in Deep Learning. Despite those many successes in research and industry applications, we may again be climbing a peak of inflated expectations. If in the past, the false solution was to “add computation power” on problems, today we try to solve them by “piling data” (Figure 1.2). Such behaviour has triggered a winner-takes-all competition for who collects more data (our data) amidst a handful of large corporations, raising ethical concerns about privacy and concentration of power (O’Neil 2016).\nNevertheless, we know that learning from way fewer samples is possible: humans show a much better generalisation ability than our current state-of-the-art artificial intelligence. To achieve such needed generalisation power, we may need to understand better how learning happens in deep learning. Rethinking generalisation might reshape the foundations of machine learning theory (Zhang et al. 2016).\n\nPossible new explanation in the horizon\nIn 2015,  Tishby and Zaslavsky (2015) proposed a theory of deep learning  (Tishby and Zaslavsky 2015) based on the information-theoretical concept of the bottleneck principle, of which Tishby is one of the authors. Later, in 2017,  Shwartz-Ziv and Tishby (2017) followed up on the IBT with the paper  , which was presented in a well-attended workshop8, with appealing visuals that clearly showed a “phase transition” happening during training. The video posted on Youtube (Tishby 2017) became a “sensation”9, and received a wealth of publicity when well-known researchers like Geoffrey Hinton10, Samy Bengio (Apple) and Alex Alemi (Google Research) have expressed interest in Tishby’s ideas (Wolchover 2017). they are called formal languages.8 Deep Learning: Theory, Algorithms, and Applications. Berlin, June 2017 http://doc.ml.tu-berlin.de/dlworkshop20179 By the time of this writing, this video as more than 84,000 views, which is remarkable for an hour-long workshop presentation in an academic niche. https://youtu.be/bLqJHjXihK810 Another Deep Learning Pioneer and Turing award winner (2018).\n\n‘I believe that the information bottleneck idea could be very important in future deep neural network research.’ — Alex Alemi\n\nAndrew Saxe (Harvard University) rebutted  Shwartz-Ziv and Tishby (2017) claims in   and was followed by other critics. According to Saxe, it was impossible to reproduce  (Shwartz-Ziv and Tishby 2017)’s experiments with different parameters.\nHas the initial enthusiasm on the IBT been unfounded? Have we let us “fool ourselves” by beautiful charts and a good story?\n\n\nProblem statement\nThe practice of modern machine learning has outpaced its theoretical development. In particular, deep learning models present generalisation capabilities unpredicted by the current machine learning theory. There is yet no established new general theory of learning which handles this problem.\nIBT was proposed as a possible new theory with the potential of filling the theory-practice gap. Unfortunately, to the extent of our knowledge, there is still no comprehensive digest of IBT nor an analysis of how it relates to current MLT."
  },
  {
    "objectID": "Chapters/context.html#objective",
    "href": "Chapters/context.html#objective",
    "title": "Introduction",
    "section": "Objective",
    "text": "Objective\nThis dissertation aims to investigate to what extent can the emergent Information Bottleneck Theory help us better understand Deep Learning and its phenomena, especially generalisation, presenting its strengths, weaknesses and research opportunities.\n\nResearch Questions\n\nWhat are the fundamentals of IBT? How do they differ from the ones from MLT?\nWhat is the relationship between IBT and current MLT? How different or similar they are?\nIs IBT capable of explaining the phenomena MLT already explains?\nDoes IBT invalidate results in MLT?\nIs IBT capable of explaining phenomena still not well understood by MLT?\nWhat are Information Bottleneck Theory’s (IBT) strengths?\nWhat are Information Bottleneck Theory’s (IBT) weaknesses?\nWhat has been already developed in IBT?\nWhat are Information Bottleneck Theory’s (IBT) research opportunities?"
  },
  {
    "objectID": "Chapters/context.html#methodology",
    "href": "Chapters/context.html#methodology",
    "title": "Introduction",
    "section": "Methodology",
    "text": "Methodology\n\nGiven that IBT is yet not a well-established learning theory, there were two difficulties that the research had to address:\n\nThere is a growing interest in the subject, and new papers are published every day. It was essential to select literature and restrain the analysis.\nEarly on, the marks of an emergent theory in its infancy manifested in the form of missing assumptions, inconsistent notation, borrowed jargon, and seeming missing steps. Foremost, it was unclear what was missing from the theory and what was missing in our understanding.\n\nAn initial literature review on IBT was conducted to define the scope.11 We then chose to narrow the research to theoretical perspective on generalisation, where we considered that it could bring fundamental advances. We made the deliberate choice of going deeper in a limited area of IBT and not broad, leaving out a deeper experimental and application analysis, all the work on ITL12  (Principe 2010) and statistical-mechanics-based analysis of SGD  (P. Chaudhari and Soatto 2018; Pratik Chaudhari et al. 2019). From this set of constraints, we chose a list of pieces of IBT literature to go deeper.\n\nIn order to answer , we discuss the epistemology of AI to choose fundamental axioms (definition of intelligence and the definition of knowledge) with which we deduced from the ground up MLT, IT and IBT, revealing hidden assumptions, pointing out similarities and differences. By doing that, we built a “genealogy” of these research fields. This comparative study was essential for identifying missing gaps and research opportunities.\nIn order to answer , we first dissected the selected literature ([[ch:literature]][3]) and organised scattered topics in a comprehensive sequence of subjects.\nIn the process of the literature digest, we identified results, strengths, weaknesses and research opportunities.\n\n11 Not even the term IBT is universally adopted.12 ITL makes the opposite path we are taking, bringing concepts of machine learning to information theory problems."
  },
  {
    "objectID": "Chapters/context.html#contributions",
    "href": "Chapters/context.html#contributions",
    "title": "Introduction",
    "section": "Contributions",
    "text": "Contributions\nIn the research conducted, we produced three main results that, to the extent of our knowledge, are original:\n\nThe dissertation itself is the main expected result: a comprehensive digest of the IBT literature and a snapshot analysis of the field in its current form, focusing on its theoretical implications for generalisation.\nWe propose an Information-Theoretical learning problem different from MDL proposed by  (Hinton and Van Camp 1993) for which we derived bounds using Shannon’s . These results, however, are only indicative as they lack peer review to be validated.\nWe present a critique on Achille (2019)’s explanation (Achille 2019; Achille and Soatto 2018) for the role of layers in Deep Representation in the IBT perspective (?sec-achille_proof_critique), pointing out a weakness in the argument that, as far as we know, has not yet been presented. We then propose a counter-intuitive hypothesis that layers reduce the model’s “effective” hypothesis space. This hypothesis is not formally proven in the present work, but we try to give the intuition behind it (?sec-proposed_hypothesis). This result has not yet been validated as well."
  },
  {
    "objectID": "Chapters/context.html#dissertation-preview-and-outline",
    "href": "Chapters/context.html#dissertation-preview-and-outline",
    "title": "Introduction",
    "section": "Dissertation preview and outline",
    "text": "Dissertation preview and outline\nThe dissertation is divided into two main parts (Background and The emergence of a theory), with a break in the middle (Intermezzo).\n\nBackground\n\nChapter 2 — Artificial Intelligence: The chapter defines what artificial intelligence is, presents the epistemological differences of intelligent agents in history, and discusses their consequences to machine learning theory.\nChapter 3 — Probability Theory: The chapter derives propositional calculus and probability theory from a list of desired characteristics for epistemic agents. It also presents basic Probability Theory concepts.\nChapter 4 — Machine Learning Theory: The chapter presents the theoretical framework of Machine Learning, the PAC model, theoretical guarantees for generalisation, and expose its weaknesses concerning Deep Learning phenomena.\nChapter 5 — Information Theory: The chapter derives Shannon Information from Probability Theory, explicates some implicit assumptions, and explains basic Information Theory concepts.\n\nIntermezzo\n\nChapter 6 — Information-Theoretical Epistemology: This chapter closes the background part and opens the IBT part of the dissertation. It shows the connection of IT and MLT in the learning problem, proves that Shannon theorems can be used to prove PAC bounds and present the MDL Principle, an earlier example of this kind of connection.\n\n\n\n\n\n\n\nIBT “genealogy” tree.\n\n\n\nThe emergence of a theory\n\nChapter 7 — IB Principle: Explains the IB method and its tools: KL as a natural distortion (loss) measure, the IB Lagrangian and the Information Plane.\nChapter 8 — IB and Representation Learning: Presents the learning problem in the IBT perspective (not specific to DL). It shows how some usual choices of the practice of DL emerge naturally from a list of desired properties of representations. It also shows that the information in the weights bounds the information in the activations.\nChapter 9 — IB and Deep Learning: This chapter presents the IBT perspective specific to Deep Learning. It presents IBT analysis of Deep Learning training, some examples of applications of IBT to improve or create algorithms; and the IBT learning theory of Deep Learning. We also explain Deep Learning phenomena in the IBT perspective.\nChapter 10 — Conclusion: In this chapter, we present a summary of the findings, answer the research questions, and present suggestions for future work.\n\n\nWe found out that IBT does not invalidate MLT; it just interprets complexity not as a function of the data (number of parameters) but as a function of the information contained in the data. With this interpretation, there is no paradox in improving generalisation by adding layers.\nFurthermore, they both share more or less the same “genealogy” of assumptions. IBT can be seen as particular case of MLT. Nevertheless, IBT allows us to better understand the training process and provide a different narrative that helps us comprehend Deep Learning phenomena in a more general way.\n\n\n\n\n\n\n\n\n\n\nAchille, Alessandro. 2019. “Emergent Properties of Deep Neural Networks.” PhD thesis, UCLA. https://escholarship.org/uc/item/8gb8x6w9.\n\n\nAchille, Alessandro, and Stefano Soatto. 2018. “Emergence of Invariance and Disentangling in Deep Representations.” J. Mach. Learn. Res. 19 (1): 1947–80.\n\n\nChaitin, Gregory. 2006. Meta Math! The Quest for Omega. Vintage Books.\n\n\nChaudhari, Pratik, Anna Choromanska, Stefano Soatto, Yann LeCun, Carlo Baldassi, Christian Borgs, Jennifer Chayes, Levent Sagun, and Riccardo Zecchina. 2019. “Entropy-Sgd: Biasing Gradient Descent into Wide Valleys.” Journal of Statistical Mechanics: Theory and Experiment 2019 (12).\n\n\nChaudhari, P., and S. Soatto. 2018. “Stochastic Gradient Descent Performs Variational Inference, Converges to Limit Cycles for Deep Networks.” In 2018 Information Theory and Applications Workshop (ITA), 1–10. https://doi.org/10.1109/ITA.2018.8503224.\n\n\nFarrington, Karen. 2016. The Blitzed City: The Destruction of Coventry, 1940. London: Aurum Press.\n\n\nFeynman, Richard. 1994. The Character of Physical Law. Modern Library.\n\n\nGleiser, Marcelo, and Damian Sowinski. 2018. “The Map and the Territory.” In The Frontiers Collection, edited by Shyam Wuppuluri and Francisco Antonio Doria. Springer International Publishing. https://doi.org/10.1007/978-3-319-72478-2.\n\n\nHinton, Geoffrey E, and Drew Van Camp. 1993. “Keeping the Neural Networks Simple by Minimizing the Description Length of the Weights.” In Proceedings of the Sixth Annual Conference on Computational Learning Theory, 5–13.\n\n\nKlein, Martin J. 1974. “Carnots Contribution to Thermodynamics.” Physics Today 27 (8): 23–28. https://doi.org/10.1063/1.3128802.\n\n\nLipton, Zachary C., and Jacob Steinhardt. 2018. “Troubling Trends in Machine Learning Scholarship.” https://arxiv.org/abs/1807.03341.\n\n\nO’Neil, Cathy. 2016. Weapons of Math Destruction: How Big Data Increases Inequality and Threatens Democracy. USA: Crown Publishing Group.\n\n\nPierce, John R. n.d. An Introduction to Information Theory: Symbols, Signals and Noise. Dover Publications.\n\n\nPopper, Karl. 2004. A Lógica Da Pesquisa Científica. Translated by Leonidas Hegenberg and Octanny Silveira. São Paulo: Cultrix.\n\n\nPrincipe, Jose C. 2010. Information Theoretic Learning: Renyi’s Entropy and Kernel Perspectives. Springer Science & Business Media.\n\n\nRahimi, Ali. 2018. “Ali Rahimi NIPS 2017 Test-of-Time Award Presentation Speech.” https://youtu.be/x7psGHgatGM.\n\n\nRussell, Stuart J., Peter Norvig, and Ernest Davis. 2010. Artificial Intelligence: A Modern Approach. 3rd ed. Prentice Hall  Series in Artificial Intelligence. Prentice Hall.\n\n\nShannon, Claude E. 1948. “A Mathematical Theory of Communication.” Bell System Technical Journal 27 (3): 379–423.\n\n\nShwartz-Ziv, Ravid, and Naftali Tishby. 2017. “Opening the Black Box of Deep Neural Networks via Information.” https://arxiv.org/abs/1703.00810.\n\n\nTishby, Naftali. 2017. “Information Theory of Deep Learning.” https://youtu.be/bLqJHjXihK8. https://youtu.be/bLqJHjXihK8.\n\n\nTishby, Naftali, and Noga Zaslavsky. 2015. “Deep Learning and the Information Bottleneck Principle.” In 2015 IEEE Information Theory Workshop (ITW), 1–5. IEEE.\n\n\nWolchover, Natalie. 2017. “New Theory Cracks Open the Black Box of Deep Learning.” https://www.quantamagazine.org/new-theory-cracks-open-the-black-box-of-deep-learning-20170921/; Simons Foundation.\n\n\nZhang, Chiyuan, Samy Bengio, Moritz Hardt, Benjamin Recht, and Oriol Vinyals. 2016. “Understanding Deep Learning Requires Rethinking Generalization.” https://arxiv.org/abs/1611.03530."
  },
  {
    "objectID": "Chapters/ai.html#artificial-intelligence",
    "href": "Chapters/ai.html#artificial-intelligence",
    "title": "Artificial Intelligence",
    "section": "What is Artificial Intelligence?",
    "text": "What is Artificial Intelligence?\n\nDefinition 2.1 AI is the branch of Computer Science that studies general principles of intelligent agents and how to construct them (Russell, Norvig, and Davis 2010).\n\nThis definition uses the terms intelligence and intelligent agents, so let us start from them.\n\nWhat is intelligence?\nDespite a long history of research, there is still no consensual definition of intelligence.1 Whatever it is, though, humans are particularly proud of it. We even call our species homo sapiens, as intelligence was an intrinsic human characteristic.1 For a list with 70 definitions of intelligence, see Legg and Hutter (2007).\nIn this dissertation:\n\nDefinition 2.2 Intelligence is the ability to predict a course of action to achieve success in specific goals.\n\n\n\nIntelligent Agents\nUnder our generous definition, intelligence is not limited to humans. It applies to any agent2: animal or machine. For example, a bacteria can perceive its environment through chemical signals, process them, and then produce chemicals to signal other bacteria. An air-conditioning can observe temperature changes, know its state, and adapt its functioning, turning off if it is cold or on if it is hot — intelligence exempts understanding. The air-conditioning does not comprehend what it is doing. The same way a calculator does not know arithmetics.2 An agent is anything that perceives its environment and acts on it.\n\n\nA strange inversion of reasoning\nThis competence without comprehension is what the philosopher Daniel Dennett calls Turing’s strange inversion of reasoning3. The idea of a strange inversion comes from one of Darwin’s 19th-century critics Dennett (2009):3 In his work, Turing discusses if computers can “think”, meaning to examine if they can perform indistinguishably from the way thinkers do.\n\n‘In the theory with which we have to deal, Absolute Ignorance is the artificer; so that we may enunciate as the fundamental principle of the whole system, that, in order to make a perfect and beautiful machine, it is not requisite to know how to make it. This proposition will be found, on careful examination, to express, in condensed form, the essential purport of the [Evolution] Theory, and to express in a few words all Mr Darwin’s meaning; who, by a strange inversion of reasoning, seems to think Absolute Ignorance fully qualified to take the place of Absolute Wisdom in all of the achievements of creative skill.’\n— Robert MacKenzie\n\n\nCounterintuitively to  MacKenzie (1868) and many others to this date, intelligence can emerge from absolute ignorance. Turing’s strange inversion of reasoning comes from the realisation that his automata can perform calculations by symbol manipulation, proving that it is possible to build agents that behave intelligently, even if they are entirely ignorant of the meaning of what they are doing (Turing 2007)."
  },
  {
    "objectID": "Chapters/ai.html#dreaming-of-robots",
    "href": "Chapters/ai.html#dreaming-of-robots",
    "title": "Artificial Intelligence",
    "section": "Dreaming of robots",
    "text": "Dreaming of robots\n\nFrom mythology to Logic\nThe idea of creating an intelligent agent is perhaps as old as humans. There are accounts of artificial intelligence in almost any ancient mythology: Greek, Etruscan, Egyptian, Hindu, Chinese (Mayor 2018). For example, in Greek mythology, the story of the bronze automaton of Talos built by Hephaestus, the god of invention and blacksmithing, first mentioned around 700 BC.\nThis interest may explain why, since ancient times, philosophers have looked for mechanical methods of reasoning. Chinese, Indian and Greek philosophers all developed formal deduction in the first millennium BC.In particular, Aristotelian syllogism, laws of thought, provided patterns for argument structures to yield irrefutable conclusions, given correct premises. These ancient developments were the beginning of the field we now call Logic.\n\n\nRationalism: The Cartesian view of Nature\n\n\n\n\n\n\nExample of Lull’s Ars Magna’s paper discs.\n\nIn the 13th century, the Catalan philosopher Ramon Lull wanted to produce all statements the human mind can think. For this task, he developed logic paper machines, discs of paper filled with esoteric coloured diagrams that connected symbols representing statements. Unfortunately, according to Gardner (1959), in a modern reassessment of his work, “it is impossible, perhaps, to avoid a strong sense of anticlimax”. With megalomaniac self-esteem that suggests psychosis, his delusional sense of importance is more characteristic of cult founders. On the bright side, his ideas and books exerted some magic appeal that helped them be rapidly disseminated through all Europe.\nLull’s work greatly influenced Leibniz and Descartes, who, in the 17thcentury, believed that all rational thought could be mechanised. This belief was the basis of rationalism, the epistemic view of the Enlightenment that regarded reason as the sole source of knowledge. In other words, they believed that reality has a logical structure and that certain truths are self-evident, and all truths can be derived from them.\nThere was considerable interest in developing artificial languages during this period. Nowadays, they are called formal languages.\n\n‘If controversies were to arise, there would be no more need for disputation between two philosophers than between two accountants. For it would suffice to take their pencils in their hands, to sit down to their slates, and to say to each other: Let us calculate.’\n— Gottfried Leibniz\n\nThe rationalist view of the world has had an enduring impact on society until today. In the 19thcentury, George Boole and others developed a precise notation for statements about all kinds of objects in Nature and their relations. Before them, Logic was philosophical rather than mathematical. The name of Boole’s masterpiece, “The Laws of Thought”, is an excellent indicator of his Cartesian worldview.\nAt the beginning of the 20th century, some of the most famous mathematicians, David Hilbert, Bertrand Russel, Alfred Whitehead, were still interested in formalism: they wanted mathematics to be formulated on a solid and complete logical foundation. In particular, Hilbert’s Entscheidungs Problem (decision problem) asked if there were limits to mechanical Logic proofs (Chaitin 2006).\nKurt Gödel’s incompleteness theorem (1931) proved that any language expressive enough to describe arithmetics of the natural numbers is either incomplete or inconsistent. This theorem imposes a limit on logic systems. There will always be truths that will not be provable from within such languages: there are “true” statements that are undecidable.\nAlan Turing brought a new perspective to the Entscheidungs Problem: a function on natural numbers that an algorithm in a formal language cannot represent cannot be computable (Chaitin 2006). Gödel’s limit appears in this context as functions that are not computable,  no algorithm can decide whether another algorithm will stop or not (the halting problem). To prove that, Turing developed a whole new general theory of computation: what is computable and how to compute it, laying out a blueprint to build computers, and making possible Artificial Intelligence research as we know it. An area in which Turing himself was very much invested.\n\n\nEmpiricism: The sceptical view of Nature\n\n\n\n\n\nDavid Hume, Scottish Enlightenment philosopher, historian, economist, librarian and essayist.\n\n\nThe response to rationalism was empiricism, the epistemological view that knowledge comes from sensory experience, our perceptions of the world. Locke explains this with the peripatetic axiom4: “there is nothing in the intellect that was not previously in the senses” (Uzgalis 2020). Bacon, Locke and Hume were great exponents of this movement, which established the grounds of the scientific method.4 This citation is the principle from the Peripatetic school of Greek philosophy and is found in Thomas Aquinas’ work cited by Locke.\nDavid Hume, in particular, presented in the 18th century a radical empiricist view: reason only does not lead to knowledge. In (Hume 2009), Hume distinguishes relations of ideas, propositions that derive from deduction and matters of facts, which rely on the connection of cause and effect through experience (induction). Hume’s critiques, known as the Problem of Induction, added a new slant on the debate of the emerging scientific method.\nFrom Hume’s own words:\n\n‘The bread, which I formerly eat, nourished me; that is, a body of such sensible qualities was, at that time, endued with such secret powers: but does it follow, that other bread must also nourish me at another time, and that like sensible qualities must always be attended with like secret powers? The consequence seems nowise necessary.’\n— David Hume\n\nThere is no logic to deduce that the future will resemble the past. Still, we expect uniformity in Nature. As we see more examples of something happening, it is wise to expect that it will happen in the future just as it did in the past. There is, however, no rationality5 in this expectation.5 In the philosophical sense.\nHume explains that we see conjunction repeatedly, “bread” and “nourish”, and we expect uniformity in Nature; we hope that “nourish” will always follow “eating bread”; When we fulfil this expectancy, we misinterpret it as causation. In other words, we project causation into phenomena. Hume explained that this connection does not exist in Nature. We do not “see causation”; we create it.\nThis projection is Hume’s strange inversion of reasoning (Huebner 2017): We do not like sugar because it is sweet; sweetness exists because we like (or need) it. There is no sweetness in honey. We wire our brain so that glucose triggers a labelled desire we call sweetness. As we will see later, sweetness is information. This insight shows the pattern matching nature of humans. Musicians have relied on this for centuries. Music is a sequence of sounds in which we expect a pattern. The expectancy is the tension we feel while the chords progress. When the progression finally resolves, forming a pattern, we release the tension. We feel pattern matching in our core. It is very human, it can be beneficial and wise, but it is, stricto sensu, irrational.\nThe epistemology of the sceptical view of Nature is science: to weigh one’s beliefs to the evidence. Knowledge is not absolute truth but justified belief. It is a Babylonian epistemology.\nIn rationalism, Logic connects knowledge and good actions. In empiricism, the connection between knowledge and justifiable actions is determined by probability. More specifically, Bayes’ theorem. As Jaynes puts it, probability theory is the “Logic of Science” . 66 The Bayes’ theorem is attributed to the Reverend Thomas Bayes after the posthumous publication of his work. By the publication time, it was an already known theorem, derived by Laplace.\n\n\nThe birth of AI as a research field\nIn 1943, McCulloch and Pitts, a neurophysiologist and a logician, demonstrated that neuron-like electronic units could be wired together, act and interact by physiologically plausible principles and perform complex logical calculations (Russell, Norvig, and Davis 2010). Moreover, they showed that any computable function could be computed by some network of connected neurons (McCulloch and Pitts 1943). Their work marks the birth of ANNs, even before the field of AI had this name. It was also the birth of Connectionism, using artificial neural networks, loosely inspired by biology, to explain mental phenomena and imitate intelligence.\n\n\n\n\nFigure 2.1: Claude Shannon, father of “information theory”.\n\nTheir work inspired John von Neumann’s demonstration of how to create a universal Turing machine out of electronic components, which lead to the advent of computers and programming languages. Ironically, these advents hastened the ascent of the formal logicist approach called Symbolism, disregarding Connectionism.\nIn 1956, John McCarthy, Claude Shannon (who invented Information Theory, Figure 2.1), Marvin Minsky and Nathaniel Rochester organised a 2-month summer workshop in Dartmouth College to bring researchers of different fields concerned with “thinking machines” (cybernetics, information theory, automata theory). The workshop attendees became a community of researchers and chose the term “artificial intelligence” for the field.\n\n \n\n\n\nFigure 2.2: The Blind Men and the Elephant.\n\n\n\n\nIt was six men of Indostan\nTo learning much inclined,\nWho went to see the Elephant\n(Though all of them were blind),\nThat each by observation\nMight satisfy his mind\n\n—John Godfrey Saxe,\nThe Blind Men and the Elephant"
  },
  {
    "objectID": "Chapters/ai.html#building-intelligent-agents",
    "href": "Chapters/ai.html#building-intelligent-agents",
    "title": "Artificial Intelligence",
    "section": "Building Intelligent Agents",
    "text": "Building Intelligent Agents\n\nAnatomy of intelligent agents\nLike the blind men in the parable, an intelligent agent shall model her understanding of Nature from limited sensory data.\n\n\n\nFigure 2.3: Anatomy of an Intelligent Agent. Inspired by art in Russell, Norvig, and Davis (2010).\n\n\n\nThe expected result of this conversation is a change in the agent’s KB, therefore in her model and, more importantly, her future decisions. The model is an abstraction of how the agent “thinks” the world is (her “mental picture” of the environment). Therefore, it should be consistent with it: if something is true in Nature, it is equally valid, mutatis mutandis, in the model. A Model should also be as simple as possible so that the agent can make decisions that maximise a chosen performance measure, but not simpler. As the agent knows more about Nature, less it gets surprised by it.\nThis rudimentary anatomy is flexible enough to entail different epistemic views, like the rationalist (mathematical) and the empiricist (scientific); different approaches to how to implement the knowledge base (it can be learned, therefore updatable, or it can be set in stone from an expert prior knowledge); and also from how to implement it (a robot or software).\nNoteworthy, though, is that the model that transforms input data into decisions should be the target of our focus.\n\n\nSymbolism\nSymbolism is the pinnacle of rationalism. In the words of Thomas Hobbes, one of the forerunners of rationalism, “thinking is the manipulation of symbols and reasoning is computation”. Symbolism is the approach to building intelligent agents that does just that. It attempts to represent knowledge with a formal language and explicitly connects the knowledge with actions. It is competence from comprehension. In other words, it is programmed.\nEven though McCulloch and Pitts work on artificial neural networks predates Von Neumann’s computers, Symbolism dominated AI until the \\(1980\\)s. It was so ubiquitous that symbolic AI is even called “good old fashioned AI” (Russell, Norvig, and Davis 2010).\nThe symbolic approach can be traced back to Nichomachean Ethics (Aristotle 2000):\n\n‘We deliberate not about ends but means. For a doctor does not deliberate whether he shall heal, nor an orator whether he shall persuade, nor a statesman whether he shall produce law and order, nor does anyone else deliberate about his end. They assume the end and consider how and by what means it is to be attained; and if it seems to be produced by several means, they consider by which it is most easily and best produced, while if it is achieved by one only they consider how it will be achieved by this and by what means this will be achieved, till they come to the first cause, which in the order of discovery is last.’\n— Aristotle \n\nThis perspective is so entrenched that Russell, Norvig, and Davis (2010, 7) still says: “(\\(\\ldots\\)) Only by understanding how actions can be justified can we understand how to build an agent whose actions are justifiable”; even though, in the same book, they cover machine learning (which we will address later in this chapter) without noticing it is proof that there are other ways to build intelligent agents. Moreover, it is also a negation of competence without comprehension. It seems that even for AI researchers, the strange inversion of reasoning is uncomfortable ([[ch:introduction]][3]).\nAll humans, even those in prisons and under mental health care, think their actions are justifiable. Is that not an indication that we rationalise our actions ex post facto? We humans tend to think our rational assessments lead to actions, but it is also likely possible that we act and then rationalise afterwards to justify what we have done, fullheartedly believing that the rationalisation came first.\nClaude Shannon’s Theseus\nAfter writing what is probably the most important master’s dissertation of the 20th century and “inventing” IT, what made possible the Information Age we live in today, Claude Shannon enjoyed the freedom to pursue any interest to which his curious mind led him (Soni and Goodman 2017). In the \\(1950\\)s, his interest shifted to building artificial intelligence. He was not a typical academic, in any case. A lifelong tinkerer, he liked to “think” with his hand as much as with his mind. Besides developing an algorithm to play chess (when he even did not have a computer to run it), one of his most outstanding achievements in AI was Theseus, a robotic maze-solving mouse.77 Many AI students will recognise in Theseus the inspiration to Russel and Norvig’s Wumpus World.\nTo be more accurate, Theseus was just a bar magnet covered with a sculpted wooden mouse with copper whiskers; the maze was the “brain” that solved itself (Klein 2018).\n\n‘Under the maze, an electromagnet mounted on a motor-­powered carriage can move north, south, east, and west; as it moves, so does Theseus. Each time its copper whiskers touch one of the metal walls and complete the electric circuit, two things happen. First, the corresponding relay circuit’s switch flips from “on” to “off,” recording that space as having a wall on that side. Then Theseus rotates \\(90^{\\circ}\\) clockwise and moves forward. In this way, it systematically moves through the maze until it reaches the target, recording the exits and walls for each square it passes through.’\n— Martin Klein\n\nSymbolic AI problems\nSeveral symbolic AI projects sought to hard-code knowledge about domains in formal languages, but it has always been a costly, slow process that could not scale.\nAnyhow, by \\(1965\\), there were already programs that could solve any solvable problem described in logical notation (Russell, Norvig, and Davis 2010, 4). However, hubris and lack of philosophical perspective made computer scientists believe that “intelligence was a problem about to be solved8.”8 Marvin Minsky, head of the artificial intelligence laboratory at MIT (\\(1967\\))\nThose inflated expectations lead to disillusionment and funding cuts9 (Russell, Norvig, and Davis 2010). They failed to estimate the inherent difficulty in slating informal knowledge in formal terms: the world has many shades of grey. Besides, complexity theory had yet to be developed: they did not count on the exponential explosion of their problems.9 Sometimes called winters.\n\n\nConnectionism: a different approach\nThe fundamental idea in Connectionism is that intelligent behaviour emerges from a large number of simple computational units when networked together (Goodfellow, Bengio, and Courville 2016).\nIt was pioneered by McCulloch and Pitts in 1943 (McCulloch and Pitts 1943). One of Connectionism’s first wave developments was Frank Rosenblatt’s Perceptron, an algorithm for learning binary classifiers, or more specifically threshold functions:\n\\[\n\\begin{aligned}          \n     y=\n     \\begin{cases}\n         1 \\text{ if } \\mW\\vx + \\vb > 0\\\\\n         0 \\text{ otherwise }\n     \\end{cases}\n\\end{aligned}\n\\]\nwhere \\(\\mW\\) is the vector of weights, \\(\\vx\\) is the input vector, \\(\\vb\\) is a bias, and \\(\\vy\\) is the classification. In neural networks, a perceptron is an artificial neuron using a step function as the activation function.\n\n\n\n\n\n\n\n\nBuilding in Harare, Zimbabwe, is mod- elled after termite mounds. Photo by Mike Pearce.\n\n\n\n\n\n\n\nCathedral termite mound, Australia. Photo by Awoisoak Kaosiowa, 2008.\n\n\n\n\nFigure 2.4: Biomimicry of termite technique achieves superior energy efficiency in buildings.\n\n\nSee Figure 2.4, termites self-cooling mounds keep the temperature inside at exactly \\(31^{\\circ} C\\), ideal for their fungus-farming; while the temperatures outside range from 2 to \\(40^{\\circ} C\\) throughout the day. Such building techniques inspired architect Mike Pearce to design a shopping mall that uses a tenth of the energy used by a conventional building of the same size.\nFrom where does termites intelligence come?\n\n‘Individual termites react rather than think, but at a group level, they exhibit a kind of cognition and awareness of their surroundings. Similarly, in the brain, individual neurons do not think, but thinking arises in their connections.’\n— Radhika Nagpal, Harvard University (Margonelli 2016).\n\nSuch collective intelligence happens in groups of just a couple of million termites. There are around 80 to 90 billion neurons in the human brain, each less capable than a termite, but collectively they show incomparable intelligence capabilities.\n\n\n\nA brief history of connectionism.\n\n\nIn contrast with the symbolic approach, in neural networks, the knowledge is not explicit in symbols but implicit in the strength of the connections between the neurons. Besides, it is a very general and flexible approach since these connections can be updated algorithmically: they are algorithms that learn: the connectionist approach is an example of what we now call Machine Learning.\n\n\nMachine Learning\n\n\n\n\nFigure 2.5: Is this a cat?\n\nLook at Figure 2.5. Is this a picture of a cat? How to write a program to do such a simple classification task (cat/no cat)? One could develop clever ways to use features from the input picture and process them to guess. Though, it is not an easy program to design. Worse, even if one manages to program such a task, how much would it worth to accomplish a related task, to recognise a dog, for example? For long, this was the problem of researchers in many areas of interest of AI:CV, NLP, Speech Recognition SR; much mental effort was put, with inferior results, in problems that we humans solve with apparent ease.\nThe solution is an entirely different approach for building artificial intelligence: instead of making the program do the task, build the program that outputs the program that does the task. In other words, learning algorithms use “training data” to infer the transformations to the input that generates the desired output.\n\n\nTypes of learning\nMachine Learning can happen in different scenarios, which differ in the availability of training data, how training data is received, and how the test data is used to evaluate the learning. Here, we describe the most typical of them (Mohri, Rostamizadeh, and Talwalkar 2012):\n\nSupervised learning: The most successful scenario. The learner receives a set of labelled examples as training data and makes predictions for unseen data.\nUnsupervised learning: The learner receives unlabelled training data and makes predictions for unseen instances.\nSemi-supervised learning: The learner receives a training sample consisting of labelled and unlabelled data and makes predictions for unseen examples. Semi-supervised learning is usual in settings where unlabelled data is easily accessible, but labelling is too costly.\nReinforcement learning: The learner actively interacts with the environment and receives an immediate reward for her actions. The training and testing phases are intermixed.\n\n\n\nDeep Learning\nThe 2010s have been an AI Renaissance not only in academia but also in the industry. Such successes are mostly due to DL, in particular, supervised deep learning with vast amounts of data trained in GPUs. It was the decade of DL.\n\n‘Deep learning algorithms seek to exploit the unknown structure in the input distribution to discover good representations, often at multiple levels, with higher-level learned features defined in terms of lower-level features.’\n— Joshua Bengio (Bengio 2012)*\n\nThe name is explained by  Goodfellow, Bengio, and Courville (2016): “A graph showing the concepts being built on top of each other is a deep graph. Therefore the name, deep learning” (Goodfellow, Bengio, and Courville 2016). Although it is a direct descendant of the connectionist movement, it goes beyond the neuroscientific perspective in its modern form. It is more a general principle of learning multiple levels of compositions.\nThe quintessential example of a deep learning model is the deep feedforward network or MLP (Russell, Norvig, and Davis 2010).\n\n\nDefinition 2.3 Let,\n\n\\({\\vx}\\) be the input vector \\(\\{\\vx_1, \\ldots, \\vx_m\\}\\)\n\\(k\\) be the layer index, such that \\(k \\in [1,l]\\),\n\\(\\mW^{(k)}_{i,j}\\)be the matrix of weights in the \\(k\\)-th layer, where \\(i \\in [0,d_{k-1}], j \\in [1, d_k] \\text{ and }\\mW^{(k)}_{0,:}\\) are the biases\n\\(\\sigma\\) be a nonlinear function,\n\na MLPs is a neural network where the input is defined by:\n\\[\n\\begin{aligned}          \n    h^{(0)}= 1\\frown \\vx\n\\end{aligned}\n\\]\na hidden layer is defined by:\n\\[\n\\begin{aligned}          \n        h^{(k)}&=\\sigma^{(k)}(\\mW^{(k)~\\top} h^{(k-1)}).\n\\end{aligned}\n\\]\nThe output is defined by: \\[\n\\begin{aligned}          \n  \\hat{y}&=h^{(l)}.\n\\end{aligned}\n\\]\n\nDeep Learning is usually associated with DNNs, but the network architecture is only one of its components:\n\nDNN architecture\nSGD — the optimiser\nDataset\nLoss function\n\nThe architecture is not the sole component essential to current Deep Learning success. The SGD plays a crucial role, and so does the usage of large datasets.\nA known problem, though, is that DNNs are prone to overfitting ([[sec:bias-variance]][6]).  Zhang et al. (2016) show state-of-the-art convolutional deep neural networks can easily fit a random labelling of training data (Zhang et al. 2016)."
  },
  {
    "objectID": "Chapters/ai.html#concluding-remarks",
    "href": "Chapters/ai.html#concluding-remarks",
    "title": "Artificial Intelligence",
    "section": "Concluding Remarks",
    "text": "Concluding Remarks\nThis chapter derived the need for a language from the definitions of intelligence and intelligent agents. An intelligent agent needs language to store her knowledge (what she has learned) and with that to communicate/share this knowledge with its future self and with other agents.\nWe claim (without proving) that a language can be derived from a definition of knowledge: an epistemic choice. We claim that mathematics and science can be seen as languages that differ in consequence of different views on what knowledge is and gave historical background on two epistemic views, Rationalism and Empiricism (Section 2.2.2,Section 2.2.3).\nWe gave historical background on AI and showed that different epistemic views relate to AI movements: Symbolism and Connectionism. We gave some background on basic AI concepts: intelligent agents, machine learning, types of learning, neural networks and deep learning, showing that DL relates to Connectionism and, hence, to science and an empiricist epistemology. Previously (?sec-bringing_science), we have discussed that Computer Science generally relates to the rationalist epistemology. We hope this can help us better understand our research community.\n\nAssumptions\n\nA definition of intelligence Definition 2.2\nAn epistemic choice on the definition of Knowledge Section 2.2.2\n\n\n\n\n\n\n\n\n\n\n\nAristotle. 2000. Aristotle: Nicomachean Ethics. Cambridge Texts in the History of Philosophy. Cambridge University Press. https://doi.org/10.1017/CBO9780511802058.\n\n\nBengio, Yoshua. 2012. “Deep Learning of Representations for Unsupervised and Transfer Learning.” In Proceedings of ICML Workshop on Unsupervised and Transfer Learning, 17–36.\n\n\nChaitin, Gregory. 2006. Meta Math! The Quest for Omega. Vintage Books.\n\n\nDennett, Daniel. 2009. “Darwin’s “Strange Inversion of Reasoning”.” In Proceedings of the National Academy of Sciences, 106:10061–65. Supplement 1. National Academy of Sciences. https://doi.org/10.1073/pnas.0904433106.\n\n\nGardner, Martin. 1959. Logic Machines and Diagrams. McGraw-Hill Book Company.\n\n\nGoodfellow, Ian J., Yoshua Bengio, and Aaron C. Courville. 2016. Deep Learning. Adaptive Computation and Machine Learning. MIT Press.\n\n\nHuebner, Bryce. 2017. The Philosophy of Daniel Dennett. Oxford University Press.\n\n\nHume, David. 2009. Tratado Da Natureza Humana. Editora UNESP.\n\n\nKlein, Daniel. 2018. “Mighty Mouse.” Technology Review. https://www.technologyreview.com/s/612529/mighty-mouse/.\n\n\nLegg, Shane, and Marcus Hutter. 2007. “A Collection of Definitions of Intelligence.” https://arxiv.org/abs/0706.3639.\n\n\nMacKenzie, Robert Beverley. 1868. The Darwinian Theory of the Transmutation of Species Examined. J. Nisbet.\n\n\nMargonelli, Lisa. 2016. “Collective Mind in the Mound: How Do Termites Build Their Huge Structures?” https://www.nationalgeographic.com/news/2014/8/140731-termites-mounds-insects-entomology-science/.\n\n\nMayor, Adrienne. 2018. Gods and Robots: Myths, Machines, and Ancient Dreams of Technology. Princeton University Press.\n\n\nMcCulloch, Warren S., and Walter Pitts. 1943. “A Logical Calculus of the Ideas Immanent in Nervous Activity.” The Bulletin of Mathematical Biophysics 5 (4): 115–33.\n\n\nMohri, Mehryar, Afshin Rostamizadeh, and Ameet Talwalkar. 2012. Foundations of Machine Learning. The MIT Press.\n\n\nRussell, Stuart J., Peter Norvig, and Ernest Davis. 2010. Artificial Intelligence: A Modern Approach. 3rd ed. Prentice Hall  Series in Artificial Intelligence. Prentice Hall.\n\n\nSoni, Jimmy, and Rob Goodman. 2017. A Mind at Play: How Claude Shannon Invented the Information Age. Simon; Schuster.\n\n\nTuring, Alan M. 2007. “Computing Machinery and Intelligence.” In Parsing the Turing Test, 23–65. Springer Netherlands. https://doi.org/10.1007/978-1-4020-6710-5_3.\n\n\nUzgalis, William. 2020. “John Locke.” In The Stanford Encyclopedia of Philosophy, edited by Edward N. Zalta, Spring 2020. https://plato.stanford.edu/archives/spr2020/entries/locke/; Metaphysics Research Lab, Stanford University. https://plato.stanford.edu/archives/spr2020/entries/locke/.\n\n\nZhang, Chiyuan, Samy Bengio, Moritz Hardt, Benjamin Recht, and Oriol Vinyals. 2016. “Understanding Deep Learning Requires Rethinking Generalization.” https://arxiv.org/abs/1611.03530."
  },
  {
    "objectID": "Chapters/prob.html#sec:language_probability",
    "href": "Chapters/prob.html#sec:language_probability",
    "title": "Probability Theory",
    "section": "From Language to Probability",
    "text": "From Language to Probability\n\nFormal Languages\nWe, as intelligent agents, do not know how Nature is; we only know how we perceive it. Our ideas are mental pictures of how we imagine Nature. Like in the story of the blind men and the elephant ([blind_men]), how do we know that our model is the same as someone else’s? Communicating. We need to communicate with each other to check if our mental picture of Nature, our model, is consistent with the experience of others.11 We can take this idea further and think that at any moment, we need to communicate with our past selves to check if new evidence is consistent with our prior model.\nWe use language to describe Nature. However, natural languages, like English, German, Portuguese, are ambiguous, and we need contextual clues and other information to more clearly communicate meaning. To avoid this, an intelligent agent uses formal language.\nA formal language is a mathematical tool created for precise communication about a specific subject. For example, arithmetic is a language for calculations. Chemists have a language that represents the chemical structures of molecules. Programming languages are formal languages that express computations. In a nutshell, a formal language is a set of words (strings) whose letters (symbols) are taken from an alphabet and are well-formed according to a specific set of rules, grammar. Let \\(\\lang= <\\Sigma, \\Phi>\\) be a formal language where: \\[\\begin{aligned}\n    \\Sigma &= \\{S_1, S_2, \\cdots, S_n\\} \\text{ is an alphabet,}\\\\\n    \\Phi &= {\\Phi_1 \\cup \\Phi_2 \\cup \\cdots \\cup \\Phi_k} \\text{ is a set of operations, the grammar,}\n\\end{aligned}\\] and: \\[\\begin{aligned}\n    \\Phi_1 &\\text{ is the set of unary operations}, \\nonumber\\\\\n    \\Phi_2 &\\text{ is the set of binary operations}, \\nonumber\\\\\n    \\cdots &\\nonumber \\\\\n    \\Phi_k &\\text{ is the set of k-ary operations.}\\nonumber\n\\end{aligned}\\] A formal language allows a quantitative description of a state of knowledge and defines how this state can be updated on new evidence.22 An inference method defines the rules for updating knowledge.\nWith this definition, we can also think that a formal language is what Sowinski (2016) calls a realm of discourse, all the valid formed strings3 that one can derive; everything one can say about Nature.3 Strings, words, sentences, propositions, formulae are names used interchangeably through the literature.\nInterestingly, formal languages allow us to manipulate representations of the environment without dealing with their semantics. They are the basis of “Turing’s strange inversion”, (see [turing_strange_inversion]) by doing allowed operations on strings, computers can compute at a superhuman speed and accuracy without ever comprehending what they are doing.\n\n\nFrom Rationalism to Propositional Calculus\n\nRational Agents\ncan form representations of a complex world, use deduction as the inference process to derive updated representations, and use these new representations to decide what to do. In other words, rational agents are the consequence of the epistemological view of rationalism.\nWhen a rational agent establishes a particular statement’s truth value, all statements formed in her knowledge base from that statement instantly feel that update. Therefore, a rational agent cannot hold contradictions.\n\n\nDesiderata for a rational language\nWe want to build a language for rational agents with the following desired characteristics:\n\nknowledge is absolute; a sentence4 can be either true or false;\nunambiguous, a constructed sentence can only have one meaning;\nconsistent; a language without paradoxes, whatever path chosen to derive a sentence truth value will lead to the same assignment;\nminimal; uses the most reduced set of symbols possible.\n\n4 A sentence can be either a single symbol or a string formed with several symbols according to the grammar.Let \\(\\lang_R= <\\Sigma_R, \\Phi_R>\\) be the formal language built from these constraints, where sentences are either axiom symbols or compounded sentences formed using special symbols called operators, each operator denoting one operation, \\(\\phi \\in \\Phi_R\\).\nIt is possible to prove that \\(\\lang_R\\) only needs one operator (Sowinski 2016; Jaynes 2003): NAND (or XOR), and it is also equivalent to Propositional Calculus.5 In other words, Logic is the language that emerges from our desiderata, from rationalism. Logic is the language of mathematics.5 Proposition is synonym to sentence and Propositional Calculus is also known as Sentential Calculus.\nA point worth mentioning is that using Logic as an agent formal language means the implicit acceptance of the constraints above.\n\n\n\nFrom Empiricism to Probability Theory\nThe constraints that lead to Logic are very restrictive to use in the real-world; rational language has a comparatively small realm of discourse. Hume would say that it is only helpful for relations of ideas, talking in the abstract, and not for matters of facts, talking about reality.\nA realm of discourse to talk about reality needs at least the empiricist perspective where knowledge is justified belief, and that one should weigh her beliefs to the evidence. The quantity that specifies to what degree we believe a proposition is true is constrained by other beliefs, i.e., previous experience and evidence gathered.\n\nSceptical Agents\nIn the sceptical agent, the one derived from the empiricist epistemology (authors have called these agents epistemic agents (Caticha 2008), idealised epistemic agents (Sowinski 2016) or robots (Jaynes 2003)), beliefs are not independent of each other (Caticha 2008), they form an interconnected web that is the agent’s knowledge base. The update mechanism, its inference method, follows the principle of minimality, i.e. it tries to minimise the change in the knowledge base.\n\n\nDesiderata for a sceptical language\nAs we did for rational agents, let us state a set of desired characteristics for the language of science, \\(\\lang_S= <\\Sigma_S, \\Phi_S>\\) 6:6   also present this same idea of deriving probability theory from desiderata.\n\nKnowledge is a set of beliefs, quantifiable by real numbers and dependent on prior evidence (Sowinski 2016; Caticha 2008; Jaynes 2003): Let \\(S_i \\in \\Sigma_S\\) be sentences about the world. Given any two statements \\(S_1\\), \\(S_2\\), the agent must be able to say that \\(S_1\\) is more plausible than \\(S_2\\), or that \\(S_2\\) is more plausible than \\(S_1\\) or that \\(S_1\\) and \\(S_2\\) are equally plausible. Thus we can list statements in an increasing plausibility order. Real numbers can represent this transitive ordering.7\nLet \\(b\\) be a measure of degrees of belief in \\(S\\) given some previous knowledge \\(K\\): 8\n\n\\[\\begin{aligned}\n        &b: \\Sigma_S \\to \\Real\\\\\n        &b: S \\mapsto b(S|K)\n\\end{aligned}\\]\n\nHere we capture that plausibility (degrees of belief) is not a function of a sentence, but a relation between a sentence and a given assumed prior knowledge \\(K\\).\n“Common sense:”\nThe plausibility of compound sentences should be related by some logical function to the plausibility of the sentences that form them. We already showed that a minimal rational language has only one operator. Here, instead of using the NAND operator, for a matter of familiarity, let us use the almost minimal language with the operators NOT (\\(\\neg\\)) and AND (\\(\\land\\)). In this setting, we are saying there are such functions \\(f\\) and \\(g\\) that (Sowinski 2016):\n\n\\[\\begin{aligned}\n        &b(\\neg S|K) = f[b(S|K)] \\tag{NOT}\\\\\n        &b(S-1 \\land S_2 | K) = g[b(S_1|K), b(S_1|S_2), b(S_2|K), b(S_2|S_1)]\n\\end{aligned}\\]\n\n\nConsistency: The functions \\(f\\) and \\(g\\) must be consistent with the grammar \\(\\Phi\\) (production rules). Consistency guarantees that whatever path used to compute the plausibility of a statement in the context of the same knowledge web (the same set of constraints) must lead to the same degree of belief.\n\nBeliefs that depend on multiple propositions cannot depend on the order in which they are presented.\nNo proposition can be arbitrarily ignored.\nPropositions that are identical must be assigned the same degree of belief.\n\n\n7 We are implicitly assuming that the language we are building has infinite statements. A further discussion on this continuity assumption can be found in .8 Using \\((S|K)\\) in a function is a notation abuse that we accept to explain the idea better.Such desiderata have a name; it is known as Cox’s axioms, and one can derive the Sum Rule and the Product Rule (see 1.4) from them, therefore, also the Bayes’ Theorem (1.9), and reverse-engineer Kolmogorov’s Axioms of Probability Theory (that will be seen in [sec:kolmogorov_axioms], [fig:kolmogorov]) (Sowinski 2016; Jaynes 2003; Caticha 2008; Terenin and Draper 2015).\nIn other words, Probability Theory is the language that emerges from our desiderata, from empiricism. “Probability theory is the Logic of Science” (Jaynes 2003), and our measure \\(b\\) is usually called probability \\(P\\).\nAgain, here we explicit that by using Bayesian inference to build and communicate concepts of the world (models), we are assuming Cox’s axioms above.\n\n\n\nAssumptions and their consequences\nLet us take this opportunity to explore what some assumptions mean to human intelligence in particular. It is indisputable9 that humans are not rational, neither sceptical agents. The whole idea of imagining an epistemic agent is a consequence of addressing intelligence without human complexities.9 Unless you are an economist.\nHowever, are humans irrational because of biology or psychology? Are we irrational for lack of will, or could it be that Nature wires the human brain in a way that prevents us from following these axioms? Here we argue that biology has an important role. Researchers have found, for instance, that visual acuity can be permanently impaired if there is a sensory deficit during early post-natal development (wiesel:1982?). Futhermore, if the human brain is not exposed to some samples in its infancy, it will never achieve the accuracy level if it had experienced them, regardless of experiencing those examples later. In other words, human beliefs depend on the order in which pieces of evidence are presented, contradicting Cox’s axiom [axiom:order]."
  },
  {
    "objectID": "Chapters/prob.html#formalizing-probability-theory",
    "href": "Chapters/prob.html#formalizing-probability-theory",
    "title": "Probability Theory",
    "section": "Formalizing Probability Theory",
    "text": "Formalizing Probability Theory\nWe derived Cox’s axioms from a list of desired properties of the language for sceptical agents. We also know that it is possible to derive Kolmogorov’s Axioms (which will be defined soon in [sec:kolmogorov_axioms]) from those axioms. In the next sections, we will use the Kolmogorov Axioms to formalise Probability theory.\nSeveral concepts in the following sections are relations of ideas, not matters of fact. For example, the probability of an event E, P(E), can be computed by marginalisation (as we will show in 1.8), but as discussed before, there are no beliefs in a vacuum. In reality, there is only the probability of an event E given some background knowledge \\(K\\). This change of epistemological perspective is essential to be remembered now that we will expose the idealised development of Probability Theory."
  },
  {
    "objectID": "Chapters/prob.html#experiments-sample-spaces-and-events",
    "href": "Chapters/prob.html#experiments-sample-spaces-and-events",
    "title": "Probability Theory",
    "section": "Experiments, Sample Spaces and Events",
    "text": "Experiments, Sample Spaces and Events\nThe set of possible outcomes of an experiment is the sample space \\(\\Omega\\). Let us use the canonical experiment of rolling a dice. In this experiment, the sample space is: \\[\\begin{aligned}\n    \\Omega = \\left\\{\\epsdice{1},\\epsdice{2},\\epsdice{3},\\epsdice{4},\\epsdice{5},\\epsdice{6} \\right\\}\n\\end{aligned}\\] An outcome or realisation is a point \\(\\omega \\in \\Omega\\): \\[\\begin{aligned}\n    \\omega_3&=\\epsdice{3}\\\\\n    \\Omega &= \\left\\{\\omega_1=\\epsdice{1},\\cdots,\\omega_6=\\epsdice{6} \\right\\}.\n\\end{aligned}\\] An Event is something that can be said about the experiment, “The dice rolled to an odd number”. It is a true proposition. Nevertheless, easier than writing so much, we denote events with letters. Events are subsets of \\(\\Omega\\) (see [fig:event_A]). \\[\\begin{aligned}\n    A &= \\left\\{\\va_1=\\epsdice{1}, \\va_2=\\epsdice{3}, \\va_3=\\epsdice{5} \\right\\}\\\\\n    A &\\subset \\Omega\n\\end{aligned}\\] We say that \\(A_1, A_2, \\cdots\\) are mutually exclusive or disjoint events if \\(A_i \\cap A_j=\\emptyset, \\forall i\\neq j\\). For example, \\(A\\) is the event “the dice rolled to the value 5” and \\(B\\) is the event “the dice rolled to an even number”. In this case, \\(A\\) and \\(B\\) are disjoint (see [fig:disjoint_events]).\n\nEvents, disjoint events and partitions.\n\nA partition of \\(\\Omega\\) is a sequence of disjoint events (sets) \\(A_i\\) (see [fig:partition]), where: \\[\\begin{aligned}\n    A_1, A_2, \\cdots A_i \\text{ s.t. } (A_1 \\cup A_2 \\cup A_3 \\cdots = \\bigcup\\limits_{i=1}^{\\infty} A_i) = \\Omega\n\\end{aligned}\\]"
  },
  {
    "objectID": "Chapters/prob.html#sec:probability",
    "href": "Chapters/prob.html#sec:probability",
    "title": "Probability Theory",
    "section": "Kolmogorov’s definition of Probability",
    "text": "Kolmogorov’s definition of Probability\n\n\nA function \\(P: \\powerset(\\Omega) \\to \\sR\\) that maps any event \\(A\\) to a real number \\(P(A)\\) is called the probability measure or a probability distribution if it satisfies Kolmogorov’s axioms (Wasserman 2013):\n\n\\(P(A)\\geq 0, \\forall A\\)\n\\(P(\\Omega)=1\\)\nIf \\(A\\) and \\(B\\) are disjoint, i.e. \\(A \\ind B\\), $$\n\\[\\begin{aligned}\n            P(A \\lor B)= P(A)+P(B)\\label{eq:sum_rule}\\tag{Sum Rule}\n\n\\end{aligned}\\]\n$$\n\n\nVisually, we can represent the probability of an event \\(A\\), \\(P(A)\\), as the proportion of the sample space the event occupies. To differentiate events from their probabilities, we will shade the area of the event.\n\nKolmogorov’s Axioms and their direct consequences.\n\nDirectly from the Kolmogorov Axioms, one can derive (Jaynes 2003) other properties (see [fig:axiom1, fig:axiom2, fig:axiom3]): \\[\\begin{aligned}\nP(\\emptyset)&=0\\\\\nB \\subset A &\\implies P(B) \\leq P(A)\\\\\n0 &\\leq P(A) \\leq 1\\\\\nP(\\bar{A})&=1-P(A).\n\\end{aligned}\\]"
  },
  {
    "objectID": "Chapters/prob.html#joint-event",
    "href": "Chapters/prob.html#joint-event",
    "title": "Probability Theory",
    "section": "Joint event",
    "text": "Joint event\n\nA joint event (A, B) is the set of outcomes where: \\[(A, B) = {\\omega \\in \\Omega: (\\omega \\in A \\cap B) }\\] Therefore, \\[P(A, B) =P({\\omega \\in \\Omega: (\\omega \\in A \\cap B) })\\]\n\nWhen talking about events as propositions, it is straightforward to use logic notation \\(P(A \\land B)\\), but when we start to use random variables (1.10), we will adopt the shorthand notation \\(P(\\rvA, \\rvB)\\)."
  },
  {
    "objectID": "Chapters/prob.html#sec:independent_events",
    "href": "Chapters/prob.html#sec:independent_events",
    "title": "Probability Theory",
    "section": "Independent events",
    "text": "Independent events\n\n Events \\(A\\) and \\(B\\) are independent (\\(A \\ind B\\)) if: \\[\\begin{aligned}\nA\\neq \\emptyset, B\\neq \\emptyset \\implies P(A)>0, P(B)>0\\label{eq:P(A, B)>0}\\\\\nP(A, B) = P(A \\land B) = P(A) \\cdot P(B)\\label{eq:Product_Rule}\\\\\n\\nonumber \\tag{Product Rule}\n\\end{aligned}\\]\n\nDisjoint events cannot be independent, since (from [eq:P(A, B)>0]) \\(P(A) \\cdot P(B)> 0\\), but as disjoint events ([fig:disjoint_events]) \\(P(A \\land B)=P(\\emptyset)=0\\), leading to contradiction.\nIndependence can be assumed or derived by verifying: \\[\\begin{aligned}\nP(A \\land B)= P(A) \\cdot P(B).\\\\\n\\nonumber \\tag{Independent variables}\n\\end{aligned}\\]"
  },
  {
    "objectID": "Chapters/prob.html#conditional-probability",
    "href": "Chapters/prob.html#conditional-probability",
    "title": "Probability Theory",
    "section": "Conditional probability",
    "text": "Conditional probability\nAs we have explained before (1.1.3.0.1), the plausibility of an outcome or a set of outcomes depends on a web of interconnected prior beliefs. So, what exists are probabilities conditional to a given prior assumption.\n\nIf \\(P(B)>0\\) then the conditional probability of A given B is: \\[\\begin{aligned}\n\\label{eq:conditional_probability}\nP(A|B) \\eqdef \\frac{P(A,B)}{P(B)}\n\\end{aligned}\\] \\[\\begin{aligned}\n\\label{eq:joint_probability}\nP(A, B) \\eqdef P(A|B)\\cdot P(B)\n\\end{aligned}\\]\n\nExcept if \\(P(A) \\equiv P(B)\\), \\(P(A|B) \\neq P(B|A)\\). Also, \\(P(A|B)=P(A) \\iff A \\ind B\\).1010 Venn diagrams are not helpful to see that the events are independent, as it all depends on the areas of intersection and the sizes of A and B, which are tricky to estimate without computational help."
  },
  {
    "objectID": "Chapters/prob.html#marginalisation",
    "href": "Chapters/prob.html#marginalisation",
    "title": "Probability Theory",
    "section": "Marginal probability",
    "text": "Marginal probability\n\nLet \\(A_1, \\cdots, A_k\\) be a partition of \\(\\Omega\\). Then, for any event B, \\[\\begin{aligned}\nP(B)=\\sum_{i=1}^k P(B|A_i)\\cdot P(A_i)\\label{eq:law_of_total_probabilities}\n\\end{aligned}\\]\n\n\nProof. Proof. 11 Define \\(C_i = (B,A_i)\\). Let \\(C_1, \\cdots C_k\\) be disjoint and \\(B = \\bigcup\\limits_{i=1}^k C_i\\).\nTherefore:11 Remember: \\((B, A) \\equiv (B \\cap A)\\).\n$$\n\\[\\begin{aligned}\n        P(B) &\\triangleq P(\\bigcup\\limits_{i=1}^k C_i)\n        \\overset{\\text{\\ref{eq:sum_rule}}}{=} \\sum_i P(C_i)\\\\\n        &\\triangleq \\sum_i P(B,A_i)\n        \\overset{\\text{\\ref{eq:conditional_probability}}}{=} \\sum_{i=1}^k P(B|A_i)\\cdot P(A_i) \\tag{Law of Total Probability}\n    \n\\end{aligned}\\]\n$$ ◻"
  },
  {
    "objectID": "Chapters/prob.html#sec:bayes_theorem",
    "href": "Chapters/prob.html#sec:bayes_theorem",
    "title": "Probability Theory",
    "section": "Bayes’ theorem",
    "text": "Bayes’ theorem\n\nLet \\(A_1, \\cdots, A_k\\) be a partition of \\(\\Omega\\) s.t. \\(P(A_i)>0, \\forall i\\) then, \\(\\forall i=1, \\cdots, k\\): $$\n\\[\\begin{aligned}\n        P(A_i|B)= \\frac{P(B|A_i)\\cdot P(A_i)}{\\sum_i P(B|A_i)\\cdot P(A_i)}\n    \n\\end{aligned}\\]\n$$\n\n\nProof. Proof. From equations [eq:conditional_probability], [eq:joint_probability] and [eq:law_of_total_probabilities]: $$\n\\[\\begin{aligned}\n        P(A_i|B)&\\overset{\\text{\\ref{eq:conditional_probability}}}{=}\\frac{P(A_i,B)}{P(B)} \\overset{\\text{\\ref{eq:joint_probability}}}{=} \\frac{P(B|A_i) \\cdot P(A_i)}{P(B)}  \\\\\n        &\\overset{\\text{\\ref{eq:law_of_total_probabilities}}}{=}\\frac{P(B|A_i)\\cdot P(A_i)}{\\sum_{i=1}^k P(B|A_i)\\cdot P(A_i)}\n    \n\\end{aligned}\\]\n$$ ◻\n\nWe call \\(P(A_i)\\) the prior of A, and \\(P(A_i|B)\\) the posterior probability of A."
  },
  {
    "objectID": "Chapters/prob.html#sec:random_variables",
    "href": "Chapters/prob.html#sec:random_variables",
    "title": "Probability Theory",
    "section": "Random variables",
    "text": "Random variables\n\nA random variable is a mapping \\(\\rvX:\\Omega \\to \\Real\\) that assigns a real number \\(\\rvX(\\omega)\\) to each outcome \\(\\omega\\), \\(\\omega \\mapsto \\rvX(\\omega)\\).\n\nGiven a random variable \\(\\rvX\\), the probability of an outcome \\(\\rx\\) can be expressed as: \\[\\begin{aligned}\n    P(\\rvX=\\rx) = P(\\rvX^{-1}(\\rx)) = P(\\{\\omega \\in \\Omega: \\rvX(\\omega)=\\rx\\})\\label{eq:P(X=x)}\n\\end{aligned}\\]\nSeveral works on Probability Theory choose to start by defining random variables, rarely mentioning sample spaces, events or the connection with logical propositions.\nThis usual approach is, nevertheless, confusing. Beyond the fact that random variables are not variables, but functions, nor random, they model uncertain events; it is hard to grasp what random variables are without understanding their reasons for being.\nThe difference between a random variable \\(\\rvX\\) and its “realisation” is the difference between a distribution and a sample from that distribution. In particular, a random variable \\(\\rvX\\) is “formalised” in terms of a function from the sample space to some result space, typically \\(\\Real\\). The realisation of a random variable is “what you get” when an experiment is run, and you apply \\(\\rvX\\) to events that happened.\n\nNotation abuse\nIf a random variable is a function, how can we write \\(P(\\rvX=4)\\) or \\(P(\\rvX > 7)\\)? Such confusion is due to some notation abuse that became standard in works on probability theory. It is not easy to grasp it initially, but the explanation was already stated at [eq:P(X=x)]. \\(P(\\rvX=\\rx)\\) is a shorthand for \\(P(\\rvX^{-1}(\\rx))\\).\nTechnically, a random variable is a function. In practice, it is just a mathematical tool to help us associate propositions with numbers. It is called a random variable because the notation abuse treats the function as a variable.\nTo help clear up such confusion, let us recap a little the notation we have established before:\nIn the canonical experiment of rolling a dice, instead of writing the proposition “The dice will roll to number 4.” plausibility is \\(\\frac{1}{6}\\), it is easier to assign a letter to the proposition, or as we called the event. Let us use event \\(D\\) to represent the proposition. Then, we can use \\(P(D)=\\frac{1}{6}\\). Now, we are going one step further; instead of using the event \\(D\\) we use the random variable \\(\\rvD\\), in italic, and say \\(P(\\rvD=4)=\\frac{1}{6}\\).\nNotice the difference between a random variable and an event:12 \\(\\rvD\\) could assume any value (even \\(\\rvD=7\\), which is outside of our sample space). Would it not be easier then to use an index to the event letters, \\(D_4\\) to value 4, and \\(D_1\\) to value 1, etc.? Not really.12 An event can be seen as a special kind of random variable. , a random variable \\(\\rvD\\) is the truth function (also known as the indicator function) over an event \\(D\\): \\[\\begin{aligned}\n    \\rvD=\\truth_D\n\\end{aligned}\\] That is the reason one can say that “random variables define events.”\nBesides providing this shorter notation, the mapping of the random variable allows us to manipulate events as numbers: for example, we can chart probability distributions using random variables, which we cannot cope with events."
  },
  {
    "objectID": "Chapters/prob.html#probability-distributions",
    "href": "Chapters/prob.html#probability-distributions",
    "title": "Probability Theory",
    "section": "Probability Distributions",
    "text": "Probability Distributions\n\nA probability distribution of a discrete random variable \\(\\rvX\\) or probability mass function (pmf) is a function \\(p: \\Omega \\to [0,1]\\) that provides the probabilities of occurrence of different possible outcomes in an experiment (sample space):\n$$\n\\[\\begin{aligned}\n        p(\\rx) = P(\\rvX = \\rx), \\tag{pmf}\n    \n\\end{aligned}\\]\n$$\n\nIf \\(\\rvX\\) is continuous, \\(P(\\rvX=\\rx)\\to 0\\), therefore we need to use intervals in this case.\n\nA probability distribution of a countinous random variable \\(\\rvX\\) in an interval \\(A\\), or probability density function (pdf) is a function \\(p(\\rx)\\) that measures the probability of randomly selecting a value within the interval \\(A=[a, b]\\), as the area under its curve for the interval A:\n\nP(A) &= P[a b] = _a^b p()   d,\n&\n\np() , x\n_^ p()   d= 1\n\n\n\nNow that we explained what distributions are,13 here we highlight some useful distributions:13 In this dissertation, we will use \\(P(\\rvX)\\) to express the probability of a random variable, and \\(p(\\rx)\\) to represent a pmf or pdf of the random variable outcomes.\n\nStatistical model\nA statistical model is a function \\(p_{\\theta}(\\rx) \\equiv p(\\rx | \\theta)\\) representing the relationship between a parameter14 \\(\\theta\\) and potential outcomes \\(\\rx\\) of a random variable \\(\\rvX\\). In practice, we usually define a statistical model of a stochastic process for which we do not know the real distribution. Therefore, the parameter \\(\\theta\\) has to be inferred from the observed data.14 In this dissertation we are interested in vector-valued \\(\\theta\\).\n\n\nUniform distribution\n\\(\\nonumber \\\\\\rvX \\sim \\text{Uniform}(a,b)\\), if:\n\\[\\begin{aligned}\n    p(\\rx)=\n    \\begin{cases}\n        \\frac{1}{b-a} & x \\in [a,b]\\\\\n        0 & \\rx \\notin [a,b]\n    \\end{cases}\n\\end{aligned}\\]\n\n\nNormal distribution\n\\(\\nonumber \\\\\\rvX \\sim \\mathcal{N}(\\mu, \\sigma^2)\\), if: \\[\\begin{aligned}\n    p(\\rx)=\\frac{1}{\\sigma \\sqrt{2\\pi}}\\exp{\\Biggl{\\{}{-\\frac{1}{2\\sigma^2}{(x-\\mu)}^2}\\Biggr{\\}}}, \\\\~x \\in \\Real \\\\\n\\end{aligned}\\] where \\(\\mu \\in \\Real\\) (mean) and \\(\\sigma > 0\\) (standard deviation). We say that \\(\\rvX\\) has a standard Normal distribution if \\(\\mu = 0\\), \\(\\sigma =1\\).\n\n\nExponential distribution\n\\(\\rvX \\sim \\text{Exp}(\\lambda)\\), if: \\[\\begin{aligned}\n    p(\\rx;\\lambda) =\n    \\begin{cases}\n        \\lambda e^{-\\lambda \\rx} & \\rx \\ge 0, \\\\\n        0 & \\rx < 0.\n    \\end{cases}\n\\end{aligned}\\] where \\(\\lambda > 0\\) is the rate parameter of the distribution."
  },
  {
    "objectID": "Chapters/prob.html#joint-distributions",
    "href": "Chapters/prob.html#joint-distributions",
    "title": "Probability Theory",
    "section": "Joint Distributions",
    "text": "Joint Distributions\n\nGiven a pair of discrete random variables \\(\\rvX\\) and \\(\\rvY\\), we define the joint mass function by \\(p(\\rx, \\ry)=P(\\rvX=\\rx,\\rvY=\\ry)\\).\n\n\nGiven a pair of continuous random variables \\(\\rvX\\) and \\(\\rvY\\), we define the joint density function by \\(p(\\rx, \\ry)\\), where:\n\n\\(p(\\rx, \\ry) \\geq 0\\)\n\\(\\iint_{-\\infty}^{\\infty} p(\\rx,\\ry) \\, d\\rx d\\ry =1\\)\n\\(\\forall A \\subset \\Real \\times \\Real, P((\\rvX,\\rvY)\\in A)=\\iint_{A}p(\\rx,\\ry)\\, d\\rx d\\ry\\)."
  },
  {
    "objectID": "Chapters/prob.html#expectancy-variance-and-covariance",
    "href": "Chapters/prob.html#expectancy-variance-and-covariance",
    "title": "Probability Theory",
    "section": "Expectancy, Variance and Covariance",
    "text": "Expectancy, Variance and Covariance\n\nThe expected value or mean of \\(\\rvX\\) is: $$\n\\[\\begin{aligned}\n        \\E (\\rvX)=\\langle \\rvX \\rangle = \\SumInt_x \\rx~p(\\rx)~dx = \\mu = \\mu_X\n    \n\\end{aligned}\\]\n$$\n\n\nLet \\(\\rvX_1, \\cdots, \\rvX_n\\) be random variables and \\(a_1, \\cdots, a_n\\) be constants, then from the Sum Rule: $$\n\\[\\begin{aligned}\n        \\E \\biggl(\\sum_i a_i\\rvX_i\\biggr)=\\sum_i a_i(\\E (\\rvX_i))\n    \n\\end{aligned}\\]\n$$\n\n\nLet \\(\\rvX_1, \\cdots, \\rvX_n\\) be independent random variables, then from the Product Rule: $$\n\\[\\begin{aligned}\n        \\E (\\prod_i \\rvX_i)=\\prod_i \\E (\\rvX_i)\n    \n\\end{aligned}\\]\n$$\n\n\nLet \\(\\rvX\\) be a random variable with mean \\(\\mu\\). The variance of \\(\\rvX\\) is defined by: $$\n\\[\\begin{aligned}\n        \\sigma^2 = \\sigma_{\\rvX}^2 =\\E {(\\rvX - \\mu)}^2\n    \n\\end{aligned}\\]\n$$ assumming this expectation exists. The standard deviation is \\(\\sigma\\).\n\n\nLet \\({\\rvX}\\) and \\({\\rvY}\\) be random variables with means \\(\\mu_{\\rvX}\\) and \\(\\mu_{\\rvY}\\), and with standard deviations \\(\\sigma_{\\rvX}\\) and \\(\\sigma_{\\rvY}\\). The covariance between \\({\\rvX}\\) and \\({\\rvY}\\) is defined as (Wasserman 2013, 74): $$\n\\[\\begin{aligned}\n        \\operatorname{Cov}({\\rvX},{\\rvY}) = \\E (({\\rvX} - \\mu_{\\rvX})({\\rvY} - \\mu_{\\rvY}))\n    \n\\end{aligned}\\]\n\\[ and the correlation as: \\]\n\\[\\begin{aligned}\n        \\rho = \\rho_{{\\rvX},{\\rvY}} = \\rho({\\rvX},{\\rvY}) = \\frac{\\operatorname{Cov}({\\rvX},{\\rvY})}{\\sigma_{\\rvX} \\sigma_{\\rvY}}\n    \n\\end{aligned}\\]\n$$\n\n\nThe covariance satisfies: $$\n\\[\\begin{aligned}\n        \\operatorname{Cov}({\\rvX},{\\rvY})=\\E ({\\rvX}{\\rvY})- \\E({\\rvX}) \\E({\\rvY}).\n    \n\\end{aligned}\\]\n$$"
  },
  {
    "objectID": "Chapters/prob.html#independent-sampling",
    "href": "Chapters/prob.html#independent-sampling",
    "title": "Probability Theory",
    "section": "Independent Sampling",
    "text": "Independent Sampling\nA sample is a set of examples15 drawn from a distribution. One common assumption in Machine Learning Theory is that examples are identically and independently distributed — i.i.d. This means that the probability of obtaining a first training example. \\((\\rx_1, \\ry_1)\\) does not affect which \\((\\rx_2, \\ry_2)\\) will be drawn in the following observation.15 In this dissertation, an element of a sampling is called an example.\nThe i.i.d. assumption is useful wherever a census of the population of interest, knowing all possible values, is unfeasible. In this usual case, data analysis is carried out using a sample to represent the population. When the sample is i.i.d., each example in the population has the same chance of being observed ([fig:sampling] — left).\nIf there is a constraint on which examples of the population are sampled, we say that the sample is biased ([fig:sampling] — right)."
  },
  {
    "objectID": "Chapters/prob.html#concluding-remarks",
    "href": "Chapters/prob.html#concluding-remarks",
    "title": "Probability Theory",
    "section": "Concluding Remarks",
    "text": "Concluding Remarks\nThis chapter derived Logic from the definition of knowledge as absolute truth and Probability Theory from knowledge as justified beliefs ([sec:from_rationalism, sec:from_empiricism]). To remind that our definition of knowledge is the basis for the Bayesian perspective of probability and that inference methods are languages, we can say (and prefer) that we derived Bayesian inference as the language of science. We proved what we claimed in the previous chapter ([ch:artificial_intelligence]).\nWe needed to define formal languages (1.1.1) and assume desiderata for the languages we wanted to build formally ([sec:desiderata_language,sec:desiderata_language_sceptical]). We called rational agents the epistemic agents that use Logic as its inference method, and sceptical agents use Bayesian inference.\nWe found out that the desiderata for the sceptical language are equivalent to Cox’s axioms ([cox]). From Cox’s axioms, it is possible to derive Kolmogorov’s axioms of Probability Theory. Which made us conclude that Bayesian inference is the language of science.1616 Our definition of knowledge hinted at a Bayesian perspective of knowledge.\nFrom the derivation, we did a basic Statistics review (influenced by (Wasserman 2013)). Many essential topics were left out from this short review chapter, where the focus was to present the concepts that we will use later on in this dissertation.\n\nAssumptions\n\nA definition of intelligence ([def:intelligence]);\nA epistemic choice on the definition of Knowledge ([sec:rationalism, sec:empiricism]);\nA definition of formal language;\nCommon assumptions of the epistemic agent language:\n\nconsistency (1.1.3.0.2, [consistency] and 1.1.2.0.2, [rational_consistency]);\nminimality (1.1.2.0.2, [rational_minimality]).\n\nAssumption of the rational agent language:\n\nknowledge is absolute, a set of true or false sentences (1.1.2.0.2, [absolute_truth]);\nthe language must be unambiguous (1.1.2.0.2, [unambiguous]).\n\nAssumption of the sceptical agent language:\n\nKnowledge is a set of beliefs, quantifiable by real numbers and dependent on prior evidence (1.1.3.0.2, [beliefs]);\nCommon sense: The plausibility of compound sentences should be related by some logical function to the plausibility of the sentences that form them (1.1.3.0.2, [common_sense]).\n\n\nAs we have settled that our focus is Deep Learning, which relates to the sceptical agent, we will abstain from keeping the rational language assumptions in our analysis and assume an epistemic agent is sceptical.\n\n\n\n\nCaticha, Ariel. 2008. “Lectures on Probability, Entropy, and Statistical Physics.” https://arxiv.org/abs/0808.0012.\n\n\nJaynes, E. T. 2003. Probability Theory: The Logic of Science. Cambridge University Press.\n\n\nSowinski, Damian Radoslaw. 2016. “Complexity and Stability for Epistemic Agents: The Foundations and Phenomenology of Configurational Entropy.” PhD thesis.\n\n\nTerenin, Alexander, and David Draper. 2015. “Cox’s Theorem and the Jaynesian Interpretation of Probability.” https://arxiv.org/abs/1507.06597.\n\n\nWasserman, Larry. 2013. All of Statistics: A Concise Course in Statistical Inference. Springer Science & Business Media."
  },
  {
    "objectID": "Back/quarto-questions.html#list-of-questions",
    "href": "Back/quarto-questions.html#list-of-questions",
    "title": "Questions",
    "section": "List of questions",
    "text": "List of questions\n\nWhy Welcome from /index.qmd is appearing in pdf?\nPart /background.qmd is not appearing in TOC (pdf). Is this a Tufte-book.cls problem?\npdf figures in website are being framed. Why?\n\ncan I remove file extension and site get svg or png and pdf get .pdf?\n\nHeader configuration is set to include short-tile instead of title. Couldn’t insert just a space, though. Why blank space without mathmode didn’t work?\nHow to include math macros in the html?\n\nWhat is the page full of definitions that appear while routing in the website? math_definitions.tex?\nToo much info in the margin citation. Create custom bib style?\nHow to show title-block for each chapter in website?"
  },
  {
    "objectID": "Back/quarto-questions.html#known-issues",
    "href": "Back/quarto-questions.html#known-issues",
    "title": "Questions",
    "section": "Known-issues",
    "text": "Known-issues\n\nTufte-book can’t handle label inside caption.\nTufte-book.cls breaks when processing the line bellow: markdown  ![A way of flying](/Images/goya.jpg){.column-body #fig-goya} which becomes:\n\\begin{figure}\n\n{\\centering \\includegraphics{Images/goya.jpg}\n\n}\n\n\\caption{\\label{fig-goya}A way of flying}\n\n\\end{figure}\nCan’t render svg image in pdf and can’t render pdf image in html.\n\ncurrent solution is quite ugly:\n\n::: {.content-hidden unless-format=\"pdf\" }\n\n![IBT \"genealogy\" tree.](/Images/dissertation-map.pdf){.column-margin width=90%}\n\n:::\n\n::: {.content-hidden unless-format=\"html\"}\n\n![IBT \"genealogy\" tree.](/Images/dissertation-map.svg){.column-margin width=90%}\n\n:::\nTufte-class works only until subsection -> ### subsection; #### subsubsection-> returns error\nsidecite is duplicating citations in the same margin. Solved this same problem in my dissertation and in the kaobook class. Only problem is that Tufte-book class is a little too cryptic for me."
  },
  {
    "objectID": "Back/todo.html",
    "href": "Back/todo.html",
    "title": "To-dos",
    "section": "",
    "text": "create documentation showcasing project type\nshow jupyter/matplotlib examples:\nhttps://matplotlib.org/2.0.2/examples/pylab_examples/polar_legend.html\nhttps://www.ajnisbet.com/blog/tufte-in-matplotlib\nhttps://www.andrewheiss.com/blog/2017/08/10/exploring-minards-1812-plot-with-ggplot2/\nhttps://deepnote.com/workspace/fred-guth-c6b0391e-7e85-43ad-ba15-bd505c864b75/project/tufte-5ba3f476-1ce2-4d99-b9be-d045148bbc77/notebook/tufte-in-python-58e3589d00c34ac699f715ad69c4bfd3\nhttps://matplotlib.org/2.0.2/examples/pylab_examples/ellipse_collection.html\nhttps://matplotlib.org/2.0.2/examples/lines_bars_and_markers/fill_demo_features.html\nhttps://matplotlib.org/2.0.2/examples/shapes_and_collections/scatter_demo.html\n\n\nmove my dissertation to an examples folder (maybe another repo? .quartoignore my dissertation?)\nfix known bugs"
  },
  {
    "objectID": "Chapters/prob.html",
    "href": "Chapters/prob.html",
    "title": "The emergence of\\\nan Information Bottleneck Theory\\\nof Deep Learning\\\n",
    "section": "",
    "text": "Artificial Intelligence\n                \n  \n  \n      \n        Questions"
  },
  {
    "objectID": "Chapters/prob.html#sec:language_probability",
    "href": "Chapters/prob.html#sec:language_probability",
    "title": "Probability Theory",
    "section": "From Language to Probability",
    "text": "From Language to Probability\n\nFormal Languages\nWe, as intelligent agents, do not know how Nature is; we only know how we perceive it. Our ideas are mental pictures of how we imagine Nature. Like in the story of the blind men and the elephant ([blind_men]), how do we know that our model is the same as someone else’s? Communicating. We need to communicate with each other to check if our mental picture of Nature, our model, is consistent with the experience of others.11 We can take this idea further and think that at any moment, we need to communicate with our past selves to check if new evidence is consistent with our prior model.\nWe use language to describe Nature. However, natural languages, like English, German, Portuguese, are ambiguous, and we need contextual clues and other information to more clearly communicate meaning. To avoid this, an intelligent agent uses formal language.\nA formal language is a mathematical tool created for precise communication about a specific subject. For example, arithmetic is a language for calculations. Chemists have a language that represents the chemical structures of molecules. Programming languages are formal languages that express computations. In a nutshell, a formal language is a set of words (strings) whose letters (symbols) are taken from an alphabet and are well-formed according to a specific set of rules, grammar. Let \\(\\lang= <\\Sigma, \\Phi>\\) be a formal language where: \\[\\begin{aligned}\n    \\Sigma &= \\{S_1, S_2, \\cdots, S_n\\} \\text{ is an alphabet,}\\\\\n    \\Phi &= {\\Phi_1 \\cup \\Phi_2 \\cup \\cdots \\cup \\Phi_k} \\text{ is a set of operations, the grammar,}\n\\end{aligned}\\] and: \\[\\begin{aligned}\n    \\Phi_1 &\\text{ is the set of unary operations}, \\nonumber\\\\\n    \\Phi_2 &\\text{ is the set of binary operations}, \\nonumber\\\\\n    \\cdots &\\nonumber \\\\\n    \\Phi_k &\\text{ is the set of k-ary operations.}\\nonumber\n\\end{aligned}\\] A formal language allows a quantitative description of a state of knowledge and defines how this state can be updated on new evidence.22 An inference method defines the rules for updating knowledge.\nWith this definition, we can also think that a formal language is what Sowinski (2016) calls a realm of discourse, all the valid formed strings3 that one can derive; everything one can say about Nature.3 Strings, words, sentences, propositions, formulae are names used interchangeably through the literature.\nInterestingly, formal languages allow us to manipulate representations of the environment without dealing with their semantics. They are the basis of “Turing’s strange inversion”, (see [turing_strange_inversion]) by doing allowed operations on strings, computers can compute at a superhuman speed and accuracy without ever comprehending what they are doing.\n\n\nFrom Rationalism to Propositional Calculus\n\nRational Agents\ncan form representations of a complex world, use deduction as the inference process to derive updated representations, and use these new representations to decide what to do. In other words, rational agents are the consequence of the epistemological view of rationalism.\nWhen a rational agent establishes a particular statement’s truth value, all statements formed in her knowledge base from that statement instantly feel that update. Therefore, a rational agent cannot hold contradictions.\n\n\nDesiderata for a rational language\nWe want to build a language for rational agents with the following desired characteristics:\n\nknowledge is absolute; a sentence4 can be either true or false;\nunambiguous, a constructed sentence can only have one meaning;\nconsistent; a language without paradoxes, whatever path chosen to derive a sentence truth value will lead to the same assignment;\nminimal; uses the most reduced set of symbols possible.\n\n4 A sentence can be either a single symbol or a string formed with several symbols according to the grammar.Let \\(\\lang_R= <\\Sigma_R, \\Phi_R>\\) be the formal language built from these constraints, where sentences are either axiom symbols or compounded sentences formed using special symbols called operators, each operator denoting one operation, \\(\\phi \\in \\Phi_R\\).\nIt is possible to prove that \\(\\lang_R\\) only needs one operator (Sowinski 2016; Jaynes 2003): NAND (or XOR), and it is also equivalent to Propositional Calculus.5 In other words, Logic is the language that emerges from our desiderata, from rationalism. Logic is the language of mathematics.5 Proposition is synonym to sentence and Propositional Calculus is also known as Sentential Calculus.\nA point worth mentioning is that using Logic as an agent formal language means the implicit acceptance of the constraints above.\n\n\n\nFrom Empiricism to Probability Theory\nThe constraints that lead to Logic are very restrictive to use in the real-world; rational language has a comparatively small realm of discourse. Hume would say that it is only helpful for relations of ideas, talking in the abstract, and not for matters of facts, talking about reality.\nA realm of discourse to talk about reality needs at least the empiricist perspective where knowledge is justified belief, and that one should weigh her beliefs to the evidence. The quantity that specifies to what degree we believe a proposition is true is constrained by other beliefs, i.e., previous experience and evidence gathered.\n\nSceptical Agents\nIn the sceptical agent, the one derived from the empiricist epistemology (authors have called these agents epistemic agents (Caticha 2008), idealised epistemic agents (Sowinski 2016) or robots (Jaynes 2003)), beliefs are not independent of each other (Caticha 2008), they form an interconnected web that is the agent’s knowledge base. The update mechanism, its inference method, follows the principle of minimality, i.e. it tries to minimise the change in the knowledge base.\n\n\nDesiderata for a sceptical language\nAs we did for rational agents, let us state a set of desired characteristics for the language of science, \\(\\lang_S= <\\Sigma_S, \\Phi_S>\\) 6:6   also present this same idea of deriving probability theory from desiderata.\n\nKnowledge is a set of beliefs, quantifiable by real numbers and dependent on prior evidence (Sowinski 2016; Caticha 2008; Jaynes 2003): Let \\(S_i \\in \\Sigma_S\\) be sentences about the world. Given any two statements \\(S_1\\), \\(S_2\\), the agent must be able to say that \\(S_1\\) is more plausible than \\(S_2\\), or that \\(S_2\\) is more plausible than \\(S_1\\) or that \\(S_1\\) and \\(S_2\\) are equally plausible. Thus we can list statements in an increasing plausibility order. Real numbers can represent this transitive ordering.7\nLet \\(b\\) be a measure of degrees of belief in \\(S\\) given some previous knowledge \\(K\\): 8\n\n\\[\\begin{aligned}\n        &b: \\Sigma_S \\to \\Real\\\\\n        &b: S \\mapsto b(S|K)\n\\end{aligned}\\]\n\nHere we capture that plausibility (degrees of belief) is not a function of a sentence, but a relation between a sentence and a given assumed prior knowledge \\(K\\).\n“Common sense:”\nThe plausibility of compound sentences should be related by some logical function to the plausibility of the sentences that form them. We already showed that a minimal rational language has only one operator. Here, instead of using the NAND operator, for a matter of familiarity, let us use the almost minimal language with the operators NOT (\\(\\neg\\)) and AND (\\(\\land\\)). In this setting, we are saying there are such functions \\(f\\) and \\(g\\) that (Sowinski 2016):\n\n\\[\\begin{aligned}\n        &b(\\neg S|K) = f[b(S|K)] \\tag{NOT}\\\\\n        &b(S-1 \\land S_2 | K) = g[b(S_1|K), b(S_1|S_2), b(S_2|K), b(S_2|S_1)]\n\\end{aligned}\\]\n\n\nConsistency: The functions \\(f\\) and \\(g\\) must be consistent with the grammar \\(\\Phi\\) (production rules). Consistency guarantees that whatever path used to compute the plausibility of a statement in the context of the same knowledge web (the same set of constraints) must lead to the same degree of belief.\n\nBeliefs that depend on multiple propositions cannot depend on the order in which they are presented.\nNo proposition can be arbitrarily ignored.\nPropositions that are identical must be assigned the same degree of belief.\n\n\n7 We are implicitly assuming that the language we are building has infinite statements. A further discussion on this continuity assumption can be found in .8 Using \\((S|K)\\) in a function is a notation abuse that we accept to explain the idea better.Such desiderata have a name; it is known as Cox’s axioms, and one can derive the Sum Rule and the Product Rule (see 1.4) from them, therefore, also the Bayes’ Theorem (1.9), and reverse-engineer Kolmogorov’s Axioms of Probability Theory (that will be seen in [sec:kolmogorov_axioms], [fig:kolmogorov]) (Sowinski 2016; Jaynes 2003; Caticha 2008; Terenin and Draper 2015).\nIn other words, Probability Theory is the language that emerges from our desiderata, from empiricism. “Probability theory is the Logic of Science” (Jaynes 2003), and our measure \\(b\\) is usually called probability \\(P\\).\nAgain, here we explicit that by using Bayesian inference to build and communicate concepts of the world (models), we are assuming Cox’s axioms above.\n\n\n\nAssumptions and their consequences\nLet us take this opportunity to explore what some assumptions mean to human intelligence in particular. It is indisputable9 that humans are not rational, neither sceptical agents. The whole idea of imagining an epistemic agent is a consequence of addressing intelligence without human complexities.9 Unless you are an economist.\nHowever, are humans irrational because of biology or psychology? Are we irrational for lack of will, or could it be that Nature wires the human brain in a way that prevents us from following these axioms? Here we argue that biology has an important role. Researchers have found, for instance, that visual acuity can be permanently impaired if there is a sensory deficit during early post-natal development (wiesel:1982?). Futhermore, if the human brain is not exposed to some samples in its infancy, it will never achieve the accuracy level if it had experienced them, regardless of experiencing those examples later. In other words, human beliefs depend on the order in which pieces of evidence are presented, contradicting Cox’s axiom [axiom:order]."
  },
  {
    "objectID": "Chapters/prob.html#formalizing-probability-theory",
    "href": "Chapters/prob.html#formalizing-probability-theory",
    "title": "Probability Theory",
    "section": "Formalizing Probability Theory",
    "text": "Formalizing Probability Theory\nWe derived Cox’s axioms from a list of desired properties of the language for sceptical agents. We also know that it is possible to derive Kolmogorov’s Axioms (which will be defined soon in [sec:kolmogorov_axioms]) from those axioms. In the next sections, we will use the Kolmogorov Axioms to formalise Probability theory.\nSeveral concepts in the following sections are relations of ideas, not matters of fact. For example, the probability of an event E, P(E), can be computed by marginalisation (as we will show in 1.8), but as discussed before, there are no beliefs in a vacuum. In reality, there is only the probability of an event E given some background knowledge \\(K\\). This change of epistemological perspective is essential to be remembered now that we will expose the idealised development of Probability Theory."
  },
  {
    "objectID": "Chapters/prob.html#experiments-sample-spaces-and-events",
    "href": "Chapters/prob.html#experiments-sample-spaces-and-events",
    "title": "Probability Theory",
    "section": "Experiments, Sample Spaces and Events",
    "text": "Experiments, Sample Spaces and Events\nThe set of possible outcomes of an experiment is the sample space \\(\\Omega\\). Let us use the canonical experiment of rolling a dice. In this experiment, the sample space is: \\[\\begin{aligned}\n    \\Omega = \\left\\{\\epsdice{1},\\epsdice{2},\\epsdice{3},\\epsdice{4},\\epsdice{5},\\epsdice{6} \\right\\}\n\\end{aligned}\\] An outcome or realisation is a point \\(\\omega \\in \\Omega\\): \\[\\begin{aligned}\n    \\omega_3&=\\epsdice{3}\\\\\n    \\Omega &= \\left\\{\\omega_1=\\epsdice{1},\\cdots,\\omega_6=\\epsdice{6} \\right\\}.\n\\end{aligned}\\] An Event is something that can be said about the experiment, “The dice rolled to an odd number”. It is a true proposition. Nevertheless, easier than writing so much, we denote events with letters. Events are subsets of \\(\\Omega\\) (see [fig:event_A]). \\[\\begin{aligned}\n    A &= \\left\\{\\va_1=\\epsdice{1}, \\va_2=\\epsdice{3}, \\va_3=\\epsdice{5} \\right\\}\\\\\n    A &\\subset \\Omega\n\\end{aligned}\\] We say that \\(A_1, A_2, \\cdots\\) are mutually exclusive or disjoint events if \\(A_i \\cap A_j=\\emptyset, \\forall i\\neq j\\). For example, \\(A\\) is the event “the dice rolled to the value 5” and \\(B\\) is the event “the dice rolled to an even number”. In this case, \\(A\\) and \\(B\\) are disjoint (see [fig:disjoint_events]).\n\nEvents, disjoint events and partitions.\n\nA partition of \\(\\Omega\\) is a sequence of disjoint events (sets) \\(A_i\\) (see [fig:partition]), where: \\[\\begin{aligned}\n    A_1, A_2, \\cdots A_i \\text{ s.t. } (A_1 \\cup A_2 \\cup A_3 \\cdots = \\bigcup\\limits_{i=1}^{\\infty} A_i) = \\Omega\n\\end{aligned}\\]"
  },
  {
    "objectID": "Chapters/prob.html#sec:probability",
    "href": "Chapters/prob.html#sec:probability",
    "title": "Probability Theory",
    "section": "Kolmogorov’s definition of Probability",
    "text": "Kolmogorov’s definition of Probability\n\n\nA function \\(P: \\powerset(\\Omega) \\to \\sR\\) that maps any event \\(A\\) to a real number \\(P(A)\\) is called the probability measure or a probability distribution if it satisfies Kolmogorov’s axioms (Wasserman 2013):\n\n\\(P(A)\\geq 0, \\forall A\\)\n\\(P(\\Omega)=1\\)\nIf \\(A\\) and \\(B\\) are disjoint, i.e. \\(A \\ind B\\), $$\n\\[\\begin{aligned}\n            P(A \\lor B)= P(A)+P(B)\\label{eq:sum_rule}\\tag{Sum Rule}\n\n\\end{aligned}\\]\n$$\n\n\nVisually, we can represent the probability of an event \\(A\\), \\(P(A)\\), as the proportion of the sample space the event occupies. To differentiate events from their probabilities, we will shade the area of the event.\n\nKolmogorov’s Axioms and their direct consequences.\n\nDirectly from the Kolmogorov Axioms, one can derive (Jaynes 2003) other properties (see [fig:axiom1, fig:axiom2, fig:axiom3]): \\[\\begin{aligned}\nP(\\emptyset)&=0\\\\\nB \\subset A &\\implies P(B) \\leq P(A)\\\\\n0 &\\leq P(A) \\leq 1\\\\\nP(\\bar{A})&=1-P(A).\n\\end{aligned}\\]"
  },
  {
    "objectID": "Chapters/prob.html#joint-event",
    "href": "Chapters/prob.html#joint-event",
    "title": "Probability Theory",
    "section": "Joint event",
    "text": "Joint event\n\nA joint event (A, B) is the set of outcomes where: \\[(A, B) = {\\omega \\in \\Omega: (\\omega \\in A \\cap B) }\\] Therefore, \\[P(A, B) =P({\\omega \\in \\Omega: (\\omega \\in A \\cap B) })\\]\n\nWhen talking about events as propositions, it is straightforward to use logic notation \\(P(A \\land B)\\), but when we start to use random variables (1.10), we will adopt the shorthand notation \\(P(\\rvA, \\rvB)\\)."
  },
  {
    "objectID": "Chapters/prob.html#sec:independent_events",
    "href": "Chapters/prob.html#sec:independent_events",
    "title": "Probability Theory",
    "section": "Independent events",
    "text": "Independent events\n\n Events \\(A\\) and \\(B\\) are independent (\\(A \\ind B\\)) if: \\[\\begin{aligned}\nA\\neq \\emptyset, B\\neq \\emptyset \\implies P(A)>0, P(B)>0\\label{eq:P(A, B)>0}\\\\\nP(A, B) = P(A \\land B) = P(A) \\cdot P(B)\\label{eq:Product_Rule}\\\\\n\\nonumber \\tag{Product Rule}\n\\end{aligned}\\]\n\nDisjoint events cannot be independent, since (from [eq:P(A, B)>0]) \\(P(A) \\cdot P(B)> 0\\), but as disjoint events ([fig:disjoint_events]) \\(P(A \\land B)=P(\\emptyset)=0\\), leading to contradiction.\nIndependence can be assumed or derived by verifying: \\[\\begin{aligned}\nP(A \\land B)= P(A) \\cdot P(B).\\\\\n\\nonumber \\tag{Independent variables}\n\\end{aligned}\\]"
  },
  {
    "objectID": "Chapters/prob.html#conditional-probability",
    "href": "Chapters/prob.html#conditional-probability",
    "title": "Probability Theory",
    "section": "Conditional probability",
    "text": "Conditional probability\nAs we have explained before (1.1.3.0.1), the plausibility of an outcome or a set of outcomes depends on a web of interconnected prior beliefs. So, what exists are probabilities conditional to a given prior assumption.\n\nIf \\(P(B)>0\\) then the conditional probability of A given B is: \\[\\begin{aligned}\n\\label{eq:conditional_probability}\nP(A|B) \\eqdef \\frac{P(A,B)}{P(B)}\n\\end{aligned}\\] \\[\\begin{aligned}\n\\label{eq:joint_probability}\nP(A, B) \\eqdef P(A|B)\\cdot P(B)\n\\end{aligned}\\]\n\nExcept if \\(P(A) \\equiv P(B)\\), \\(P(A|B) \\neq P(B|A)\\). Also, \\(P(A|B)=P(A) \\iff A \\ind B\\).1010 Venn diagrams are not helpful to see that the events are independent, as it all depends on the areas of intersection and the sizes of A and B, which are tricky to estimate without computational help."
  },
  {
    "objectID": "Chapters/prob.html#marginalisation",
    "href": "Chapters/prob.html#marginalisation",
    "title": "Probability Theory",
    "section": "Marginal probability",
    "text": "Marginal probability\n\nLet \\(A_1, \\cdots, A_k\\) be a partition of \\(\\Omega\\). Then, for any event B, \\[\\begin{aligned}\nP(B)=\\sum_{i=1}^k P(B|A_i)\\cdot P(A_i)\\label{eq:law_of_total_probabilities}\n\\end{aligned}\\]\n\n\nProof. Proof. 11 Define \\(C_i = (B,A_i)\\). Let \\(C_1, \\cdots C_k\\) be disjoint and \\(B = \\bigcup\\limits_{i=1}^k C_i\\).\nTherefore:11 Remember: \\((B, A) \\equiv (B \\cap A)\\).\n$$\n\\[\\begin{aligned}\n        P(B) &\\triangleq P(\\bigcup\\limits_{i=1}^k C_i)\n        \\overset{\\text{\\ref{eq:sum_rule}}}{=} \\sum_i P(C_i)\\\\\n        &\\triangleq \\sum_i P(B,A_i)\n        \\overset{\\text{\\ref{eq:conditional_probability}}}{=} \\sum_{i=1}^k P(B|A_i)\\cdot P(A_i) \\tag{Law of Total Probability}\n    \n\\end{aligned}\\]\n$$ ◻"
  },
  {
    "objectID": "Chapters/prob.html#sec:bayes_theorem",
    "href": "Chapters/prob.html#sec:bayes_theorem",
    "title": "Probability Theory",
    "section": "Bayes’ theorem",
    "text": "Bayes’ theorem\n\nLet \\(A_1, \\cdots, A_k\\) be a partition of \\(\\Omega\\) s.t. \\(P(A_i)>0, \\forall i\\) then, \\(\\forall i=1, \\cdots, k\\): $$\n\\[\\begin{aligned}\n        P(A_i|B)= \\frac{P(B|A_i)\\cdot P(A_i)}{\\sum_i P(B|A_i)\\cdot P(A_i)}\n    \n\\end{aligned}\\]\n$$\n\n\nProof. Proof. From equations [eq:conditional_probability], [eq:joint_probability] and [eq:law_of_total_probabilities]: $$\n\\[\\begin{aligned}\n        P(A_i|B)&\\overset{\\text{\\ref{eq:conditional_probability}}}{=}\\frac{P(A_i,B)}{P(B)} \\overset{\\text{\\ref{eq:joint_probability}}}{=} \\frac{P(B|A_i) \\cdot P(A_i)}{P(B)}  \\\\\n        &\\overset{\\text{\\ref{eq:law_of_total_probabilities}}}{=}\\frac{P(B|A_i)\\cdot P(A_i)}{\\sum_{i=1}^k P(B|A_i)\\cdot P(A_i)}\n    \n\\end{aligned}\\]\n$$ ◻\n\nWe call \\(P(A_i)\\) the prior of A, and \\(P(A_i|B)\\) the posterior probability of A."
  },
  {
    "objectID": "Chapters/prob.html#sec:random_variables",
    "href": "Chapters/prob.html#sec:random_variables",
    "title": "Probability Theory",
    "section": "Random variables",
    "text": "Random variables\n\nA random variable is a mapping \\(\\rvX:\\Omega \\to \\Real\\) that assigns a real number \\(\\rvX(\\omega)\\) to each outcome \\(\\omega\\), \\(\\omega \\mapsto \\rvX(\\omega)\\).\n\nGiven a random variable \\(\\rvX\\), the probability of an outcome \\(\\rx\\) can be expressed as: \\[\\begin{aligned}\n    P(\\rvX=\\rx) = P(\\rvX^{-1}(\\rx)) = P(\\{\\omega \\in \\Omega: \\rvX(\\omega)=\\rx\\})\\label{eq:P(X=x)}\n\\end{aligned}\\]\nSeveral works on Probability Theory choose to start by defining random variables, rarely mentioning sample spaces, events or the connection with logical propositions.\nThis usual approach is, nevertheless, confusing. Beyond the fact that random variables are not variables, but functions, nor random, they model uncertain events; it is hard to grasp what random variables are without understanding their reasons for being.\nThe difference between a random variable \\(\\rvX\\) and its “realisation” is the difference between a distribution and a sample from that distribution. In particular, a random variable \\(\\rvX\\) is “formalised” in terms of a function from the sample space to some result space, typically \\(\\Real\\). The realisation of a random variable is “what you get” when an experiment is run, and you apply \\(\\rvX\\) to events that happened.\n\nNotation abuse\nIf a random variable is a function, how can we write \\(P(\\rvX=4)\\) or \\(P(\\rvX > 7)\\)? Such confusion is due to some notation abuse that became standard in works on probability theory. It is not easy to grasp it initially, but the explanation was already stated at [eq:P(X=x)]. \\(P(\\rvX=\\rx)\\) is a shorthand for \\(P(\\rvX^{-1}(\\rx))\\).\nTechnically, a random variable is a function. In practice, it is just a mathematical tool to help us associate propositions with numbers. It is called a random variable because the notation abuse treats the function as a variable.\nTo help clear up such confusion, let us recap a little the notation we have established before:\nIn the canonical experiment of rolling a dice, instead of writing the proposition “The dice will roll to number 4.” plausibility is \\(\\frac{1}{6}\\), it is easier to assign a letter to the proposition, or as we called the event. Let us use event \\(D\\) to represent the proposition. Then, we can use \\(P(D)=\\frac{1}{6}\\). Now, we are going one step further; instead of using the event \\(D\\) we use the random variable \\(\\rvD\\), in italic, and say \\(P(\\rvD=4)=\\frac{1}{6}\\).\nNotice the difference between a random variable and an event:12 \\(\\rvD\\) could assume any value (even \\(\\rvD=7\\), which is outside of our sample space). Would it not be easier then to use an index to the event letters, \\(D_4\\) to value 4, and \\(D_1\\) to value 1, etc.? Not really.12 An event can be seen as a special kind of random variable. , a random variable \\(\\rvD\\) is the truth function (also known as the indicator function) over an event \\(D\\): \\[\\begin{aligned}\n    \\rvD=\\truth_D\n\\end{aligned}\\] That is the reason one can say that “random variables define events.”\nBesides providing this shorter notation, the mapping of the random variable allows us to manipulate events as numbers: for example, we can chart probability distributions using random variables, which we cannot cope with events."
  },
  {
    "objectID": "Chapters/prob.html#probability-distributions",
    "href": "Chapters/prob.html#probability-distributions",
    "title": "Probability Theory",
    "section": "Probability Distributions",
    "text": "Probability Distributions\n\nA probability distribution of a discrete random variable \\(\\rvX\\) or probability mass function (pmf) is a function \\(p: \\Omega \\to [0,1]\\) that provides the probabilities of occurrence of different possible outcomes in an experiment (sample space):\n$$\n\\[\\begin{aligned}\n        p(\\rx) = P(\\rvX = \\rx), \\tag{pmf}\n    \n\\end{aligned}\\]\n$$\n\nIf \\(\\rvX\\) is continuous, \\(P(\\rvX=\\rx)\\to 0\\), therefore we need to use intervals in this case.\n\nA probability distribution of a countinous random variable \\(\\rvX\\) in an interval \\(A\\), or probability density function (pdf) is a function \\(p(\\rx)\\) that measures the probability of randomly selecting a value within the interval \\(A=[a, b]\\), as the area under its curve for the interval A:\n\nP(A) &= P[a b] = _a^b p()   d,\n&\n\np() , x\n_^ p()   d= 1\n\n\n\nNow that we explained what distributions are,13 here we highlight some useful distributions:13 In this dissertation, we will use \\(P(\\rvX)\\) to express the probability of a random variable, and \\(p(\\rx)\\) to represent a pmf or pdf of the random variable outcomes.\n\nStatistical model\nA statistical model is a function \\(p_{\\theta}(\\rx) \\equiv p(\\rx | \\theta)\\) representing the relationship between a parameter14 \\(\\theta\\) and potential outcomes \\(\\rx\\) of a random variable \\(\\rvX\\). In practice, we usually define a statistical model of a stochastic process for which we do not know the real distribution. Therefore, the parameter \\(\\theta\\) has to be inferred from the observed data.14 In this dissertation we are interested in vector-valued \\(\\theta\\).\n\n\nUniform distribution\n\\(\\nonumber \\\\\\rvX \\sim \\text{Uniform}(a,b)\\), if:\n\\[\\begin{aligned}\n    p(\\rx)=\n    \\begin{cases}\n        \\frac{1}{b-a} & x \\in [a,b]\\\\\n        0 & \\rx \\notin [a,b]\n    \\end{cases}\n\\end{aligned}\\]\n\n\nNormal distribution\n\\(\\nonumber \\\\\\rvX \\sim \\mathcal{N}(\\mu, \\sigma^2)\\), if: \\[\\begin{aligned}\n    p(\\rx)=\\frac{1}{\\sigma \\sqrt{2\\pi}}\\exp{\\Biggl{\\{}{-\\frac{1}{2\\sigma^2}{(x-\\mu)}^2}\\Biggr{\\}}}, \\\\~x \\in \\Real \\\\\n\\end{aligned}\\] where \\(\\mu \\in \\Real\\) (mean) and \\(\\sigma > 0\\) (standard deviation). We say that \\(\\rvX\\) has a standard Normal distribution if \\(\\mu = 0\\), \\(\\sigma =1\\).\n\n\nExponential distribution\n\\(\\rvX \\sim \\text{Exp}(\\lambda)\\), if: \\[\\begin{aligned}\n    p(\\rx;\\lambda) =\n    \\begin{cases}\n        \\lambda e^{-\\lambda \\rx} & \\rx \\ge 0, \\\\\n        0 & \\rx < 0.\n    \\end{cases}\n\\end{aligned}\\] where \\(\\lambda > 0\\) is the rate parameter of the distribution."
  },
  {
    "objectID": "Chapters/prob.html#joint-distributions",
    "href": "Chapters/prob.html#joint-distributions",
    "title": "Probability Theory",
    "section": "Joint Distributions",
    "text": "Joint Distributions\n\nGiven a pair of discrete random variables \\(\\rvX\\) and \\(\\rvY\\), we define the joint mass function by \\(p(\\rx, \\ry)=P(\\rvX=\\rx,\\rvY=\\ry)\\).\n\n\nGiven a pair of continuous random variables \\(\\rvX\\) and \\(\\rvY\\), we define the joint density function by \\(p(\\rx, \\ry)\\), where:\n\n\\(p(\\rx, \\ry) \\geq 0\\)\n\\(\\iint_{-\\infty}^{\\infty} p(\\rx,\\ry) \\, d\\rx d\\ry =1\\)\n\\(\\forall A \\subset \\Real \\times \\Real, P((\\rvX,\\rvY)\\in A)=\\iint_{A}p(\\rx,\\ry)\\, d\\rx d\\ry\\)."
  },
  {
    "objectID": "Chapters/prob.html#expectancy-variance-and-covariance",
    "href": "Chapters/prob.html#expectancy-variance-and-covariance",
    "title": "Probability Theory",
    "section": "Expectancy, Variance and Covariance",
    "text": "Expectancy, Variance and Covariance\n\nThe expected value or mean of \\(\\rvX\\) is: $$\n\\[\\begin{aligned}\n        \\E (\\rvX)=\\langle \\rvX \\rangle = \\SumInt_x \\rx~p(\\rx)~dx = \\mu = \\mu_X\n    \n\\end{aligned}\\]\n$$\n\n\nLet \\(\\rvX_1, \\cdots, \\rvX_n\\) be random variables and \\(a_1, \\cdots, a_n\\) be constants, then from the Sum Rule: $$\n\\[\\begin{aligned}\n        \\E \\biggl(\\sum_i a_i\\rvX_i\\biggr)=\\sum_i a_i(\\E (\\rvX_i))\n    \n\\end{aligned}\\]\n$$\n\n\nLet \\(\\rvX_1, \\cdots, \\rvX_n\\) be independent random variables, then from the Product Rule: $$\n\\[\\begin{aligned}\n        \\E (\\prod_i \\rvX_i)=\\prod_i \\E (\\rvX_i)\n    \n\\end{aligned}\\]\n$$\n\n\nLet \\(\\rvX\\) be a random variable with mean \\(\\mu\\). The variance of \\(\\rvX\\) is defined by: $$\n\\[\\begin{aligned}\n        \\sigma^2 = \\sigma_{\\rvX}^2 =\\E {(\\rvX - \\mu)}^2\n    \n\\end{aligned}\\]\n$$ assumming this expectation exists. The standard deviation is \\(\\sigma\\).\n\n\nLet \\({\\rvX}\\) and \\({\\rvY}\\) be random variables with means \\(\\mu_{\\rvX}\\) and \\(\\mu_{\\rvY}\\), and with standard deviations \\(\\sigma_{\\rvX}\\) and \\(\\sigma_{\\rvY}\\). The covariance between \\({\\rvX}\\) and \\({\\rvY}\\) is defined as (Wasserman 2013, 74): $$\n\\[\\begin{aligned}\n        \\operatorname{Cov}({\\rvX},{\\rvY}) = \\E (({\\rvX} - \\mu_{\\rvX})({\\rvY} - \\mu_{\\rvY}))\n    \n\\end{aligned}\\]\n\\[ and the correlation as: \\]\n\\[\\begin{aligned}\n        \\rho = \\rho_{{\\rvX},{\\rvY}} = \\rho({\\rvX},{\\rvY}) = \\frac{\\operatorname{Cov}({\\rvX},{\\rvY})}{\\sigma_{\\rvX} \\sigma_{\\rvY}}\n    \n\\end{aligned}\\]\n$$\n\n\nThe covariance satisfies: $$\n\\[\\begin{aligned}\n        \\operatorname{Cov}({\\rvX},{\\rvY})=\\E ({\\rvX}{\\rvY})- \\E({\\rvX}) \\E({\\rvY}).\n    \n\\end{aligned}\\]\n$$"
  },
  {
    "objectID": "Chapters/prob.html#independent-sampling",
    "href": "Chapters/prob.html#independent-sampling",
    "title": "Probability Theory",
    "section": "Independent Sampling",
    "text": "Independent Sampling\nA sample is a set of examples15 drawn from a distribution. One common assumption in Machine Learning Theory is that examples are identically and independently distributed — i.i.d. This means that the probability of obtaining a first training example. \\((\\rx_1, \\ry_1)\\) does not affect which \\((\\rx_2, \\ry_2)\\) will be drawn in the following observation.15 In this dissertation, an element of a sampling is called an example.\nThe i.i.d. assumption is useful wherever a census of the population of interest, knowing all possible values, is unfeasible. In this usual case, data analysis is carried out using a sample to represent the population. When the sample is i.i.d., each example in the population has the same chance of being observed ([fig:sampling] — left).\nIf there is a constraint on which examples of the population are sampled, we say that the sample is biased ([fig:sampling] — right)."
  },
  {
    "objectID": "Chapters/prob.html#concluding-remarks",
    "href": "Chapters/prob.html#concluding-remarks",
    "title": "Probability Theory",
    "section": "Concluding Remarks",
    "text": "Concluding Remarks\nThis chapter derived Logic from the definition of knowledge as absolute truth and Probability Theory from knowledge as justified beliefs ([sec:from_rationalism, sec:from_empiricism]). To remind that our definition of knowledge is the basis for the Bayesian perspective of probability and that inference methods are languages, we can say (and prefer) that we derived Bayesian inference as the language of science. We proved what we claimed in the previous chapter ([ch:artificial_intelligence]).\nWe needed to define formal languages (1.1.1) and assume desiderata for the languages we wanted to build formally ([sec:desiderata_language,sec:desiderata_language_sceptical]). We called rational agents the epistemic agents that use Logic as its inference method, and sceptical agents use Bayesian inference.\nWe found out that the desiderata for the sceptical language are equivalent to Cox’s axioms ([cox]). From Cox’s axioms, it is possible to derive Kolmogorov’s axioms of Probability Theory. Which made us conclude that Bayesian inference is the language of science.1616 Our definition of knowledge hinted at a Bayesian perspective of knowledge.\nFrom the derivation, we did a basic Statistics review (influenced by (Wasserman 2013)). Many essential topics were left out from this short review chapter, where the focus was to present the concepts that we will use later on in this dissertation.\n\nAssumptions\n\nA definition of intelligence ([def:intelligence]);\nA epistemic choice on the definition of Knowledge ([sec:rationalism, sec:empiricism]);\nA definition of formal language;\nCommon assumptions of the epistemic agent language:\n\nconsistency (1.1.3.0.2, [consistency] and 1.1.2.0.2, [rational_consistency]);\nminimality (1.1.2.0.2, [rational_minimality]).\n\nAssumption of the rational agent language:\n\nknowledge is absolute, a set of true or false sentences (1.1.2.0.2, [absolute_truth]);\nthe language must be unambiguous (1.1.2.0.2, [unambiguous]).\n\nAssumption of the sceptical agent language:\n\nKnowledge is a set of beliefs, quantifiable by real numbers and dependent on prior evidence (1.1.3.0.2, [beliefs]);\nCommon sense: The plausibility of compound sentences should be related by some logical function to the plausibility of the sentences that form them (1.1.3.0.2, [common_sense]).\n\n\nAs we have settled that our focus is Deep Learning, which relates to the sceptical agent, we will abstain from keeping the rational language assumptions in our analysis and assume an epistemic agent is sceptical.\n\n\n\n\nCaticha, Ariel. 2008. “Lectures on Probability, Entropy, and Statistical Physics.” https://arxiv.org/abs/0808.0012.\n\n\nJaynes, E. T. 2003. Probability Theory: The Logic of Science. Cambridge University Press.\n\n\nSowinski, Damian Radoslaw. 2016. “Complexity and Stability for Epistemic Agents: The Foundations and Phenomenology of Configurational Entropy.” PhD thesis.\n\n\nTerenin, Alexander, and David Draper. 2015. “Cox’s Theorem and the Jaynesian Interpretation of Probability.” https://arxiv.org/abs/1507.06597.\n\n\nWasserman, Larry. 2013. All of Statistics: A Concise Course in Statistical Inference. Springer Science & Business Media."
  },
  {
    "objectID": "Chapters/ai.html",
    "href": "Chapters/ai.html",
    "title": "The emergence of\\\nan Information Bottleneck Theory\\\nof Deep Learning\\\n",
    "section": "",
    "text": "Introduction\n                \n  \n  \n      \n        Probability Theory"
  }
]