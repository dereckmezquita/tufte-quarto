# Information-Theoretical Machine Learning: An Epistemology {#ch:it_epistemology}

[]{#ch:intermezzo label="ch:intermezzo"}

This chapter discusses an [ITML]{acronym-label="ITML" acronym-form="singular+full"}[^1] perspective not specific to the [IB]{acronym-label="IB" acronym-form="singular+short"} Principle.

## Learning as a conversation with Nature {#sec:conversation}

Imagine some "Law of Nature" ($\rvT$)[^2][^3] an epistemic agent can comprehend.[^4] $\rvT$ explains the relationship among observations in $D$. We can think of learning as communication between Nature and the epistemic agent.

We assume learning is possible, $\rvT$ is encoded in the observed data $\rvD$. $\hat{\rvT}$ is what the epistemic agent understand about $\rvT$ through $\rvD$, a representation of $\rvT$ in the agent's "mind". $\hat{\rvT}$ is the agent's **understanding**. $$\begin{aligned}
  \hat{\rvT}:=U(\rvD) \tag{$\hat{\rvT}$ is an understanding of $\rvT$ through $\rvD$}
\end{aligned}$$

In this scenario, $D$ is an **expression** of $T$. $$\begin{aligned}
  E(\rvT)=:\rvD \tag{$\rvD$ is an expression of $\rvT$}
\end{aligned}$$

As we do not know the smallest representation size of $\rvT$, $H[\rvT]$, we do not know if the $\rvT\to\rvD$ channel capacity ($C_{\rvD}=I[\rvD;\rvT]$), is enough to noiseless transmit $\rvT$ through $\rvD$. Therefore, we have to admit that the encoding of $\rvT$ into $\rvD$ is lossy.[^5] Thus, $E$ is stochastic, and the understanding of the agent shall be stochastic as well: $$\begin{aligned}
  &E(\rvT)=P(\rvD|\rvT),\\
  &U(\rvD)=Q(\hat{\rvT}|\rvD).
\end{aligned}$$

While $\hat{\rvT}$ is only in the epistemic agent mind, it has no practical importance for other agents. The agent will need to encode $\hat{\rvT}$ into an agreed $(n,k)$ language/code to communicate with other agents.

A **hypothesis $h$** is the epistemic agent's attempt to represent the compressed description of the observation in her mind into the agreed language $\rvX$, $h:=\rvX(\hat{\rvT})$. Without loss of generality, we can assume that any agent mind has the same size in bits and, as a consequence, $\rvX(\hat{\rvT})$ is a lossless encoding. Therefore,[^6] $h:=\hat{\rvT}$.

Moreover, $h$ is falsifiable, as any agent can use $h$ to predict $\hat{\rvD}:=Q(\rvD|h)$. The $Q(h,D)$ distribution contains the **understanding** of the epistemic agent of the "Law of Nature" ($Q(h|\rvD)$) and the **expression** of this understanding (the prediction $Q(\rvD|h)$). In other words, $Q(\rvD,\rvH)$ defines an encoder (understanding) - decoder (expression).[^7]

If other epistemic agents have competing hypothesis ($h_j, h_k, \ldots$), how should we select the best hypothesis?

The best hypothesis is the one that on average describes $D$ with $H[\rvD]$ bits. Any hypothesis that take less bits than $H[\rvD]$ cannot perfectly reconstruct $\rvD$ (underfitting). Any hypothesis that uses more than $H[\rvD]$ bits is adding spurious correlations to the data (overfitting) and might not generalise well.

Besides selecting the best model among the available competitors, the epistemic agent wants to transform her winning hypothesis into a theory that works within a tolerance of error ($\epsilon$) and margin of confidence ($\delta$):

$$\begin{aligned}
  \Pr{\truth_{\theory(\rvD) \neq \rvT(\rvD)} \leq \epsilon} \geq (1-\delta).
\end{aligned}$$

In reality, unfortunately, she can access only a sample $\rvS^{n}$ of the true distribution of the data $P(\rvD|\rvT)$. How confident can agents be in the performance of $h$ in future data if they can only access the error of $h$ in the sample (past) data?

## PAC-Shannon {#sec:pac-shannon}

This section will use Shannon's theorems to give PAC bounds to the information-theoretical learning setting presented in the previous section.

We recognise that:

1.  [ITML]{acronym-label="ITML" acronym-form="singular+short"} setting is equivalent to [MDL]{acronym-label="MDL" acronym-form="singular+short"} (which will be described in [1.5]);

2.  using information in the weights as a measure of complexity was already discussed by other authors ( [@tishby:2020DeepMath; @achille:2019phd; @shamir:2010]); and also that

3.  @shamir:2010 has presented the first PAC formulation of [IBT]{acronym-label="IBT" acronym-form="singular+short"} [@shamir:2010].

Yet, to the extent of our knowledge, the specific PAC formulation we are about to describe is an original contribution of this dissertation.[^8]

Recall :

We can rewrite it as:

::: theorem
[]{#th:shannon_mlt_1 label="th:shannon_mlt_1"} Let $X=\rx \sim P(X)$ and $S^{n}=\left\{ x_1,..,x_n \right\}, x~\sim~P(X)$, $$\begin{aligned}
  Pr\left[ \left( \frac{H[\rvS^{n}]}{n} - H[\rvX] \right)>\epsilon \right] < \delta
  
\end{aligned}$$
:::

The same for :

Let us rewrite this theorem in parts. First, we use $Q$ to denote our hypothesis $Q=h:\XX \times \Theta \to \YY$. We also assume that $\XX \equiv \sA_{\rvX}$ and $\YY \equiv \sA_{\rvY}$ are finite. In Information-theoreical terms, $Q$ is an encoder of the alphabet $\sA_{\rvX}$ to the alphabet $\sA_{\rvY}$. If all rates bellow capacity $C=\IXY$ are achievable[^9][^10], $$\begin{aligned}
  \exists Q: \text{Rate}(Q)\leq\IXY
\end{aligned}$$ The theorem also says that this encoder has maximum probability of error $\lambda^{(n)} \to 0$. From that we can infer that the expected error of $Q, R(Q)$, is arbitrarily small. $$\begin{aligned}
  \exists Q: \text{Rate}(Q)\leq\IXY, R(Q) < \epsilon
\end{aligned}$$ Now we can summarize this in a theorem statement:

::: theorem
[]{#th:shannon_mlt_2 label="th:shannon_mlt_2"} Let two discrete random variables (from finite spaces) represent $\rvX$, the input, and $\rvY$, the output of a stochastic mapping $\QXY$ (lossy encoder/channel). $(\rvX, \rvY) \sim \PXY$, $P$ is unknown and let $r$ represent the information rate of $Q$ (the expected number of bits it needs to represent a symbol of the alphabet $\sA_{\rvX}$). $$\begin{aligned}
  \forall r : r \leq \IXY, \\
  \exists Q:  \Rate(Q)=r, R(Q) < \epsilon\label{eq:shannon_mlt_2}
  
\end{aligned}$$
:::

### Shannon guarantees

Let $\rvD \sim P(\rvD)$ represent observable data and $S^{n}$ a sample of $n$ observations from $D$. We assume that the hypothesis $h$ is parametrised by $\rvW$. Let a learning algorithm $\AA: \rvD \to \rvW, \AA:=\QWD$ generate the hypothesis via $\rvW$. By [RI]{acronym-label="RI" acronym-form="singular+full"}, $H[h(\rvW)]=H[\rvW]$. If $h$ trained with the sample $S^{n}$ achieves training error $\varepsilon_{S^{n}}(h)$. What is the expected out-of-sample error of h, $\varepsilon_{\rvD}(h)$?

For simplification sake, let us assume the supervised case where $\rvD=(\rvX, \rvY)$ and $\rvY \in \{0,1\}$.[^11] By , $$\begin{aligned}
  \exists h_{\text{Shannon}}:=Q(D|w^*): \Rate(h_{\text{Shannon}})&=I_{P(D)}[{\rvW};{\rvD}],~ \text{and}\\
  \varepsilon_{\rvD}(h_{\text{Shannon}}) = R_{\rvD}(h_{\text{Shannon}})&<\epsilon
\end{aligned}$$ Let us remember that $h_{\text{Shannon}} \in \HH_{Q}$ is the theoretical optimal $Q^{\star}(\rvX|\rvW)\to \hat{\rvY}$. Also, in [\[noisy_channel_theorem\]][1], we saw that the channel capacity $\IWD$ defines the number of non-confusable inputs/mappings (or the number of confused mappings limited to a certain $\epsilon$ margin), $2^{\IWD}$ [\[eq:typical_set_size\]][2]. We then call $\HH^{\delta}_{Q}$ the typical hypothesis space of $h$ and we know that its cardinality can be computed:

$$\begin{aligned}
  \Rate(Q^*) = \IWD\\
  |\tQ|=|\sA_{\rvY}| ^ {2^{\IWD}} = 2^{2^{\IWD}}\label{eq:typical_hypothesis_space_cardinality}
\end{aligned}$$ Serendipitously, we purposely did not restrict our hypothesis, but [\[eq:typical_hypothesis_space_cardinality\]][3] produced for us the cardinality of the hypothesis space of the solutions within a tolerance error and confidence.

Using , we can already give an upper bound to the out-of-sample error of $h$: $$\begin{aligned}
  \epsilon(h) \leq &\sqrt{\frac{\ln{2^{2^{\IWD}}}+ \ln{2/\delta}}{2n}}\\
  \approx~&\sqrt{\frac{2^{\IDW} + \textit{const.} + \ln{2/\delta}}{2n}}\\
  \therefore~ n~ \leq~ &\frac{ 2^{\IWD} + \ln{2/\delta}}{2\epsilon^2}
\end{aligned}$$ We get a non-vacuous bound as long as $\OO\,(\IWD) < \OO\,(\log n)$. Unfortunately, we cannot access the true $\IWD$; we only access its empirical approximation $\empIWD$.

Let us see if we can at least bound the true mutual information between $\rvW$ and $\rvD$. Let us choose the [KL]{acronym-label="KL" acronym-form="singular+short"} as a loss function.[^12] $$\begin{aligned}
  R_D(h)&=\E(\varepsilon(h)) \\
  &= \E_{\rvW,\rvD} \left[ \KL (P(\rvW, \rvD) \Vert Q(\rvW, \rvD)) \right]\\
  &= \E_{\rvW,\rvD} \left[ \log \frac{P(\rvW, \rvD)} {Q(\rvW, \rvD)} \right]\\
  &= - \E_{\rvW,\rvD} \left[  \log \frac{Q(\rvW, \rvD)} {P(\rvW, \rvD)} \right]\\
  &= - \E_{\rvW,\rvD} \left[  \log Q(\rvW, \rvD) - \cancelto{\text{const.}}{\log P(\rvW, \rvD)}  \right]\\
  &\approx - \E_{\rvW,\rvD} \left[  \log Q(\rvW, \rvD) \right] + k = \IWD + k\\
  R_{\rvS^{n}}&= - \E_{\rvW,\rvS^{n}} \left[  \log Q(\rvW, \rvS^{n}) \right] = \empIWD \\
  &\OO \left( |R_{\rvS^{n}}(h) - R_D(h)| \right) = \OO \left( |\empIWD - \IWD| \right)
\end{aligned}$$

From , $$\begin{aligned}
  \pr(|\empIWD - \IWD|> \epsilon)<\delta \label{shannon_inequality} %
\end{aligned}$$

Now we can follow  [@haussler:1988] steps as we did in [\[sec:pac-bayes\]][4].

::: theorem
[]{#th:pac-shannon_1.1 label="th:pac-shannon_1.1"} Let $\AA$ be a learning algorithm that returns a consistent hypothesis $h$, i.e. $\hat{R}_{S}(h)=0$, for any hypothesis $h$ and unknown distribution $\DD=\PXY$. Let $|S|=n$, then, $\forall n \geq N_0$: $$\begin{aligned}
        Pr[h \in \HH: R_{\DD}(h)>\epsilon]< e^{- \epsilon n + \empIWD }
    
\end{aligned}$$
:::

::: proof
*Proof.* Let $h$ be parametrised by $\rvW$ and the empirical mutual information of the weights w.r.t the available sample $S^{n}$ be $\empIWD$. From [\[shannon_inequality\]][5], let us call $h_{\text{bad}}$ a consistent hypothesis that does not generalises and $\HH_{\text{bad}}$ the space of all possible bad hypotheses. $$\begin{aligned}
  Pr~(|\empIWD - \IWD|> \epsilon)&<\delta ~\therefore \\
  Pr~[R(h)=0 \land |\empIWD - \IWD| >\epsilon]&=1-\delta\\
  \E_S[R(h)=0 \land |\empIWD - \IWD| >\epsilon]&=(1-\delta)^n\\
  \E_D[R(h)=0 \land |\empIWD - \IWD| >\epsilon]&=|\HH_{\text{bad}}|(1-\delta)^n\\
\end{aligned}$$ Fortunately, we know how to find the cardinality of $\HH_{\text{bad}}$. $\empIWD$ is our channel capacity, the number of typical different encodings (or transformations) we can have. Every transformation of an input $\rvX$ can lead to $|\sA_{\rvY}|$ values. Therefore, $|\HH^{\delta}_{\rvD}| \approx 2^{2^{\IWD}}$ and $|\HH^{\delta}_{S^{n}}| \approx 2^{2^{\empIXY}}$. Consequently, $|\HH_{\text{bad}}| = 2^{2^{|\empIWD - \IWD|}}$. From where we follow: $$\begin{aligned}
  \E_D[R(h)=0 \land |\empIWD - \IWD| >\epsilon]&=|\HH_{\text{bad}}|(1-\delta)^n\\
  &=|\HH_{\text{bad}}|~ e^{-\epsilon n} \\
 \epsilon &< e^{- \epsilon n + 2^{|\empIWD - \IWD|}}
\end{aligned}$$ As we already said, $\IWD$ is intractable, but we still can get a bound: $$\begin{aligned}
  \epsilon< e^{- \epsilon n + 2^{\empIWD}}
\end{aligned}$$ ◻
:::

::: theorem
[]{#th:pac-shannon_1.2 label="th:pac-shannon_1.2"} A learning algorithm $\AA$ can learn task with: $$\begin{aligned}
    n < \frac{1}{\epsilon}\left(\empIWD +\ln{\frac{1}{\delta}}\right)
  
\end{aligned}$$ training examples.
:::

::: proof
*Proof.* $$\begin{aligned}
        \delta &> e^{- \epsilon n + 2^{\empIWD}} \\
    \ln{\delta} &>- \epsilon n + 2^{\empIWD} \\
        \epsilon n &< 2^{\empIWD} - \ln{\delta} \\
        n &< \frac{1}{\epsilon}(2^{\empIWD} - \ln{\delta}) \\
        n \in &\OO\,\biggl( \frac{1}{\epsilon}\left(2^{\empIWD} - \ln{\delta}\right)\biggr)
    
\end{aligned}$$ ◻
:::

::: theorem
[]{#th:pac-shannon_2.1 label="th:pac-shannon_2.1"} Let $\AA$ be a learning algorithm that returns an inconsistent hypothesis $h$, i.e. $\hat{R}_{S}(h)>0$, for any hypothesis $h$ and unknown distribution $\DD=\PXY$. Let $|S|=n$, then, $\forall n \geq N_0$: $$\begin{aligned}
        \epsilon< \sqrt{\frac{ 2^{\empIWD} + \ln\dfrac{2}{\delta}}{2n}}
    
\end{aligned}$$
:::

::: proof
*Proof.* Using the Chernoff-Hoeffding inequality and the union bound as [\[th:haussler\]][6] and [\[sec:pac-bayes\]][4], we have: $$\begin{aligned}
    Pr[w \in W:|\empIWD - \IWD| > \epsilon] &< 2e^{-2n\epsilon^2}\\
    Pr[w \in W:|\empIWD - \IWD| > \epsilon] &= \frac{1}{|\HH^{\delta}_Q|}\delta\\
    2e^{-2n\epsilon^2} &< |\HH^{\delta}_Q|^{-1}\delta  \\
    ln 2 - 2n\epsilon^2 &< -\ln{|\HH^{\delta}_Q|} + \ln{\delta}\\
    \epsilon &< \sqrt{\frac{\ln |\HH^{\delta}_Q| + \ln\frac{2}{\delta}}{2n}}\\
    \epsilon &< \sqrt{\frac{\ln 2^{2^{\empIWD}} + \ln\frac{2}{\delta}}{2n}}\\
    \epsilon &< \sqrt{\frac{2^{\empIWD}\cancelto{1}{\ln2} + \ln\frac{2}{\delta}}{2n}}
  
\end{aligned}$$ ◻
:::

::: theorem
[]{#th:pac-shannon_2.2 label="th:pac-shannon_2.2"}A learning algorithm $\AA$ can learn task with: $$\begin{aligned}
      n~<~\frac{ 2^{\empIWD} + \ln\dfrac{2}{\delta}}{2\epsilon^2}
    
\end{aligned}$$ training examples.
:::

::: proof
*Proof.* $$\begin{aligned}
      \epsilon^2 &< \frac{ 2^{\empIWD} + \ln\dfrac{2}{\delta}}{2n}\\
      n~&<~\frac{ 2^{\empIWD} + \ln\dfrac{2}{\delta}}{2\epsilon^2}
    
\end{aligned}$$ ◻
:::

## "Reals" are not really a problem {#sec:real_problem}

A possible weakness of the proposed [ITML]{acronym-label="ITML" acronym-form="singular+short"} perspective is that we limited the space of the data $D$ to a finite set (discrete random variable).

Foremost, there is a mathematical argument  [@chaitin:2006 99-116] against the physical existence of a "continuum": after all, some real numbers are uncomputable [@turing:1936].[^13] Similarly, in [1.1], we argued that there was no pointing in learning a concept that could not fit the finite epistemic agents' minds.

[MLT]{acronym-label="MLT" acronym-form="singular+short"}, however, is agnostic to the unknown distribution, hence, it can be a continuous function. Bayes' rule is the same for [pmfs]{acronym-label="pmf" acronym-form="plural+short"} and [pdfs]{acronym-label="pdf" acronym-form="plural+short"} after all [@mackay:2002; @valpola:2000]. However, when models use continuous random variables, there is no sense in choosing "the most probable model": the probability of a continuous random variable tends to zero at any single point. Only a nonzero range has a nonzero probability. As  [@valpola:2000] puts it: "($\cdots$) a high density *per se* is not important, but the overall probability mass in the vicinity of a model is."

 @rissanen:1986 gave a more formal version of this justification. He noticed that we could always choose a $(n,k)$ code such that the quantisation error of the real distribution is within a margin of error. Imagine the dataset $\sample$ is sampled from a continuous distribution $\rvD=f(x)$ and there is a uniform distribution encoder (raw bit encoder) $\rvU(\rvD)$ that encodes $\rvD$ into a code of k bits. $$\begin{aligned}
  &\rvU(x) = \frac{f(x)}{2^k}\\
  &H[U(\sample)] = - \sum_{x_1}^{x_n} \log \frac{2^k}{f(x)} = - n (\log {f(x)} - \log {2^k})\\
  &Pr\left[ \left( \frac{H[U(\rvS^{n})]}{n} - H[U(x)] \right) \leq \log {\frac{1}{2^k}} >\epsilon \right] < \delta
\end{aligned}$$

This is Shannon's argument that for sufficiently large $n$ and we can always *digitise* the sample to a desired small tolerance of error $\epsilon$, .

## Information measures the complexity of tasks

In [1.2], we proved that information measures the complexity of a task. The information-complexity relation, however, was already presented in  [@rissanen:1986; @hinton:1993], and goes back to  [@wallace:1968] (according to  [@mackay:2002; @valpola:2000]).

In our setting, Nature is a "supervisor" who knows the true distribution of the data $P(\rvD)$ and send us a message $\rvD$ (the observations). The message $\rvD$ implicitly carries the intrinsic pattern $P(\rvD)$ that governs it. Our epistemic agent comes up with an hypothesis $h_i$ that predicts observations $Q(\rvD|h_i)$.

### Minimal Description Length Principle

Supose there is a supervisor (sender) who wants to transmit a given data ($\rvD$) to a receiver. The supervisor will use a model to compress the data, but will also need to send the misfit bits of the model prediction to the data.

The Minimum Description Length Principle  [@rissanen:1986] asserts that the best model for a data distribution minimises the combined cost of describing the model and describing the misfit between the model and the data.[^14] $P_\theta(\rvD)=P(\rvD|\theta)$ determines the probability of the observation $\rvD$. Imagine there a statistical model of the real $P_\theta$ parametrised by $w$, $P(w|\theta)$. The supervisor send a message with:

1.  $L(\theta)$ bits pertaining which model $h(w)$ to use;

2.  $L(\rvD|\theta)$ bits corresponding to the data $\rvD$ predicted by the model, which can be further subdivided onto:

    1.  Parameter block: $L(w|\theta)= - \log P(w|\theta)\delta w$;

    2.  Data misfit block: $L(\rvD| w, \theta) = - \log P(\rvD| w, \theta) \delta D$.

There is a clear tradeoff between the parameter block and the data misfit (see [\[fig:model_selection\]][7]): models with fewer parameters (large $\delta w$) have smaller parameter blocks but do not fit the data as well and therefore have larger misfit blocks; conversely, over parametrised models (small $\delta w$) have larger parameter blocks, but smaller misfit blocks. The optimal description minimises the combined length of the parameter and data misfit blocks ([\[fig:model_selection\]][7], $h_3$).

#### Correspondence to Bayesian inference

Thus, Rissanen's complexity is[^15] $L(\rvD, \theta) = L(\theta) + L(\rvD|\theta)$. In a Bayesian interpretation, the length $L(\theta)$ for different $h$ defines an implicit prior $P(\theta)$ over alternative hypotheses [@mackay:2002]. If there is no bias towards one or another hypothesis, $P(\theta)=2^{-L(\theta)}$ is uniform and the identifier for the model has the same "cost" $L(\theta)$. Likewise, $L(\rvD|\theta)$ defines the density $P(\rvD|\theta)$ that relates to the evidence for each hypothesis.

In other words, message lengths can be mapped onto posterior probabilities: $$\begin{aligned}
  L(\rvD, \theta) &= - \log P(\theta) - \log (P(\rvD|\theta) \delta\rvD)\\
&= -\log P(\rvD|\theta) + \text{const.}
\end{aligned}$$

As a consequence, [MDL]{acronym-label="MDL" acronym-form="singular+short"} has always a Bayesian model comparison interpretation, and *vice-versa*.

## Minimum Description Length Learning {#sec:mdl}

Using the [MDL]{acronym-label="MDL" acronym-form="singular+short"} principle,  [@hinton:1993] proposed an information-theoretical machine learning framework.

Notice that in the [MDL]{acronym-label="MDL" acronym-form="singular+short"} coding scheme ([\[sec:mdl_encoding\]][8]), to send the value of $\delta w$ which is *arbitrarily* small, we will need an encoding that can lead to *arbitrarily* long messages.

#### The bits-back argument

To avoid this potential peril,  @hinton:1993 propose the following coding scheme where a decodable message is obtained without encoding $\delta w$:

1.  The sender computes a distribution $Q(\rvW | \rvD, \theta)$ based on observations of $\rvD$.[^16]

2.  The sender draws a random sample $w$ from $Q(\rvW | \rvD, \theta)$ and encode it with $P(w|\theta)$.

3.  The sender encodes $\rvD$ using $P(D|w,\theta)$.

The trick is that in the second step, instead of using random bits to choose $w$ from $Q(\rvW|\rvD, \theta)$, the sender can use a secondary message as the random bits. So, a long communication, we can say that on average the cost (or length) of the messages are: $$\begin{aligned}
  L(w|\theta)& &+ L(D|w,\theta) &- \text{'bits back'}&\\
  L(w|\theta)& &+ L(D|w,\theta) &- L(w|D,\theta)&\\
  - \log P(w|\theta)\delta w& &- \log P(D|w, \theta)\delta \rvD &- \log P(w|D,\theta)\delta w
\end{aligned}$$ $$\begin{aligned}
  &- \frac{\log P(w|\theta)\cancel{\delta w} P(D|w, \theta)\delta \rvD}{P(w|D,\theta)\cancel{\delta w}}\\
  &- \log P(D|\theta)\delta \rvD\\
  &- \log P(D|\theta) \cancelto{const.}{- \log \delta \rvD} \\
\end{aligned}$$ Thus the proposed coding scheme yields the optimal description length. The only missing step is how the sender computes the distribution $Q$.

For that,  [@hinton:1993] proposes using the [KL]{acronym-label="KL" acronym-form="singular+full"} as a loss function: $$\begin{aligned}
  \ell = \KL(Q||P)
\end{aligned}$$ to approximate the parametric $Q$ to the real $P$. This method for parametric approximation of posterior [pdfs]{acronym-label="pdf" acronym-form="plural+short"} was called *ensemble learning* and is more commonly known as *variational learning*.

### Shannon, Kolmogorov-Chaitin and Rissanen complexities

Let us remind ourselves that Shannon's information measures the expected number of bits needed for encoding a random variable $\rvD$, the entropy $H_p[\rvD]$ is the expected length of $\rvD$ in bits using the optimal encoder $p$.

From [\[eq:epsilon\]][9]: $$\begin{aligned}
                 & 2 ^{-n(H[\rvD] + \epsilon)}              & \leq P\,(\sample)  & \leq 2 ^{-n(H[\rvD] - \epsilon)}              & \\
  2^{-H[\rvD]} < & 2 ^{-(H[\rvD] + \nicefrac{\epsilon}{n})} & \leq P\,(\rvD)     & \leq 2^{-(H[\rvD] - \nicefrac{\epsilon}{n})}  & < 2^{-H[\rvD]+1} \\
                 2^{-L^*(\rvD)}        &      & \leq P\,(\rvD)   &  & \leq 2^{-L^*(\rvD)+1}
\end{aligned}$$

However, one can use a non-optimal encoder $q$ for which the expected length is $H_{p,q}(\rvD)$. Each encoder/decoder $q$ can be seen as a "program" that ouputs an average number of bits $L(\rvD|q) = L_q(\rvD)=H_{p,q}(\rvD)$. The minimum program that outputs $\rvD$ or *minimum description length* of $\rvD$ is ${L}^{*}(\rvD) = L_p(\rvD)=H_{p,p}(\rvD)=H_p(\rvD)$.

In [\[sec:k-c_complexity\]][10], we mentioned the algorithmic information perspective where [KC]{acronym-label="KC" acronym-form="singular+short"} measures the length of the shortest computer program $P$ which is capable of producing the data $\rvD$. Therefore, $$\begin{aligned}
  P(\rvD)= 2^{-KC(\rvD)}
\end{aligned}$$

A well-known algorithmic information result is that [KC]{acronym-label="KC" acronym-form="singular+short"} is not computable due to the halting problem [@turing:1936; @chaitin:2006]. Therefore, we cannot know if a learning algorithm that halts when finding the best $P(\rvD)= 2^{-KC(\rvD)}$ will ever halt. This relates to the fact that the Shannon information needed to describe a continuous random variable is infinite.

Confirming Mitchel's theorem [@mitchell:1980], a bias on $P$ is needed. Either $P(\rvD)$ is binned into a probability mass function (therefore, biased by its precision $\delta\rvD$), or $P(\rvD)$ is a statistical model, it is a "family" of functions identified by a parameter vector $\theta$, $P(\rvD|\,\theta)$.[^17] The first case leads to Shannon Information as a complexity measure (where the prediction should ensemble all encoder/decoders $q_i$ weighted by their posterior probabilities $P(\rvD|q_i)$). The second case, to the idea of *stochastic complexity* developed by Rissanen  [@rissanen:1986] (where instead of averaging over all possible programs, the prediction assumes the best encoder/decoder $P(\rvD)=P(\rvD|q^*)$).

Shannon's entropy, Kolmogorov-Chaitin's complexity, and Rissanen's Stochastic complexity are different but related task complexity measures.

## Concluding Remarks

This chapter presented the information-theoretical perspective of learning and provided a bridge of this perspective to [MLT]{acronym-label="MLT" acronym-form="singular+full"}.

The previous chapter ([\[ch:information\]][11]) had already shown that information is a measure of change in belief which is also the description length of the data (using the expected negative logarithm of its distribution); therefore, a measure of the data structure or lack of pattern. Any learning method derived from [IT]{acronym-label="IT" acronym-form="singular+short"} can be translated to a Bayesian interpretation by a change of scale  [@valpola:2000]. Prior probabilities translate to a coding scheme that is needed to "decode" the data. In other words, information is a measure of complexity of a task. We related this Shannon Information complexity to Kolmogorov-Chaitin complexity and Rissanen's Stochastic complexity.[^18]

In the context where learning is a conversation with Nature ([1.1]), we used Shannon's theorems to demonstrate that information measures the complexity of the task. [MLT]{acronym-label="MLT" acronym-form="singular+short"} and [ITML]{acronym-label="ITML" acronym-form="singular+short"} are *two sides of the same coin*. If in [MLT]{acronym-label="MLT" acronym-form="singular+short"} we make no assumptions on the task and depend on the hypothesis space, [ITML]{acronym-label="ITML" acronym-form="singular+short"} does not assume any hypothesis space but is task-dependent. Either way, learning is about finding patterns in data, and the best hypothesis to describe the data regularities is also the one that compresses it the most.

We presented the [MDL]{acronym-label="MDL" acronym-form="singular+short"} framework which was the first [ITML]{acronym-label="ITML" acronym-form="singular+full"} proposed method. And showed from the correspondence of [MDL]{acronym-label="MDL" acronym-form="singular+short"} with Bayesian inference.[^19]

Therefore, even before introducing [IBT]{acronym-label="IBT" acronym-form="singular+short"}, we can conclude that anything that is explainable by it can be explained in current [MLT]{acronym-label="MLT" acronym-form="singular+short"}. If so, *what is the purpose of [IBT]{acronym-label="IBT" acronym-form="singular+short"}?* After all, according to  [@mackay:2002], [MDL]{acronym-label="MDL" acronym-form="singular+short"} "has no apparent advantage" beyond as a "pedagogical tool". Why would [IBT]{acronym-label="IBT" acronym-form="singular+short"} be any different?

The purpose of [IBT]{acronym-label="IBT" acronym-form="singular+short"} (and [MDL]{acronym-label="MDL" acronym-form="singular+short"}) is to bring a new narrative. Take a look at the transition from [\[fig:intermezzo-1\]][12] to [\[fig:intermezzo-2\]][13]. If two hypotheses generate the same result, do they represent the same understanding? In practice, yes, they do and we can address them mathematically. [^20] But if we think of understanding as meaning, not necessarily.

This other "philosophical" interpretation is understandingly not addressed by the literature. We will, nevertheless, indulge ourselves with some digression. Take, for example, the Lorenz' Ether Theory (LET) and Einstein's Special Relativity Theory (SR).[^21] There is simply no way of distinguishing LET or SR experimentally, but there is a philosophical distinction between the two [@szabo:2011] (as cited by  [@se:525808]). In this example, we can return the same question: *What is the purpose of Special Relativity Theory?*

Meanings are not part of the truth we find in Nature but represent the ideally noiseless encoding of our understanding that we create for other epistemic agents to decode. In this sense, just as the sweetness in honey ([\[honey\]][14]), meaning is projected. It is improbable that the decoded understanding in two "epistemic minds" are the same and different narratives are capable of sparking different analogies and connections.

[^1]: We call [ITML]{acronym-label="ITML" acronym-form="singular+short"} to differentiate from [IBT]{acronym-label="IBT" acronym-form="singular+short"} and [ITL]{acronym-label="ITL" acronym-form="singular+short"}.

[^2]: $\rvT$ for Truth or Theorem.

[^3]: This chapter expands the idea of science as a conversation with Nature from  

[^4]: If the epistemic agent can comprehend $\rvT$, $H[\rvT]$ can fit in the finite epistemic agent "mind".

[^5]: A lossy encoder or noisy channel are in practice the same.

[^6]: lossless $\to \hat{\rvT}=\rvX^{-1}(\rvX(\hat{\rvT}))$. Therefore, by [RI]{acronym-label="RI" acronym-form="singular+short"}, $H[\rvX(\hat{\rvT})]=H[\hat{\rvT}]$ and for all practical purposes $h:=\hat{\rvT}$.

[^7]: In Machine Learning, the *understanding* happens during training and the *expression* in test time.

[^8]: Therefore, we took the liberty of naming it PAC-Shannon.

[^9]: Remember that the Shannon theorem only states there exists such encoder, but nothing has to say about how to find it.

[^10]: We use the word $\Rate$ for rate here to differentiate from the risk $R$ symbol, which is already widespread in the [MLT]{acronym-label="MLT" acronym-form="singular+short"} community.

[^11]: Similar to [MLT]{acronym-label="MLT" acronym-form="singular+short"} problem setting.

[^12]: For reasons that will be clear in the next chapters.

[^13]: Another remark is that no physical quantity has ever been measured with more than twenty digits of precision .

[^14]: Now we give the proper attribution to this idea already presented in [\[sec:conversation, sec:pac-shannon\]][15].

[^15]: Also known as Stochastic Complexity.

[^16]: We will explain how to compute this distribution later.

[^17]: In this our conversation, $\theta$ was the truth $\rvT$.

[^18]: We will also show that Fisher information is the stochastic complexity for isotropic Gaussian distributions ([\[sec:shannon_fisher\]][16]).

[^19]: This relation was expected since we already had shown the correspondence of [IT]{acronym-label="IT" acronym-form="singular+short"} and Bayesian inference.

[^20]: Remember that Shannon decided not to address meaning in his theory.

[^21]: Lorentz ether theory describes a universe in which light moves through a medium called ether. The problem is that the ether can be seen as a mathematical construct that can not be measured or observed. It is used to facilitate predictions calculations. Those predictions in the movement of light can be measured. Einstein's Special Relativity describes a new geometry of a universe that has no ether. However, it uses Lorentz mathematical construct to do so.

  [1.5]: #sec:mdl {reference-type="ref" reference="sec:mdl"}
  [1]: #noisy_channel_theorem {reference-type="ref" reference="noisy_channel_theorem"}
  [2]: #eq:typical_set_size {reference-type="ref" reference="eq:typical_set_size"}
  [3]: #eq:typical_hypothesis_space_cardinality {reference-type="ref" reference="eq:typical_hypothesis_space_cardinality"}
  [4]: #sec:pac-bayes {reference-type="ref" reference="sec:pac-bayes"}
  [5]: #shannon_inequality {reference-type="ref" reference="shannon_inequality"}
  [6]: #th:haussler {reference-type="ref" reference="th:haussler"}
  [1.1]: #sec:conversation {reference-type="ref" reference="sec:conversation"}
  [1.2]: #sec:pac-shannon {reference-type="ref" reference="sec:pac-shannon"}
  [7]: #fig:model_selection {reference-type="ref" reference="fig:model_selection"}
  [8]: #sec:mdl_encoding {reference-type="ref" reference="sec:mdl_encoding"}
  [9]: #eq:epsilon {reference-type="ref" reference="eq:epsilon"}
  [10]: #sec:k-c_complexity {reference-type="ref" reference="sec:k-c_complexity"}
  [11]: #ch:information {reference-type="ref" reference="ch:information"}
  [12]: #fig:intermezzo-1 {reference-type="ref" reference="fig:intermezzo-1"}
  [13]: #fig:intermezzo-2 {reference-type="ref" reference="fig:intermezzo-2"}
  [14]: #honey {reference-type="ref" reference="honey"}
  [15]: #sec:conversation, sec:pac-shannon {reference-type="ref" reference="sec:conversation, sec:pac-shannon"}
  [16]: #sec:shannon_fisher {reference-type="ref" reference="sec:shannon_fisher"}
